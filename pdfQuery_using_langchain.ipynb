{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# PDF Query QA System using Langchain\n",
        "\n",
        "- Developing a QA system to query against the study and lecture material from the CMPE258 Deep Learning Class at SJSU.\n",
        "- Using Astra DB Vector Database which uses Apache Cassandra to store the embeddings.\n",
        "- Using OpenAI LLM and embeddings to quer against the pdf"
      ],
      "metadata": {
        "id": "G4ZHNSPmnJ_R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prerequisites\n",
        "\n",
        "- Astra DB API key\n",
        "- OpenAI API key"
      ],
      "metadata": {
        "id": "Ib9JkGkMnS1i"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MVTz46OF1VKi",
        "outputId": "d0005f6e-fb8b-45bc-981a-2654742f2bf5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/41.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.3/41.3 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m510.5/510.5 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m807.5/807.5 kB\u001b[0m \u001b[31m42.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.4/227.4 kB\u001b[0m \u001b[31m23.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m36.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m22.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.8/18.8 MB\u001b[0m \u001b[31m52.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m41.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m256.9/256.9 kB\u001b[0m \u001b[31m21.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.6/66.6 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.8/77.8 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.5/138.5 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q cassio datasets langchain openai tiktoken pyPDF2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.vectorstores.cassandra import Cassandra # AstraDB uses the open-source Apache Cassandra Vector DB\n",
        "from langchain.indexes.vectorstore import VectorStoreIndexWrapper\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "\n",
        "from datasets import load_dataset\n",
        "from PyPDF2 import PdfReader\n",
        "import cassio"
      ],
      "metadata": {
        "id": "2ck8M79r2tvV"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K8G1pNLy3Izb",
        "outputId": "620923d2-46b3-4450-e918-33c74867a08b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ASTRA_DB_APPLICATION_TOKEN = '' ## Use your own generated Astra DB application token key and paste as a string\n",
        "ASTRA_DB_ID = '' ## Use your own generated Astra DB ID key and paste as a string\n",
        "\n",
        "OPENAI_API_KEY = '' ## Use your own generated OpenAI API token and paste as a string"
      ],
      "metadata": {
        "id": "VefsJOks3kP2"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pdfreader = PdfReader('/content/drive/MyDrive/CMPE258 Slides/CMPE258 1-13 merged.pdf')"
      ],
      "metadata": {
        "id": "KQlTTyCI4Pot"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing_extensions import Concatenate\n",
        "\n",
        "raw_text = ''\n",
        "for i, page in enumerate(pdfreader.pages):\n",
        "  content = page.extract_text()\n",
        "  if content:\n",
        "    raw_text += content"
      ],
      "metadata": {
        "id": "35fGfX7a5oIz"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "o_0G7KsO6CTX",
        "outputId": "a1cf72bc-cbf2-488a-84e3-e34ee178cc4b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'CMPE 258 -01 \\nDeep Learning\\nDr. Kaikai Liu, Ph.D. Associate Professor\\nDepartment of Computer Engineering\\nSan Jose State University \\nEmail: kaikai.liu@sjsu.edu\\nWebsite: https://www.sjsu.edu/cmpe/faculty/tenure -\\nline/kaikai -liu.phpSpring 2024What is Deep Learning\\n•Deep Learning is a part of machine learning that deals with algorithms \\ninspired by the structure and function of the human brain. It uses \\nartificial neural networks to build intelligent models and solve complex \\nproblems. \\nWhat is Artificial Intelligence –old answer\\n•AI textbooks list\\n•http://aima.cs.berkeley.edu/2nd -ed/books.html\\n•Artificial Intelligence: A Modern Approach\\n•http://aima.cs.berkeley.edu/2nd -ed/books.html\\n•http://aima.cs.berkeley.edu/newchap00.pdf\\nWhat is Artificial Intelligence –old answer\\nhttps://aima.cs.berkeley.edu•Peter Norvig is a Director of Research at Google Inc\\n•http://www.norvig.com\\n•SJSU EIAC Membership\\nWhat is Artificial Intelligence –new answer\\n•The Present advancements in AI\\nMachine Learning Lifecycle\\n•Machine Learning play with all ‘Data’\\nOffline\\nTraining\\nDataData\\nCollectionCleaning &\\nVisualizatio n\\nFeature Eng. &\\nModel DesignTraining &\\nValidationModel Development\\nTrained\\nModelsTraining Pipelines\\nLive\\nDataTraining\\nValidationEnd User\\nApplicationQuery\\nPredictionPrediction ServiceInference\\nFeedbackLogic\\nData\\nScientistData\\nEngineerData\\nEngineerDeep Learning vs Machine Learning\\n•Deep Learning is a sub -class of Machine Learning algorithms whose \\npeculiarity is a higher level of complexity.\\n•Deep Learning belongs to Machine Learning and they are absolutely not \\nopposite concepts. We refer to shallow learning to those techniques of machine \\nlearning that are not deep\\n•Why is this complexity an advantage?\\n•As humans, the information is learnt step by step. First layers focus on learning \\nmore specific concepts while the deeper layers will use the information already \\nlearnt to soak in more abstract concepts. This procedure of constructing \\nrepresentations of the data is known as feature extraction.\\n•Their complex architecture provides deep neural nets with the ability to perform a \\nfeature extraction automatically.Deep Learning vs Machine Learning\\n•In conventional machine learning, \\nor shallow learning, this task is \\ncarried out outside the algorithmic \\nstage. People, data scientists’ \\nteams and not machines, are in \\ncharge of analyzing raw data and \\nchange it into valuable features.\\n•Deep learning is a black box .\\nMore Data Better Performance\\n•More Data Better Performance\\nDeep Learning History\\n•The theory of AI is fairly old\\n•The backpropagation algorithm, for example, \\nwas invented more than 40 years ago, and the \\nfirst computation model for neural networks was \\nproposed for the first time almost 80 years back.\\n•AI Winters: the general public lost interest and \\nfunding for AI research dried out.\\n•The history of AI has been a chain of boom -and-\\nbust cycles. We’re currently immersed in the \\nthird cycle of optimism\\nDeep Learning History\\n•The Birth of AI (1943 –1956)\\n•1943: Warren McCulloch and Walter Pitts, proposed a model of artificial neurons\\n•In 1949, Donald Hebb introduced the Hebbian learning rule\\n•Year 1950: The Turing Test\\n•The term “Artificial Intelligence” was officially coined in 1956 by American computer \\nscientist John McCarthy during the Dartmouth Conference. Establishing AI as a distinct \\narea of research and study. At this time, high -level computer languages like FORTRAN, \\nLISP, and COBOL were also invented, fueling enthusiasm for AI research and \\ndevelopment.\\n•The first AI boom took place in the late 50s and 60s when efforts focused on \\nanswering if machines could actually think\\n•The search for the so -called general or strong AI\\n•The invention of the perceptron (an early example of artificial neuron or machine \\nlearning classifier) in 1957 by Frank Rosenblatt was, for some, an unambiguous \\nindication that general or strong AI was very close.Deep Learning History\\n•Year 1969: Limitations of Perceptron was \\nreleased: it could not learn to solve problems that \\nwere not linearly separable. \\n•Year 1972: WABOT -1 —The First Humanoid \\nRobot\\n•The AI Winter of 1973 -1980\\n•UK Parliament analyze the state of AI research after two \\ndecades of disappointing progress in AI (and specifically \\nin Machine Translation): Lighthill Report\\n•DARPA\\'s frustration with the Speech Understanding \\nResearch program at Carnegie Mellon University\\n•DARPA shifting its focus on “mission -oriented”, \\nactionable research which led many AI research groups \\nto lose critical funding\\nDeep Learning History\\n•The AI in the limelight again (80s)\\n•The emergence of expert systems: the developments revolved around the idea of \\ncreating knowledge bases that an inference engine (following logical rules) used to \\nanswer questions about a specific domain of knowledge, e.g. medical diagnosis.\\n•Recurrent neural networks and the backpropagation algorithm (1986) were also \\ndeveloped. \\n•The Second AI Winter (1987 –1993)\\n•The computational power at the time hampered remarkable improvements which \\nbrought the second AI winter.\\n•1987: collapse of the LISP machine market (general -purpose computers designed to \\nefficiently run Lisp as their main software and programming language)\\n•1988: cancellation of new spending on AI by the Strategic Computing Initiative\\n•1993: resistance to new expert systems deployment and maintenanceDeep Learning History\\n•In the 90s, a new vision brought fresh air to AI\\n•Moravec\\'s paradox is the observation in artificial intelligence and robotics that, \\ncontrary to traditional assumptions, reasoning requires very little computation, but \\nsensorimotor and perception skills require enormous computational resources.\\n•The more difficult tasks , however, were indeed those that we do innately, \\nalmost effortlessly, like recognising faces and moving around. \\n•They advocated building intelligence “from the bottom up” and taking into account the role of \\n“the body” in human intelligence. \\n•Consequently, the quest for general AI lost momentum and efforts were redirected to solve \\nspecific isolated problems . This gave rise to the so -called narrow or weak AI.\\n•It was also the time when advanced ML algorithms like Support Vector Machines, \\nRandom Forests, and the area of Reinforcement Learning were developed.Deep Learning History\\n•2006: birth of deep learning\\n•While everybody moved to the algorithms like SVM and all, Geoffrey Hinton still \\nbelieved that true intelligence would be achieved only through Neural Networks. \\nSo for almost 20 years i.e. from 1986 to 2006, he worked on neural networks. \\n•And in 2006 he came up with a phenomenal paper on training a deep neural \\nnetwork. This is the beginning of the era known as Deep Learning. This paper by \\nGeoffrey Hinton did not receive much popularity until 2012.Deep Learning Milestones\\n•MILESTONES IN THE DEVELOPMENT OF NEURAL NETWORKS\\nCurrent “AI Spring”\\n•The successes of the current \"AI spring\" or \"AI boom\" are advances in language \\ntranslation (in particular, Google Translate), image recognition (spurred by the ImageNet \\ntraining database) as commercialized by Google Image Search, and in game -playing \\nsystems such as AlphaZero (chess champion) and AlphaGo (go champion), and \\nAutonomous Driving. Most of these advances have occurred since 2010.\\n•Year 1997: IBM Deep Blue’s Triumph\\n•Year 2000: Google Search uses AI\\n•Year 2002: AI in Homes —Roomba\\n•Year 2005: DARPA autonomous driving challenge: Stanford Stanley\\n•By 2006, AI had made its way into the business world. Companies like Facebook, Twitter, \\nand Netflix began using AI algorithms to improve user experience, personalized content, \\nand recommendation systems.\\n•Year 2006: Neural Networks into Deep Learning\\n•Year 2009: Introduction to ImageNetCurrent “AI Spring”\\n•Year 2009 -2015: Google self -driving car\\n•Year 2012: AlexNet\\n•Year 2012: Google Now —Predictive AI\\n•Year 2013: Deep Learning used to Understand words (Word2Vec)\\n•Year 2014: AlphaGo\\n•Year 2015: TensorFlow was built for DL\\n•Year 2016: DeepMind’s AlphaGO defeated Champion\\n•Year 2015: Tesla Autopilot\\n•Year 2018: Waymo(Self Driving Car)\\n•Year 2022: the release of OpenAI\\'s AI chatbot ChatGPT has reinvigorated the discussion \\nabout artificial intelligence and its effects on the world.\\n•New AI winter could be triggered by overly ambitious or unrealistic promises by prominent \\nAI scientists or overpromising on the part of commercial vendors.Deep Learning\\n•Fathers of the Deep Learning Revolution \\nReceive ACM A.M. Turing Award\\n•Bengio , Hinton and LeCun Ushered in Major \\nBreakthroughs in Artificial Intelligence\\n•https://www.acm.org/media -\\ncenter/2019/march/turing -award -2018\\n•Yoshua Bengio is a Professor at the \\nUniversity of Montreal\\n•Geoffrey Hinton is VP and Engineering \\nFellow of Google, Chief Scientific Adviser \\nofThe Vector Institute and \\naUniversity Professor Emeritus at the \\nUniversity of Toronto.\\n•Yann LeCun is Silver Professor of the \\nCourant Institute of Mathematical \\nSciences at New York University, and VP \\nand Chief AI Scientist at Facebook\\nAlexNet won the ImageNet challenge in 2012\\n•Geoffrey Hinton and his students\\n•Ilya Sutskever (MS 2007, Phd2013)\\n•In 2012, Sutskever built AlexNet in collaboration \\nwith Hinton and Alex Krizhevsky .\\n•Krizhevsky and Sutskever joined Hinton\\'s \\nnew research company DNNResearch , a \\nspinoff of Hinton\\'s research group. In March \\n2013, Google acquired DNNResearch for $5 \\nmillion, shortly after winning the contest (after \\nawarding the team of three $600,000 for their \\nwork in neural networks and language and \\nimage processing) –Google Brain\\n•Baidu, Microsoft, DeepMind (acquired by \\nGoogle in 2014) all want to buy\\na University of Toronto startup that studies neural \\nnetworks. The one -year-old company is launched \\nby computer science professor Geoffrey Hinton \\n(right) and two of his graduate students, Alex \\nKrizhevsky  and Ilya Sutskever  (left). AlexNet won the ImageNet challenge in 2012\\n•Geoffrey Hinton and his students\\n•Hinton retired from Google in 2023.\\n•Alex left Google in September 2017 after losing interest in the work, to work at \\nthe company Dessa in support of new deep -learning techniques. He is the \\ncreator of the CIFAR -10 and CIFAR -100 datasets\\n•At the end of 2015, Ilya Sutskever left Google to become cofounder and chief \\nscientist of the newly founded non -profit organization OpenAI\\n•The actual collected total amount of contributions of OpenAI was only $130 million until 2019, \\n$100 million from Musk.\\n•Sutskever was formerly one of the six board members of the non -profit entity which controls \\nOpenAI\\n•In 2023, people speculated that the firing of Sam Altman in part resulted from a conflict over \\nthe extent to which the company should commit to AI safety.\\n•Following these events, Sutskever stepped down from the board of OpenAIWhy Learning Deep Learning\\n•Career Opportunities : The demand for deep learning expertise is high, \\nleading to rewarding career opportunities in research, development, and \\nimplementation across industries.\\n•Advancements in AI : Deep learning is at the forefront of advancements in \\nartificial intelligence, shaping the evolution of AI technologies and \\ncapabilities.\\n•Wide Applicability : Deep learning is applicable to various domains, \\nincluding computer vision, natural language processing, robotics, and more. \\nThis versatility allows you to address diverse challenges.\\n•Skill Relevance : As AI technologies become more pervasive, understanding \\ndeep learning is crucial for professionals across fields to stay relevant and \\nharness AI\\'s potential.Why Learning Deep Learning\\n•Unprecedented Performance : Deep learning has shown exceptional performance \\nacross a wide range of tasks, from image and speech recognition to natural \\nlanguage understanding. Its ability to handle complex patterns and large datasets \\nmakes it a powerful tool in various domains.\\n•Data Abundance : In the era of big data, deep learning thrives by leveraging vast \\namounts of data to uncover insights, make predictions, and improve decision -\\nmaking.\\n•Automation and Efficiency : Deep learning can automate tasks that traditionally \\nrequired manual effort, leading to increased efficiency and reduced human error. \\nThis is particularly valuable in industries like healthcare, manufacturing, and \\nfinance.\\n•Innovation : Deep learning has fueled breakthroughs in fields such as self -driving \\ncars, personalized medicine, and recommendation systems. Learning these \\ntechniques equips you to contribute to cutting -edge innovations.Learning Objective\\n•Demonstrate a comprehensive understanding of deep neural networks, their \\narchitecture, and principles. \\n•Apply various deep learning algorithms and models to solve problems in \\ndifferent domains. \\n•Analyze and evaluate the performance of deep learning models using \\nappropriate metrics and techniques. \\n•Implement, train, and fine -tune deep neural networks using popular deep \\nlearning frameworks. \\n•Collaborate effectively in both individual and group settings to tackle real -\\nworld deep learning challenges. Demonstrate ethical considerations and \\nawareness of potential biases when working with deep learning applications. Grading\\n29Quiz (5%) \\nIn-class mini -quizHomework&\\nAssignments (20%)\\nMidterm Exam \\n(15%)+Final Exam \\n(30%) = (45%)Team Project (30%)Grading policy for Homework Assignments\\n•Homework assignments will be assessed utilizing the EMRN rubrics, a \\nfour-level specification grading system.\\n•\"E\" (excellent) and \"M\" (meets expectations) scores indicate successful \\ncompletion (full marks). \"R\" (needs revision) and \"N\" (not assessable) scores \\ncorrespond to lower marks, subject to assignment specifications.\\n•Each homework assignment will have one revision period available.\\n•Attaining an \"E\" does not affect the overall grade (full marks). However, students \\naiming for a final letter grade of \"A\" or \"A+\" must secure at least one \"E\" in \\nhomework assignments.\\n•Students got “E” in the homework, needs present their solution in the \\nclass.Determination of Grades \\n•The final grades will be calculated according to the final weighted score of all the \\nassignments (exams, homework assignments, projects). The ABCDE grades will \\nbe determined by a set of thresholds that meets appropriate distribution and all \\nrequired policies, which means what you perceive as a low score by summing up \\nall your raw points will not necessarily prevent you from getting a good grade. \\n•No extra credit options.\\n•Assignment/exam re -evaluation request will not be considered unless the \\nevaluation process is wrong.\\n•Peer ratings and comments will be considered for the project scores.\\n•The report will not be accepted when the Canvas assignment is closed. The \\nassignment will receive zero grade.\\n•For turnitin -enabled assignment, the format of the submission should be \\nacceptable to turnitin (such as WORD and PDF); otherwise the reports will be \\nconsidered late and be penalized as above.\\n31Classroom Protocol \\n•Students are responsible for lecture, book sections, project \\npresentations, and any instructions given in the class.\\n•Avoid disturbing the class.\\n•Students causing disruption in the class for other activities will be asked to leave \\nthe class and will be referred to the Judicial Affairs Officer of the University for \\ndisrupting the class after repeated offenses.\\n•Students should attend all meetings of the class.\\n•Student Excused Absences: https://www.sjsu.edu/senate/docs/S22 -2.pdf\\n•Student shall notify the instructor in writing\\n32\\n33Thank \\nYou\\nAddress:\\nENG257, SJSUEmail Address:\\nKaikai.liu@sjsu.eduCMPE 258 -01 \\nDeep Learning\\nDr. Kaikai Liu, Ph.D. Associate Professor\\nDepartment of Computer Engineering\\nSan Jose State University \\nEmail: kaikai.liu@sjsu.edu\\nWebsite: https://www.sjsu.edu/cmpe/faculty/tenure -\\nline/kaikai -liu.phpSpring 2024Grading\\n2Quiz (5%) \\nIn-class mini -quizHomework&\\nAssignments (20%)\\nMidterm Exam \\n(15%)+Final Exam \\n(30%) = (45%)Team Project (30%)Grading policy for Homework Assignments\\n•Homework assignments will be assessed utilizing the EMRN rubrics, a \\nfour-level specification grading system.\\n•\"E\" (excellent) and \"M\" (meets expectations) scores indicate successful \\ncompletion (full marks). \"R\" (needs revision) and \"N\" (not assessable) scores \\ncorrespond to lower marks, subject to assignment specifications.\\n•Each homework assignment will have one revision period available.\\n•Attaining an \"E\" does not affect the overall grade (full marks). However, students \\naiming for a final letter grade of \"A\" or \"A+\" must secure at least one \"E\" in \\nhomework assignments.\\n•Students got “E” in the homework, needs present their solution in the \\nclass.Determination of Grades \\n•The final grades will be calculated according to the final weighted score of all the \\nassignments (exams, homework assignments, projects). The ABCDE grades will \\nbe determined by a set of thresholds that meets appropriate distribution and all \\nrequired policies, which means what you perceive as a low score by summing up \\nall your raw points will not necessarily prevent you from getting a good grade. \\n•No extra credit options.\\n•Assignment/exam re -evaluation request will not be considered unless the \\nevaluation process is wrong.\\n•Peer ratings and comments will be considered for the project scores.\\n•The report will not be accepted when the Canvas assignment is closed. The \\nassignment will receive zero grade.\\n•For turnitin -enabled assignment, the format of the submission should be \\nacceptable to turnitin (such as WORD and PDF); otherwise the reports will be \\nconsidered late and be penalized as above.\\n4Classroom Protocol \\n•Students are responsible for lecture, book sections, project \\npresentations, and any instructions given in the class.\\n•Avoid disturbing the class.\\n•Students causing disruption in the class for other activities will be asked to leave \\nthe class and will be referred to the Judicial Affairs Officer of the University for \\ndisrupting the class after repeated offenses.\\n•Students should attend all meetings of the class.\\n•Student Excused Absences: https://www.sjsu.edu/senate/docs/S22 -2.pdf\\n•Student shall notify the instructor in writing\\n5\\nProject Key Components\\n•Build a full pipeline of Deep Learning application with model training\\n•Question/Problem Formulation: propose your own application and formulate the problem\\n•Data Acquisition and Processing: \\n•Identify a Suitable Dataset for Model Training and Evaluation\\n•State -of-the-Art Models\\n•Analyze the data, assess and compare various state -of-the-art open source models\\n•Model architecture change, Training, Evaluation, Fine -tunning\\n•Change the model architecture, tune parameters, and proceed with model training/fine -tunning, and \\nevaluation. Gain the insights of the performance impact from the model architecture and parameters.\\n•Inference, Optimization, and Real -time test\\n•Create a comprehensive end -to-end deep learning application to enable inference using the trained model. \\n•Depending on the chosen hardware platform, optimize model inference to enhance speed or reduce \\ncomputational costs. \\n•Perform evaluations and visualizations using real test data.\\n6\\nQuestion/Problem \\nFormulationData Acquisition \\nand ProcessingModel Training, \\nEvaluation, and \\nFine-tunningInference, \\nOptimization, and \\nReal-time testCommon Datasets\\n•Image Classification:\\n•ImageNet (https:// www.image -net.org /): A large dataset with millions of labeled images across thousands of categories. \\n•CIFAR -10 and CIFAR -100, MNIST, FashionMNIST\\n•Object Detection and Segmentation:\\n•COCO (Common Objects in Context): A dataset with images containing objects labeled with bounding boxes and \\nsegmentation masks.\\n•https://cocodataset.org/\\n•Autonomous driving related datasets : Kitti, Waymo, Argo, nuScenes\\n•Natural Language Processing:\\n•IMDB Reviews: A dataset of movie reviews classified as positive or negative sentiment.\\n•SQuAD (Stanford Question Answering Dataset): A dataset for reading comprehension tasks, where models answer questions \\nbased on a given passage.\\n•Machine Translation (WMT19).\\n•Speech Recognition:\\n•LibriSpeech :A dataset of spoken words and sentences collected from audiobooks.\\n•Mozilla Common Voice: A crowdsourced dataset of speech data for various languages.Common Datasets\\n•Time Series Analysis:\\n•UCI Time Series Data Repository: A collection of time series datasets for different applications, \\nsuch as finance, health, and meteorology.\\n•Video Analysis:\\n•Kinetics: A dataset for action recognition in videos with a wide range of human actions.\\n•Medical Imaging:\\n•MURA (Musculoskeletal Radiographs): A dataset of bone X -rays for identifying musculoskeletal \\nabnormalities.\\n•Chest X -Ray Images (Pneumonia): A dataset for diagnosing pneumonia using chest X -ray images.\\n•Autonomous Driving:\\n•KITTI Vision Benchmark Suite (https:// www.cvlibs.net /datasets/ kitti/):A dataset for tasks like \\nobject detection, tracking, and scene understanding in autonomous driving scenarios.\\n•Waymo open dataset: https://waymo.com/open/\\n•Argo open dataset: https://www.argoverse.org/\\n•NuScenes : https://www.nuscenes.org/Project Requirement\\n•Embark on an engaging exploration of a topic that captivates your interest. \\n•Limit one topic per group of four teams or fewer. While individual projects are \\nencouraged, teams of 1 -3 members are also welcomed. Should your group exceed 3 \\nmembers, an additional project design and task distribution document (>1 page) must be \\nsubmitted.\\n•The project encompasses three vital milestones:\\n•A concise, one -page (single -spaced) project proposal.\\n•An engaging presentation of your project to the class.\\n•A comprehensive final project report, accompanied by code and a demonstration video.\\n•The assessment criteria encompass:\\n•The importance of the problem addressed, the novelty of the solutions proposed, \\ntechnical excellence, the degree of complexity, creativity, code clarity, presentation \\nfinesse, and documentation quality. \\n•Each project will be individually graded, ensuring a fair evaluation of efforts.Additional Requirements\\n•It\\'s advisable to avoid selecting a Basic Deep Learning tutorial as your \\nproject. There are numerous tutorials and online examples available for such \\nprojects.\\n•Copying existing tutorials or sample code is prohibited for the project. Your \\nproject must incorporate substantial modifications in terms of \\nimplementation, model selection, or inference optimization.\\n•Refrain from utilizing commercial APIs lacking open source code. While you \\ncan employ commercial APIs for comparative purposes, they shouldn\\'t be \\nyour primary model (you need to identify open source alternatives).\\n•Strive to delve into innovative solutions and engage with advanced \\nmodels. Avoid relying solely on basic solutions to fulfill the project \\ndemonstration and meet minimum requirements . If advanced \\napproaches do not yield desired outcomes, rest assured there won\\'t be \\npenalties, provided you can effectively present your diligent efforts.Deep Learning Thinking\\n•Building the model is \\'more similar to training a dog than to ordinary \\nprogramming.’\\n•Unlike ordinary software, our models are massive neural networks. Their \\nbehaviors are learned from a broad range of data, not programmed explicitly. \\nThough not a perfect analogy, the process is more similar to training a dog \\nthan to ordinary programming. An initial “pre -training” phase comes first, in which \\nthe model learns to predict the next word in a sentence, informed by its exposure \\nto lots of Internet text (and to a vast array of perspectives). This is followed by a \\nsecond phase in which we “fine -tune” our models to narrow down system \\nbehavior. // OpenAIDeep LearningNeural Network\\n•Neural Network\\n•Neural networks are a class of machine learning algorithms used to model \\ncomplex patterns in datasets using multiple hidden layers and non -linear \\nactivation functions. \\n•A neural network takes an input, passes it through multiple layers of \\nhidden neurons (mini -functions with unique coefficients that must be \\nlearned), and outputs a prediction representing the combined input of all \\nthe neurons.\\nReal neuron \\n•Human brain is estimated to contain around \\n86,000,000,000 of such neurons. Each is \\nconnected to thousands of other neurons \\n•Dendrite : is a branched protoplasmic extension of a \\nnerve cell that propagates the electrochemical \\nstimulation received from other neural cells to the cell \\nbody, or soma\\n•Soma : or cell body is the bulbous, non -process \\nportion of a neuron\\n•Axon : or nerve fiber, is a long, slender projection of a \\nnerve cell, or neuron that typically conducts electrical \\nimpulses known as action potentials away from the \\nnerve cell body\\n•Action potentials in neurons are also known as \\n\"nerve impulses\" or \"spikes\", and the temporal \\nsequence of action potentials generated by a neuron \\nis called its \"spike train\". A neuron that emits an action \\npotential, or nerve impulse, is often said to \"fire\".\\nArtificial neuron\\n•The goal of simple artificial neurons models is to reflect some \\nneurophysiological observations, not to reproduce their dynamics.\\n•Neurons in a layer are often called units. Parameters are often called \\nweights.\\n•Stateless wrt. Time, output real values\\nCollection of artificial neuronsNonlinear Classification Problem\\n•Nonlinear classification problem.\\n•\"Nonlinear\" means that you can\\'t accurately predict a label with a model of the \\nform b+w1x1+w2x2 In other words, the \"decision surface\" is not a line. \\nNeural Network\\n•Activation Functions\\n•To model a nonlinear problem, we can directly introduce a nonlinearity. We can \\npipe each hidden layer node through a nonlinear function.\\n•In the model represented by the following graph, the value of each node in \\nHidden Layer 1 is transformed by a nonlinear function before being passed on to \\nthe weighted sums of the next layer. This nonlinear function is called the \\nactivation function.\\nActivation functions\\n•Activation functions are really important for an Artificial Neural Network to \\nlearn and make sense of something really complicated and Non -linear \\ncomplex functional mappings between the inputs and response \\nvariable. They introduce non -linear properties to our Network .\\n•Their main purpose is to convert a input signal of a node in a A -NN to an output \\nsignal. That output signal now is used as a input in the next layer in the stack.\\n•Consider a neuron\\n•the value of Y can be anything ranging from -inf to +inf. The neuron really doesn’t know \\nthe bounds of the value. So how do we decide whether the neuron should fire or not\\n•To check the Y value produced by a neuron and decide whether outside connections \\nshould consider this neuron as “fired” or not. Or rather let’s say —“activated” or not.\\nActivation functions\\n•Activation function is a function used to transform the activation level of a \\nunit (neuron) into an output signal\\n•An activation function serves as a threshold, alternatively called classification or a \\npartition. It essentially divides the original space into typically two partitions.\\n•In its most general sense, a neural network layer performs a projection that is followed \\nby a selection. Both projection and selection are necessary for the dynamics learning. \\nThe selection operation is enforces information irreversibility, an necessary criteria for \\nlearning.\\nActivation functions\\n•If we do not apply a Activation function then the output signal would \\nsimply be a simple linear function .\\n•Alinear function is just a polynomial of one degree.\\n•Now, a linear equation is easy to solve but they are limited in their complexity \\nand have less power to learn complex functional mappings from data. A Neural \\nNetwork without Activation function would simply be a Linear regression \\nModel, which has limited power and does not performs good most of the times. \\nWe want our Neural Network to not just learn and compute a linear function but \\nsomething more complicated than that.\\n•Also without activation function our Neural network would not be able to learn \\nand model other complicated kinds of data such as images, videos , audio , \\nspeech etc.Activation functions\\n•Step and linear functions do not work well\\n•Sigmoid Function\\n•Squashes numbers to range [0,1]\\n•it is nonlinear in nature\\n•It will give an analog activation unlike step function\\n•unlike linear function, the output of the activation function is always going to be in range \\n(0,1) compared to ( -inf, inf) of linear function.\\n•Historically popular since they  have nice interpretation as a  saturating “firing rate” of a \\nneuron\\n•3problems:\\n•Saturated neurons “kill” the gradients\\n•“vanishing gradients”\\n•Sigmoid outputs are not zero -centered\\n•exp() is a bit compute expensive\\nReLU\\n•ReLU (Rectified Linear Unit)\\n•Computes f(x) = max(0,x)\\n•Does not saturate (in +region)\\n•Very computationally efficient\\n•Converges much faster than  sigmoid/tanh in practice (e.g. 6x)\\n•ReLu give us this benefit: yields 0 activation because of the characteristic of ReLu ( \\noutput 0 for negative values of x ). This means a fewer neurons are firing ( sparse \\nactivation ) and the network is lighter.\\n•Problems\\n•Not zero -centered output\\n•gradient will be 0 because of which the weights will not get adjusted during descent. \\nThat means, those neurons which go into that state will stop responding to variations in \\nerror/ input ( simply because gradient is 0, nothing changes ). This is called dying ReLu\\nproblem.\\nLeaky ReLU\\n•Leaky ReLU\\n•Does not saturate\\n•Computationally efficient\\n•Converges much faster than  sigmoid/tanh in practice! \\n(e.g. 6x)\\n•-will not “die”.\\nActivation Functions\\nSigmoid\\ntanhLeaky ReLU\\nMaxout\\nELU\\n ReLU\\nGood default choice\\nFei-Fei Li & Justin Johnson & Serena Yeung Lecture 7- April 24, 2018 Lecture 7-April 25, 20193\\n0New Activation Functions\\n•Swish\\n•Swish is a new, self -gated activation function \\ndiscovered by researchers at Google.\\n•Swish: a Self -Gated Activation Function: \\nhttps://arxiv.org/abs/1710.05941v1\\n•Mish: A Self Regularized Non -\\nMonotonic Activation Function\\n•Paper (2019): \\nhttps://arxiv.org/abs/1908.08681v3\\n31f(x)=x⋅sigmoid (x)\\nNew Activation Functions\\n•Sigmoid Linear Units, or SiLUs\\n•The activation of the SiLU is computed by the sigmoid \\nfunction multiplied by its input\\n•Paper (2017): https://arxiv.org/abs/1702.03118v3\\n•Gaussian Error Linear Unit, or GELU\\n•The GELU activation function is x times the standard \\nGaussian cumulative distribution function. The GELU \\nnonlinearity weights inputs by their percentile\\n•Paper (2016): https://arxiv.org/abs/1606.08415v5\\n•GELUs are used in GPT -3, BERT, and most other \\nTransformers.\\n32\\nNeural Network\\n•To unify the notation, the equations will be written for the selected \\nlayer [l]. By the way, subscript imark the index of a neuron in that layer.\\n•Each neuron in the layer performs a similar calculation according to the \\nfollowing equations:\\nNeural Network Regression\\n•Neural Network for Regression: \\n•Similar to Linear Regression that it also uses a set of inputs and weights to \\nproduce an output.\\n•Regression Loss Functions: Mean Squared Error\\n•Backpropagation allows us to calculate and attribute the error associated with \\neach neuron, allowing us to adjust and fit the parameters of the model(s) \\nappropriately.\\nGradient descent\\n•Gradient descent is an optimization algorithm for minimizing the loss of \\na predictive model with regard to a training dataset.\\n•Which step in this algorithm is most time consuming?\\n•Typically the loss function is really the average loss over a large dataset .\\n•Loading and computing on all the data is expensive \\nThis update is simultaneously done for all the weights.Stochastic gradient descent\\n•Batch gradient descent computes the gradient using the whole dataset.\\n•This is great for convex, or relatively smooth error manifolds. In this case, we \\nmove somewhat directly towards an optimum solution, either local or global. \\n•Stochastic gradient descent (SGD) computes the gradient using \\nrandom sample. \\n•Most applications of SGD actually use a minibatch of several samples. \\n•SGD works well for error manifolds that have lots of local maxima/minima. In this \\ncase, the somewhat noisier gradient calculated using the reduced number of \\nsamples tends to jerk the model out of local minima into a region that hopefully is \\nmore optimal. \\n•Single samples are really noisy, while minibatches tend to average a little of the \\nnoise out. Thus, the amount of jerk is reduced when using minibatches.Stochastic Gradient Descent \\n•Draw a simple random sample of data indices\\n•Often called a batch or mini-batch\\n•Choice of batch size trade -off gradient quality and speed \\n•Compute gradient estimate and uses as gradient\\n•Loss can be written as a sum of the loss on each \\nrecord. \\nStochastic Gradient Descent\\n•Dominant algorithm in modern machine learning and AI \\n•Some important variations\\n•Momentum terms (SGD with Momentum)\\n•Automatic learning rates (ADAM) \\n•Core algorithm driving progress in deep learning \\n•TensorFlow and Pytorch designed around SGD \\n•\\n42Thank \\nYou\\nAddress:\\nENG257, SJSUEmail Address:\\nKaikai.liu@sjsu.eduCMPE 258 -01 \\nDeep Learning\\nDr. Kaikai Liu, Ph.D. Associate Professor\\nDepartment of Computer Engineering\\nSan Jose State University \\nEmail: kaikai.liu@sjsu.edu\\nWebsite: https://www.sjsu.edu/cmpe/faculty/tenure -\\nline/kaikai -liu.phpSpring 2024Neural Network Regression\\n•Neural Network for Regression: \\n•Similar to Linear Regression that it also uses a set of inputs and weights to \\nproduce an output.\\n•Regression Loss Functions: Mean Squared Error\\n•Backpropagation allows us to calculate and attribute the error associated with \\neach neuron, allowing us to adjust and fit the parameters of the model(s) \\nappropriately.\\nGradient descent\\n•Gradient descent is an optimization algorithm for minimizing the loss of \\na predictive model with regard to a training dataset.\\n•Which step in this algorithm is most time consuming?\\n•Typically the loss function is really the average loss over a large dataset .\\n•Loading and computing on all the data is expensive \\nThis update is simultaneously done for all the weights.Stochastic gradient descent\\n•Batch gradient descent computes the gradient using the whole dataset.\\n•This is great for convex, or relatively smooth error manifolds. In this case, we \\nmove somewhat directly towards an optimum solution, either local or global. \\n•Stochastic gradient descent (SGD) computes the gradient using \\nrandom sample. \\n•Most applications of SGD actually use a minibatch of several samples. \\n•SGD works well for error manifolds that have lots of local maxima/minima. In this \\ncase, the somewhat noisier gradient calculated using the reduced number of \\nsamples tends to jerk the model out of local minima into a region that hopefully is \\nmore optimal. \\n•Single samples are really noisy, while minibatches tend to average a little of the \\nnoise out. Thus, the amount of jerk is reduced when using minibatches.Stochastic Gradient Descent \\n•Draw a simple random sample of data indices\\n•Often called a batch or mini-batch\\n•Choice of batch size trade -off gradient quality and speed \\n•Compute gradient estimate and uses as gradient\\n•Loss can be written as a sum of the loss on each \\nrecord. \\nStochastic Gradient Descent\\n•Dominant algorithm in modern machine learning and AI \\n•Some important variations\\n•Momentum terms (SGD with Momentum)\\n•Automatic learning rates (ADAM) \\n•Core algorithm driving progress in deep learning \\n•TensorFlow and Pytorch designed around SGD \\n•\\nLearning Rate\\n•Learning rate\\nNeural Network Training\\n•Stochastic Gradient Descent With Back -propagation\\n•Stochastic gradient descent is an optimization algorithm for minimizing the \\nloss of a predictive model with regard to a training dataset.\\n•Back -propagation is an automatic differentiation algorithm for calculating \\ngradients for the weights in a neural network graph structure.\\n•It makes gradient descent feasible for multi -layer neural networks . \\n•Stochastic gradient descent and the back -propagation of error algorithms \\ntogether are used to train neural network models.\\nBackpropagationNeural Classifier\\n•The simplest “neural” classifier\\n•Equivalent to logistic regression: sigmoid function \\noutput (0~1)\\n•Cross entropy loss is also called negative log \\nlikelihood or logistic loss.\\n•two separate cost functions: one for y=1and one for y=0.\\n•Being additive over samples allows for efficient \\nlearning.\\n•Encodes negation of logarithm of probability of \\nentirely correct classification \\nCost Function\\n•The benefits of taking the logarithm reveal themselves when you look at the \\ncost function graphs for y=1 and y=0. These smooth monotonic \\nfunctions (always increasing or always decreasing) make it easy to calculate \\nthe gradient and minimize cost.\\n•Image from Andrew Ng’s slides on logistic regression\\n•The key thing to note is the cost function penalizes confident and wrong predictions \\nmore than it rewards confident and right predictions\\nCost Function\\n•We can compress our cost function\\'s two conditional cases into one \\ncase (when y is equal to 1, then the second term will be zero and will \\nnot affect the result. If y is equal to 0, then the first term will be zero and \\nwill not affect the result):\\n•We can fully write out our entire cost function as follows:\\n•Gradient Descent\\nSoftmax\\n•Softmax function is also a type of sigmoid function but is handy when \\nwe are trying to handle classification problems. \\n•The sigmoid function was able to handle just two classes. \\n•Softmax is the most commonly used final activation in classification (produces \\nprobability estimate) and is the Multi -dimensional generalisation of sigmoid\\n•Has simple derivatives\\n•The softmax function would squeeze the outputs for each class between 0 and 1 \\nand would also divide by the sum of the outputs. This essentially gives the \\nprobability of the input being in a particular class. \\nSoftmax  function takes an N -dimensional vector of real numbers and transforms it into a vector of real \\nnumber in range (0,1) which add upto  1.\\nSoftmax\\n•The property of softmax function that it outputs a probability distribution \\nmakes it suitable for probabilistic interpretation in classification tasks.\\n•The numerical range of floating point numbers in numpy is limited. For exponential, its \\nnot difficult to overshoot that limit, in which case python returns nan\\n•To make our softmax function numerically stable, we simply normalize the values in the \\nvector, by multiplying the numerator and denominator with a constant\\n•We can choose an arbitrary value for log(C) term, but generally log(C)=−max(a) is \\nchosen, as it shifts all of elements in the vector to negative to zero\\ndef stable_softmax (X): \\n    exps  = np.exp (X - np.max(X)) \\n    return  exps  / np.sum(exps )Derivative of Softmax\\n•Due to the desirable property of softmax function outputting a probability \\ndistribution, we use it as the final layer in neural networks. \\n•For this we need to calculate the derivative or gradient and pass it back to the previous \\nlayer during backpropagation.\\n•Cross entropy indicates the distance between what the model believes the \\noutput distribution should be, and what the original distribution really is.\\n•It is used when node activations can be understood as representing the probability that \\neach hypothesis might be true, i.e. when the output is a probability distribution. Thus it is \\nused as a loss function in neural networks which have softmax activations in the output \\nlayer.\\nSoftmax + Cross entropy\\n•Softmax + Cross entropy\\n•Cross Entropy Loss with Softmax function are used as the output layer \\nextensively.\\n•Encodes negation of logarithm of probability of entirely correct classification \\n•Equivalent to multinomial logistic regression model\\n•Numerically stable combination\\nwhich is a very simple and elegant expressionMulti -Class Neural Networks\\n•Given a classification problem with N \\npossible solutions, a one -vs.-all \\nsolution consists of N separate binary \\nclassifiers —one binary classifier for \\neach possible outcome. \\n•During training, the model runs through a \\nsequence of binary classifiers, training \\neach to answer a separate classification \\nquestion.\\nMulti -Class Neural Networks\\n•Softmax extends the probability idea \\ninto a multi -class world. \\n•Softmax assigns decimal probabilities to \\neach class in a multi -class problem. \\n•Those decimal probabilities must add up \\nto 1.0. \\n•This additional constraint helps training \\nconverge more quickly than it otherwise \\nwould.\\nNeural Network Training\\n•The function computed by the network is a composition of the functions \\ncomputed by individual layers (e.g., linear layers and nonlinearities):\\n•Training a multi -layer network\\n•To train the network, we need to find the gradient of the error w.r.t. the \\nparameters of each layer\\nLayer 1\\n Layer 2\\n Layer K\\n Output Input\\n …\\n𝑓!(𝑥,𝑤!) 𝑓\"(ℎ!,𝑤\") 𝑓#(ℎ#$!,𝑤#) 𝑙(ℎ#,𝑦)ℎ!ℎ\" ℎ#$! ℎ#\\n𝑒 𝑥…\\ninput output layer loss function error first layer \\ntransformationsecond layer \\ntransformationhidden \\nrepresentationoutput\\n𝑤%←𝑤%−𝜂𝜕𝑒\\n𝜕𝑤%kth layer:Neural Network Training\\n•Chain rule\\n•Applying the chain rule\\n•Start with k=1𝑔 𝑥𝑦 𝑑𝑧\\n𝑑𝑥=𝑑𝑧\\n𝑑𝑦𝑑𝑦\\n𝑑𝑥𝑓 𝑧\\n𝑓!(𝑥,𝑤!) 𝑙(ℎ!,𝑦) 𝑥ℎ!\\n𝑒𝑤!\\n𝜕𝑒\\n𝜕ℎ!𝜕ℎ!\\n𝜕𝑤!𝑒=𝑙𝑓!𝑥,𝑤!,𝑦\\n𝑒=(𝑦−𝑤!&𝑥)\"\\n𝑒=𝑙ℎ!,𝑦=(𝑦−ℎ!)\"ℎ!=𝑓!𝑥,𝑤!=𝑤!&𝑥\\nNeural Network Training\\n•Applying the chain rule\\n•k=2\\nNeural Network Training\\n•Backpropagation\\nℎ!\"#ℎ!Upstream gradient: \\n$%\\n$&! \\nDownstream gradient: \\n$%\\n$&!\"#=$%\\n$&!$&!\\n$&!\"#𝑓#𝑤!𝜕𝑒\\n𝜕𝑤%=𝜕𝑒\\n𝜕ℎ%𝜕ℎ%\\n𝜕𝑤%\\n𝜕ℎ%\\n𝜕ℎ%$!𝜕ℎ%\\n𝜕𝑤%Local gradient\\nLocal gradientParameter update:\\nForward pass\\nBackward passNeural Network Training\\n•With branches\\n•E.g., Resnet\\nℎ!\"#ℎ!,#Upstream gradient: \\n$%\\n$&!,# \\nDownstream gradient: \\n$%\\n$&!\"#=$%\\n$&!$&!\\n$&!\"#𝑓#𝑤!𝜕𝑒\\n𝜕𝑤%=𝜕𝑒\\n𝜕ℎ%𝜕ℎ%\\n𝜕𝑤%\\n𝜕ℎ%\\n𝜕ℎ%$!𝜕ℎ%\\n𝜕𝑤%Local gradient\\nLocal gradientParameter update:\\n$%\\n$&!,% ℎ!,(+ Gradients add at \\nbranches28Thank \\nYou\\nAddress:\\nENG257, SJSUEmail Address:\\nKaikai.liu@sjsu.eduCMPE 258 -01 \\nDeep Learning\\nDr. Kaikai Liu, Ph.D. Associate Professor\\nDepartment of Computer Engineering\\nSan Jose State University \\nEmail: kaikai.liu@sjsu.edu\\nWebsite: https://www.sjsu.edu/cmpe/faculty/tenure -\\nline/kaikai -liu.phpSpring 2024Neural Network Regression\\n•Neural Network for Regression: \\n•Similar to Linear Regression that it also uses a set of inputs and weights to \\nproduce an output.\\n•Regression Loss Functions: Mean Squared Error\\n•Backpropagation allows us to calculate and attribute the error associated with \\neach neuron, allowing us to adjust and fit the parameters of the model(s) \\nappropriately.\\nGradient descent\\n•Gradient descent is an optimization algorithm for minimizing the loss of \\na predictive model with regard to a training dataset.\\n•Which step in this algorithm is most time consuming?\\n•Typically the loss function is really the average loss over a large dataset .\\n•Loading and computing on all the data is expensive \\nThis update is simultaneously done for all the weights.Stochastic gradient descent\\n•Batch gradient descent computes the gradient using the whole dataset.\\n•This is great for convex, or relatively smooth error manifolds. In this case, we \\nmove somewhat directly towards an optimum solution, either local or global. \\n•Stochastic gradient descent (SGD) computes the gradient using \\nrandom sample. \\n•Most applications of SGD actually use a minibatch of several samples. \\n•SGD works well for error manifolds that have lots of local maxima/minima. In this \\ncase, the somewhat noisier gradient calculated using the reduced number of \\nsamples tends to jerk the model out of local minima into a region that hopefully is \\nmore optimal. \\n•Single samples are really noisy, while minibatches tend to average a little of the \\nnoise out. Thus, the amount of jerk is reduced when using minibatches.Stochastic Gradient Descent \\n•Draw a simple random sample of data indices\\n•Often called a batch or mini-batch\\n•Choice of batch size trade -off gradient quality and speed \\n•Compute gradient estimate and uses as gradient\\n•Loss can be written as a sum of the loss on each \\nrecord. \\nStochastic Gradient Descent\\n•Dominant algorithm in modern machine learning and AI \\n•Some important variations\\n•Momentum terms (SGD with Momentum)\\n•Automatic learning rates (ADAM) \\n•Core algorithm driving progress in deep learning \\n•TensorFlow and Pytorch designed around SGD \\n•\\nLearning Rate\\n•Learning rate\\nNeural Network Training\\n•Stochastic Gradient Descent With Back -propagation\\n•Stochastic gradient descent is an optimization algorithm for minimizing the \\nloss of a predictive model with regard to a training dataset.\\n•Back -propagation is an automatic differentiation algorithm for calculating \\ngradients for the weights in a neural network graph structure.\\n•It makes gradient descent feasible for multi -layer neural networks . \\n•Stochastic gradient descent and the back -propagation of error algorithms \\ntogether are used to train neural network models.\\nBackpropagationNeural Classifier\\n•The simplest “neural” classifier\\n•Equivalent to logistic regression: sigmoid function \\noutput (0~1)\\n•Cross entropy loss is also called negative log \\nlikelihood or logistic loss.\\n•two separate cost functions: one for y=1and one for y=0.\\n•Being additive over samples allows for efficient \\nlearning.\\n•Encodes negation of logarithm of probability of \\nentirely correct classification \\nCost Function\\n•The benefits of taking the logarithm reveal themselves when you look at the \\ncost function graphs for y=1 and y=0. These smooth monotonic \\nfunctions (always increasing or always decreasing) make it easy to calculate \\nthe gradient and minimize cost.\\n•Image from Andrew Ng’s slides on logistic regression\\n•The key thing to note is the cost function penalizes confident and wrong predictions \\nmore than it rewards confident and right predictions\\nCost Function\\n•We can compress our cost function\\'s two conditional cases into one \\ncase (when y is equal to 1, then the second term will be zero and will \\nnot affect the result. If y is equal to 0, then the first term will be zero and \\nwill not affect the result):\\n•We can fully write out our entire cost function as follows:\\n•Gradient Descent\\nSoftmax\\n•Softmax function is also a type of sigmoid function but is handy when \\nwe are trying to handle classification problems. \\n•The sigmoid function was able to handle just two classes. \\n•Softmax is the most commonly used final activation in classification (produces \\nprobability estimate) and is the Multi -dimensional generalisation of sigmoid\\n•Has simple derivatives\\n•The softmax function would squeeze the outputs for each class between 0 and 1 \\nand would also divide by the sum of the outputs. This essentially gives the \\nprobability of the input being in a particular class. \\nSoftmax  function takes an N -dimensional vector of real numbers and transforms it into a vector of real \\nnumber in range (0,1) which add upto  1.\\nSoftmax\\n•The property of softmax function that it outputs a probability distribution \\nmakes it suitable for probabilistic interpretation in classification tasks.\\n•The numerical range of floating point numbers in numpy is limited. For exponential, its \\nnot difficult to overshoot that limit, in which case python returns nan\\n•To make our softmax function numerically stable, we simply normalize the values in the \\nvector, by multiplying the numerator and denominator with a constant\\n•We can choose an arbitrary value for log(C) term, but generally log(C)=−max(a) is \\nchosen, as it shifts all of elements in the vector to negative to zero\\ndef stable_softmax (X): \\n    exps  = np.exp (X - np.max(X)) \\n    return  exps  / np.sum(exps )Derivative of Softmax\\n•Due to the desirable property of softmax function outputting a probability \\ndistribution, we use it as the final layer in neural networks. \\n•For this we need to calculate the derivative or gradient and pass it back to the previous \\nlayer during backpropagation.\\n•Cross entropy indicates the distance between what the model believes the \\noutput distribution should be, and what the original distribution really is.\\n•It is used when node activations can be understood as representing the probability that \\neach hypothesis might be true, i.e. when the output is a probability distribution. Thus it is \\nused as a loss function in neural networks which have softmax activations in the output \\nlayer.\\nSoftmax + Cross entropy\\n•Softmax + Cross entropy\\n•Cross Entropy Loss with Softmax function are used as the output layer \\nextensively.\\n•Encodes negation of logarithm of probability of entirely correct classification \\n•Equivalent to multinomial logistic regression model\\n•Numerically stable combination\\nwhich is a very simple and elegant expressionMulti -Class Neural Networks\\n•Given a classification problem with N \\npossible solutions, a one -vs.-all \\nsolution consists of N separate binary \\nclassifiers —one binary classifier for \\neach possible outcome. \\n•During training, the model runs through a \\nsequence of binary classifiers, training \\neach to answer a separate classification \\nquestion.\\nMulti -Class Neural Networks\\n•Softmax extends the probability idea \\ninto a multi -class world. \\n•Softmax assigns decimal probabilities to \\neach class in a multi -class problem. \\n•Those decimal probabilities must add up \\nto 1.0. \\n•This additional constraint helps training \\nconverge more quickly than it otherwise \\nwould.\\nNeural Network Training\\n•The function computed by the network is a composition of the functions \\ncomputed by individual layers (e.g., linear layers and nonlinearities):\\n•Training a multi -layer network\\n•To train the network, we need to find the gradient of the error w.r.t. the \\nparameters of each layer\\nLayer 1\\n Layer 2\\n Layer K\\n Output Input\\n …\\n𝑓!(𝑥,𝑤!) 𝑓\"(ℎ!,𝑤\") 𝑓#(ℎ#$!,𝑤#) 𝑙(ℎ#,𝑦)ℎ!ℎ\" ℎ#$! ℎ#\\n𝑒 𝑥…\\ninput output layer loss function error first layer \\ntransformationsecond layer \\ntransformationhidden \\nrepresentationoutput\\n𝑤%←𝑤%−𝜂𝜕𝑒\\n𝜕𝑤%kth layer:Neural Network Training\\n•Chain rule\\n•Applying the chain rule\\n•Start with k=1𝑔 𝑥𝑦 𝑑𝑧\\n𝑑𝑥=𝑑𝑧\\n𝑑𝑦𝑑𝑦\\n𝑑𝑥𝑓 𝑧\\n𝑓!(𝑥,𝑤!) 𝑙(ℎ!,𝑦) 𝑥ℎ!\\n𝑒𝑤!\\n𝜕𝑒\\n𝜕ℎ!𝜕ℎ!\\n𝜕𝑤!𝑒=𝑙𝑓!𝑥,𝑤!,𝑦\\n𝑒=(𝑦−𝑤!&𝑥)\"\\n𝑒=𝑙ℎ!,𝑦=(𝑦−ℎ!)\"ℎ!=𝑓!𝑥,𝑤!=𝑤!&𝑥\\nNeural Network Training\\n•Applying the chain rule\\n•k=2\\nNeural Network Training\\n•Backpropagation\\nℎ!\"#ℎ!Upstream gradient: \\n$%\\n$&! \\nDownstream gradient: \\n$%\\n$&!\"#=$%\\n$&!$&!\\n$&!\"#𝑓\"𝑤!𝜕𝑒\\n𝜕𝑤%=𝜕𝑒\\n𝜕ℎ%𝜕ℎ%\\n𝜕𝑤%\\n𝜕ℎ%\\n𝜕ℎ%$!𝜕ℎ%\\n𝜕𝑤%Local gradient\\nLocal gradientParameter update:\\nForward pass\\nBackward passNeural Network Training\\n•With branches\\n•E.g., Resnet\\nℎ!\"#ℎ!,#Upstream gradient: \\n$%\\n$&!,# \\nDownstream gradient: \\n$%\\n$&!\"#=$%\\n$&!$&!\\n$&!\"#𝑓\"𝑤!𝜕𝑒\\n𝜕𝑤%=𝜕𝑒\\n𝜕ℎ%𝜕ℎ%\\n𝜕𝑤%\\n𝜕ℎ%\\n𝜕ℎ%$!𝜕ℎ%\\n𝜕𝑤%Local gradient\\nLocal gradientParameter update:\\n$%\\n$&!,% ℎ!,(+ Gradients add at \\nbranchesNeural Network Training\\n•In vector calculus, the Jacobian matrix of a vector -valued function of \\nseveral variables is the matrix of all its first -order partial derivatives\\n•The Jacobian matrix collects all first -order partial derivatives of a \\nmultivariate function that can be used for backpropagation.\\nNeural Network Training\\n•Gradient\\n•Dealing with vectors: Jacobian\\n𝑥 𝑧\\n$%\\n$) 𝑓(𝑥)𝜕𝑧\\n𝜕𝑥\\n𝑁×𝑀 \\nJacobian\\n1×𝑀 1×𝑁 \\n1×𝑁 =𝜕𝑧(!)\\n𝜕𝑥(!)…𝜕𝑧(!)\\n𝜕𝑥())\\n⋮ ⋱ ⋮\\n𝜕𝑧(*)\\n𝜕𝑥(!)…𝜕𝑧(*)\\n𝜕𝑥())\\n𝜕𝑒\\n𝜕𝑥\\t=𝜕𝑒\\n𝜕𝑧\\t𝜕𝑧\\n𝜕𝑥\\n𝑁×𝑀 1×𝑁 1×𝑀 Jacobian: row indices \\ncorrespond to outputs, \\ncolumn indices \\ncorrespond to inputs. \\nThe 𝑖,𝑗th element of \\nthe Jacobian is the \\npartial derivative of the \\n𝑖th output w.r.t. 𝑗th \\ninput\\nNeural Network Training\\n•Simple case: Elementwise operation ( ReLU layer)\\n𝑥 𝑧\\n$%\\n$) 𝑓(𝑥)=max(0,𝑥)𝜕𝑧\\n𝜕𝑥\\n𝑀×𝑀 \\nJacobian\\n1×𝑀 1×𝑀 \\n1×𝑀 =𝜕𝑧(!)\\n𝜕𝑥(!)…𝜕𝑧(!)\\n𝜕𝑥())\\n⋮ ⋱ ⋮\\n𝜕𝑧())\\n𝜕𝑥(!)…𝜕𝑧())\\n𝜕𝑥())\\n𝜕𝑒\\n𝜕𝑥\\t=𝜕𝑒\\n𝜕𝑧\\t𝜕𝑧\\n𝜕𝑥\\n1×𝑀 1×𝑀 𝑀×𝑀 What does the Jacobian for \\nan elementwise \\nfunction look like?Neural Network Training\\n•Simple case: Elementwise operation ( ReLU layer)\\n𝑥 𝑧\\n$%\\n$) 𝑓(𝑥)=max(0,𝑥)𝜕𝑧\\n𝜕𝑥\\n𝑀×𝑀 \\nJacobian\\n1×𝑀 1×𝑀 \\n1×𝑀 =𝜕𝑧(!)\\n𝜕𝑥(!)… 0\\n⋮ ⋱ ⋮\\n0 …𝜕𝑧())\\n𝜕𝑥())\\n𝜕𝑒\\n𝜕𝑥\\t=𝜕𝑒\\n𝜕𝑧\\t𝜕𝑧\\n𝜕𝑥\\n1×𝑀 1×𝑀 𝑀×𝑀 What does the Jacobian for \\nan elementwise \\nfunction look like?Backpropagation\\n•Simple example\\ne.g. x = -2, y = 5, z =-4\\nWant:\\ne.g. x = -2, y = 5, z =-4\\nWant:\\nBackpropagation: \\na simple examplee.g. x = -2, y = 5, z =-4\\nWant:\\nBackpropagation: \\na simple examplee.g. x = -2, y = 5, z =-4\\nWant:\\nBackpropagation: \\na simple examplee.g. x = -2, y = 5, z =-4\\nChain rule:\\nWant: Upstream  \\ngradientLocal  \\ngradientBackpropagation: \\na simple  exampleChain rule:\\ne.g. x = -2, y = 5, z =-4\\nBackpropagation: \\na simple example\\nWant:Upstream  \\ngradientLocal  \\ngradientBackpropagation\\n•Backpropagation is an algorithm \\nthat allows us to calculate a very \\ncomplicated gradient, like the one \\nwe need. \\n•The parameters of the neural network \\nare adjusted according to the following \\nformulae.\\n•α represents learning rate -a \\nhyperparameter which allows you to control \\nthe value of performed adjustment. \\nOptimizer algorithms\\n•Optimizer algorithms are optimization method that helps improve a \\ndeep learning model’s performance. \\n•While training the deep learning optimizers model, modify each \\nepoch’s weights and minimize the loss function. \\n•An optimizer is a function or an algorithm that adjusts the attributes of the neural \\nnetwork, such as weights and learning rates. \\n•Thus, it helps in reducing the overall loss and improving accuracy. \\n•Gradient Descent Deep Learning Optimizer\\n•Stochastic Gradient Descent Deep Learning Optimizer\\n•Gradient descent works best for most purposes. However, it has some downsides too. It is \\nexpensive to calculate the gradients if the size of the data is huge. Gradient descent works well \\nfor convex functions, but it doesn’t know how far to travel along the gradient for nonconvex \\nfunctions.Optimization: Problems with SGD\\n•What if the loss function has a  \\nlocal minima or saddle point ?\\n•Saddle points much more common \\nin high dimension\\n•Our gradients come from  \\nminibatches so they can benoisy!\\n42\\nSGD +Momentum Optimizer\\nSGD\\n-Build up “velocity” as a running mean ofgradients\\n-Rho gives “friction”; typically rho=0.9 or0.99\\nSGD+Momentum\\nFei-Fei Li & Justin Johnson & Serena Yeung Lecture 7- April 24, 2018 Lecture 7-Learning Rate\\n•Learning rate as ahyperparameter\\n•Q: Which one of these learning rates is best to use?\\n•A: All of them! Start with large learning rate and decay over time\\nOther Optimizers\\n•Adagrad (Adaptive Gradient Descent) Deep Learning Optimizer\\n•Uses different learning rates for each iteration\\n•It performs smaller updates for parameters associated with frequently occurring \\nfeatures, and larger updates for parameters associated with infrequently occurring \\nfeatures.\\n•RMSProp\\n•Root Mean Square Propagation ( RMSProp ) that also maintains per -parameter learning \\nrates that are adapted based on the average of recent magnitudes of the gradients for \\nthe weight \\n•RMSProp keep moving average of the squared gradients for each weight. And then we \\ndivide the gradient by square root the mean square.\\nOther Optimizers\\n•Adam is an adaptive learning rate optimization \\nalgorithm that utilises both momentum and \\nscaling, combining the benefits of RMSProp and \\nSGD w/ thMomentum. \\n•Adam paper: https://arxiv.org/pdf/1412.6980.pdf\\n•The algorithm updates exponential moving averages of \\nthe gradient (mt) and the squared gradient ( vt) where \\nthe hyper -parameters β1 , β2 ∈[0, 1) control the \\nexponential decay rates of these moving averages.\\nTorch Optim\\n•torch.optim is a package implementing various optimization algorithms.\\n•Optimizer s also support specifying per -parameter options.\\n•contain a params key, containing a list of parameters belonging to it.\\n•Taking an optimization stepoptimizer = optim.SGD (model.parameters (), lr=0.01, momentum=0.9)\\noptimizer = optim.Adam (model.parameters (), lr=0.05)\\noptim.SGD ([\\n                {\\'params\\': model.base.parameters ()},\\n                {\\'params\\': model.classifier.parameters (), \\'lr\\': 1e-3}\\n            ], lr=1e-2, momentum=0.9)This means that model.base ’s parameters will use the \\ndefault learning rate of 1e-2,model.classifier ’s parameters \\nwill use a learning rate of 1e-3, and a momentum of 0.9will \\nbe used for all parameters.\\nfor input, target in dataset:\\n    optimizer.zero_grad ()\\n    output = model(input)\\n    loss = loss_fn (output, target)\\n    loss.backward ()\\n    optimizer.step ()Beyond Training Error\\nBetter optimization algorithms  \\nhelp reduce training loss\\nBut we really care about error on  \\nnew data -how to reduce thegap?\\nFei-Fei Li & Justin Johnson & Serena Yeung Lecture 7- April 24, 2018 Lecture 7-Overfitting\\n•The gap between the training and validation accuracy indicates the \\namount of overfitting. \\nEarly Stopping: Always dothis\\nIterationLoss\\nIterationAccuracyTrain  \\nVal\\nStop training here\\nFei-Fei Li & Justin Johnson & Serena Yeung Lecture 7- April 24, 2018 Lecture 7-April 25,  2019 Fei-Fei Li & Justin Johnson & Serena  Yeung 53Stop training the model when accuracy on the validation set decreases  \\nOr train for a long time, but always keep track of the model snapshot  \\nthat worked best on valHow to improve single -model performance?\\nRegularization\\nFei-Fei Li & Justin Johnson & Serena Yeung Lecture 7- April 24, 2018 Lecture 7-5\\n4Regularization: reduce overfit\\nIn common use:  L2\\nregularization  L1\\nregularization\\nElastic net (L1 + L2)\\n(Weight decay)\\nLecture 7-Regularization: Dropout\\nIn each forward pass, randomly set some neurons to \\nzero  Probability of dropping is a hyperparameter; 0.5 \\niscommon\\nFei-Fei Li & Justin Johnson & Serena Yeung Lecture 7- April 24, 2018 Lecture 7-April 25,  2019 Fei-Fei Li & Justin Johnson & Serena  Yeung 56\\nSrivastava et al, “Dropout: A simple way to prevent neural networks from overfitting”, JMLR 2014Dropout\\n•Dropout\\n•Dropout is a solution proposed to this \\nproblem by Nitish Srivastava, Geoffrey \\nHinton and few other students at the \\nUniversity of Toronto in 2012. Hinton is \\nnow an employee at Google, leading to \\nthe giant picking up the patent for the \\ntechnology.\\n•https://patents.google.com/patent/WO2014105\\n866A1/en\\nSample Code\\n•Repository\\n•https://github.com/lkk688/DeepDataMiningLearning\\n•Tensorflow introduction\\n•https://colab.research.google.com/drive/188d4pSon4mSAzhGG54zXjWctTOo7D\\ns53?usp=sharing\\n•Pytorch introduction\\n•https://colab.research.google.com/drive/1KZKXqa8FkaJpruUl1XzE7vjvb4pHfMo\\nS?usp=sharing\\n5859Thank \\nYou\\nAddress:\\nENG257, SJSUEmail Address:\\nKaikai.liu@sjsu.eduCMPE 258 -01 \\nDeep Learning\\nDr. Kaikai Liu, Ph.D. Associate Professor\\nDepartment of Computer Engineering\\nSan Jose State University \\nEmail: kaikai.liu@sjsu.edu\\nWebsite: https://www.sjsu.edu/cmpe/faculty/tenure -\\nline/kaikai -liu.phpSpring 2024Neural Network Training\\n•Backpropagation\\nℎ!\"#ℎ!Upstream gradient: \\n$%\\n$&! \\nDownstream gradient: \\n$%\\n$&!\"#=$%\\n$&!$&!\\n$&!\"#𝑓#𝑤!𝜕𝑒\\n𝜕𝑤!=𝜕𝑒\\n𝜕ℎ!𝜕ℎ!\\n𝜕𝑤!\\n𝜕ℎ!\\n𝜕ℎ!\"#𝜕ℎ!\\n𝜕𝑤!Local gradient\\nLocal gradientParameter update:\\nForward pass\\nBackward passNeural Network Training\\n•With branches\\n•E.g., Resnet\\nℎ!\"#ℎ!,#Upstream gradient: \\n$%\\n$&!,# \\nDownstream gradient: \\n$%\\n$&!\"#=$%\\n$&!$&!\\n$&!\"#𝑓#𝑤!𝜕𝑒\\n𝜕𝑤!=𝜕𝑒\\n𝜕ℎ!𝜕ℎ!\\n𝜕𝑤!\\n𝜕ℎ!\\n𝜕ℎ!\"#𝜕ℎ!\\n𝜕𝑤!Local gradient\\nLocal gradientParameter update:\\n$%\\n$&!,% ℎ!,(+ Gradients add at \\nbranchesBackpropagation\\n•Backpropagation is an algorithm \\nthat allows us to calculate a very \\ncomplicated gradient, like the one \\nwe need. \\n•The parameters of the neural network \\nare adjusted according to the following \\nformulae.\\n•α represents learning rate -a \\nhyperparameter which allows you to control \\nthe value of performed adjustment. \\nBackpropagation\\n•Simple example\\ne.g. x = -2, y = 5, z =-4\\nWant:\\ne.g. x = -2, y = 5, z =-4\\nWant:\\nBackpropagation: \\na simple examplee.g. x = -2, y = 5, z =-4\\nWant:\\nBackpropagation: \\na simple examplee.g. x = -2, y = 5, z =-4\\nWant:\\nBackpropagation: \\na simple examplee.g. x = -2, y = 5, z =-4\\nChain rule:\\nWant: Upstream  \\ngradientLocal  \\ngradientBackpropagation: \\na simple  exampleChain rule:\\ne.g. x = -2, y = 5, z =-4\\nBackpropagation: \\na simple example\\nWant:Upstream  \\ngradientLocal  \\ngradientJacobian Matrix\\n•In vector calculus, the Jacobian matrix of a vector -valued function of \\nseveral variables is the matrix of all its first -order partial derivatives\\n•The Jacobian matrix collects all first -order partial derivatives of a \\nmultivariate function that can be used for backpropagation.\\nNeural Network Training\\n•Gradient\\n•Dealing with vectors: Jacobian\\n𝑥 𝑧\\n$%\\n$) 𝑓(𝑥)𝜕𝑧\\n𝜕𝑥\\n𝑁×𝑀 \\nJacobian\\n1×𝑀 1×𝑁 \\n1×𝑁 =𝜕𝑧(#)\\n𝜕𝑥(#)…𝜕𝑧(#)\\n𝜕𝑥(&)\\n⋮ ⋱ ⋮\\n𝜕𝑧(\\')\\n𝜕𝑥(#)…𝜕𝑧(\\')\\n𝜕𝑥(&)\\n𝜕𝑒\\n𝜕𝑥\\t=𝜕𝑒\\n𝜕𝑧\\t𝜕𝑧\\n𝜕𝑥\\n𝑁×𝑀 1×𝑁 1×𝑀 Jacobian: row indices \\ncorrespond to outputs, \\ncolumn indices \\ncorrespond to inputs. \\nThe 𝑖,𝑗th element of \\nthe Jacobian is the \\npartial derivative of the \\n𝑖th output w.r.t. 𝑗th \\ninput\\nNeural Network Training\\n•Simple case: Elementwise operation ( ReLU layer)\\n𝑥 𝑧\\n$%\\n$) 𝑓(𝑥)=max(0,𝑥)𝜕𝑧\\n𝜕𝑥\\n𝑀×𝑀 \\nJacobian\\n1×𝑀 1×𝑀 \\n1×𝑀 =𝜕𝑧(#)\\n𝜕𝑥(#)…𝜕𝑧(#)\\n𝜕𝑥(&)\\n⋮ ⋱ ⋮\\n𝜕𝑧(&)\\n𝜕𝑥(#)…𝜕𝑧(&)\\n𝜕𝑥(&)\\n𝜕𝑒\\n𝜕𝑥\\t=𝜕𝑒\\n𝜕𝑧\\t𝜕𝑧\\n𝜕𝑥\\n1×𝑀 1×𝑀 𝑀×𝑀 What does the Jacobian for \\nan elementwise \\nfunction look like?Neural Network Training\\n•Simple case: Elementwise operation ( ReLU layer)\\n𝑥 𝑧\\n$%\\n$) 𝑓(𝑥)=max(0,𝑥)𝜕𝑧\\n𝜕𝑥\\n𝑀×𝑀 \\nJacobian\\n1×𝑀 1×𝑀 \\n1×𝑀 =𝜕𝑧(#)\\n𝜕𝑥(#)… 0\\n⋮ ⋱ ⋮\\n0 …𝜕𝑧(&)\\n𝜕𝑥(&)\\n𝜕𝑒\\n𝜕𝑥\\t=𝜕𝑒\\n𝜕𝑧\\t𝜕𝑧\\n𝜕𝑥\\n1×𝑀 1×𝑀 𝑀×𝑀 What does the Jacobian for \\nan elementwise \\nfunction look like?PyTorch\\n•PyTorch\\n•originally developed by Meta AI and now part of the Linux Foundation umbrella\\n•It is free and open -source software released under the modified BSD license. Although the Python \\ninterface is more polished and the primary focus of development, PyTorch also has a C++ interface.\\n•Meta operates both PyTorch and Convolutional Architecture for Fast Feature Embedding (Caffe2), but \\nmodels defined by the two frameworks were mutually incompatible. The Open Neural Network \\nExchange (ONNX) project was created by Meta and Microsoft in September 2017 for converting \\nmodels between frameworks. Caffe2 was merged into PyTorch at the end of March 2018. In \\nSeptember 2022, Meta announced that PyTorch would be governed by PyTorch Foundation, a newly \\ncreated independent organization –a subsidiary of Linux Foundation.\\n•PyTorch is a Python package that provides two high -level features:\\n•Tensor computation (like NumPy) with strong GPU acceleration\\n•Deep neural networks built on a tape -based autograd system\\n•You can reuse your favorite Python packages such as NumPy, SciPy, and Cython to \\nextend PyTorch when needed.\\n•A replacement for NumPy to use the power of GPUs.\\n•A deep learning research platform that provides maximum flexibility and speed.PyTorch\\n•PyTorch is a library that consists of the following components:\\nComponent Description\\ntorch A Tensor library like NumPy, with strong GPU support\\ntorch.autogradA tape -based automatic differentiation library that supports all differentiable Tensor \\noperations in torch\\ntorch.jitA compilation stack (TorchScript) to create serializable and optimizable models \\nfrom PyTorch code\\ntorch.nnA neural networks library deeply integrated with autograd designed for maximum \\nflexibility\\ntorch.multiprocessingPython multiprocessing, but with magical memory sharing of torch Tensors across \\nprocesses. Useful for data loading and Hogwild training\\ntorch.utils DataLoader and other utility functions for conveniencePyTorch\\n•PyTorch\\n•A GPU -Ready Tensor Library: If you use NumPy, then you have used Tensors (a.k.a. \\nndarray ).\\n•PyTorch provides Tensors that can live either on the CPU or the GPU and accelerates \\nthe computation by a huge amount. A wide variety of tensor routines to accelerate and fit \\nyour scientific computation needs such as slicing, indexing, mathematical operations, \\nlinear algebra, reductions. \\n•Integrate acceleration libraries such as Intel MKL and NVIDIA ( cuDNN , NCCL) to \\nmaximize speed. At the core, its CPU and GPU Tensor and neural network backends are \\nmature and have been tested for years.\\nNVIDIA AI Chips\\n•The NVIDIA A100 (based on NVIDIA Ampere architecture) is \\nspecifically tuned toward AI and high -performance computation instead \\nof rendering 3D frames quickly for gaming.\\n•Most 8x NVIDIA A100 systems, especially at larger cloud service providers, use \\na special NVIDIA -only form factor called SXM4\\n•Each of these SXM4 A100’s is not sold as a single unit. Instead, they are sold in \\neither 4 or 8 GPU subsystems\\n2x NVIDIA A100 PCIe With NVLink  Bridges Installed\\n NVIDIA HGX A100 8 GPU AssemblyNVIDIA AI Chips\\n•The A100 is based on GA100 and has 108 SMs.\\n•The NVIDIA GA100 GPU is composed of multiple GPU Processing Clusters \\n(GPCs), Texture Processing Clusters (TPCs), Streaming Multiprocessors (SMs), \\nand HBM2 memory controllers. \\nNVIDIA AI Chips\\n•NVIDIA H100\\n•The NVIDIA H100 GPU with SXM5 board \\nform-factor includes the following units:\\n•8 GPCs, 66 TPCs, 2 SMs/TPC, 132 SMs per GPU\\n•128 FP32 CUDA Cores per SM, 16896 FP32 \\nCUDA Cores per GPU\\n•4 fourth -generation Tensor Cores per SM, 528 per \\nGPU\\n•80 GB HBM3, 5 HBM3 stacks, 10 512 -bit memory \\ncontrollers\\n•Fourth -generation NVLink and PCIe Gen 5\\nPyTorch\\n•PyTorch Introduction in \\nhttps://github.com/lkk688/DeepDataMiningLearning\\n•Pytorch Introductions, Tensors, and Autograd :colablink\\n•Pytorch Regression and Logistic Regression: colablink\\n•Pytorch Simple Neural Networks: colabPyTorch\\n•Dynamic Neural Networks: Tape -Based \\nAutograd\\n•PyTorch has a unique way of building neural \\nnetworks: using and replaying a tape \\nrecorder.\\n•Most frameworks such as TensorFlow, \\nTheano, Caffe, and CNTK have a static view \\nof the world. One has to build a neural \\nnetwork and reuse the same structure again \\nand again. Changing the way the network \\nbehaves means that one has to start from \\nscratch.\\n•With PyTorch , we use a technique called \\nreverse -mode auto -differentiation, which \\nallows you to change the way your network \\nbehaves arbitrarily with zero lag or \\noverhead. \\nWhile this technique is not unique to PyTorch , it\\'s \\none of the fastest implementations of it to date. Deep Learning Frameworks\\n•Tensorflow : https://www.tensorflow.org\\n•It was developed by the Google Brain team for Google\\'s internal use in research and production\\n•As TensorFlow\\'s market share among research papers was declining to the advantage of PyTorch , the TensorFlow \\nTeam announced a release of a new major version of the library in September 2019. TensorFlow 2.0 introduced \\nmany changes, the most significant being TensorFlow eager, which changed the automatic differentiation scheme \\nfrom the static computational graph, to the \"Define -by-Run\" scheme originally made popular by Chainer and later \\nPyTorch\\n•Keras abstractions.\\n•Jax: Composable transformations of Python+NumPy programs: differentiate, vectorize, JIT to \\nGPU/TPU, and more\\n•https://github.com/google/jax\\n•use Flax ( https://github.com/google/flax ) on top of JAX, which is a neural network library developed by Google. It \\ncontains many ready -to-use deep learning modules, layers, functions, and operations\\n•Flax is a neural network library for JAX that is designed for flexibility.\\n•Apple MLX: An array framework for Apple silicon\\n•https://github.com/ml -explore/mlx\\n•MLX has a Python API that closely follows NumPy. MLX also has a fully featured C++ API, which closely mirrors \\nthe Python API. MLX has higher -level packages like mlx.nn and mlx.optimizers with APIs that closely follow \\nPyTorch to simplify building more complex models.26Thank \\nYou\\nAddress:\\nENG257, SJSUEmail Address:\\nKaikai.liu@sjsu.eduCMPE 258 -01 \\nDeep Learning\\nDr. Kaikai Liu, Ph.D. Associate Professor\\nDepartment of Computer Engineering\\nSan Jose State University \\nEmail: kaikai.liu@sjsu.edu\\nWebsite: https://www.sjsu.edu/cmpe/faculty/tenure -\\nline/kaikai -liu.phpSpring 2024Project Key Components\\n•Build a full pipeline of Deep Learning application with model training\\n•Question/Problem Formulation: propose your own application and formulate the problem\\n•Data Acquisition and Processing: \\n•Identify a Suitable Dataset for Model Training and Evaluation\\n•State -of-the-Art Models\\n•Analyze the data, assess and compare various state -of-the-art open source models\\n•Model architecture change, Training, Evaluation, Fine -tunning\\n•Change the model architecture, tune parameters, and proceed with model training/fine -tunning, and \\nevaluation. Gain the insights of the performance impact from the model architecture and parameters.\\n•Inference, Optimization, and Real -time test\\n•Create a comprehensive end -to-end deep learning application to enable inference using the trained model. \\n•Depending on the chosen hardware platform, optimize model inference to enhance speed or reduce \\ncomputational costs. \\n•Perform evaluations and visualizations using real test data.\\n2\\nQuestion/Problem \\nFormulationData Acquisition \\nand ProcessingModel Training, \\nEvaluation, and \\nFine-tunningInference, \\nOptimization, and \\nReal-time testCommon Datasets\\n•NLP: Question -Answering, Semantic Search, Text Translation\\n•SQuAD (Stanford Question Answering Dataset): A dataset for reading comprehension tasks, \\nwhere models answer questions based on a given passage.\\n•Machine Translation (WMT19).\\n•Audio: Speech Recognition, Translation\\n•LibriSpeech :A dataset of spoken words and sentences collected from audiobooks.\\n•Mozilla Common Voice: A crowdsourced dataset of speech data for various languages.\\n•Vision: Autonomous Driving: 2D/3D object detection, tracking, scene \\nunderstanding\\n•KITTI Vision Benchmark Suite (https:// www.cvlibs.net /datasets/ kitti/):A dataset for tasks like \\nobject detection, tracking, and scene understanding in autonomous driving scenarios.\\n•Waymo open dataset: https://waymo.com/open/\\n•Argo open dataset: https://www.argoverse.org/\\n•NuScenes : https://www.nuscenes.org/PyTorch\\n•PyTorch Introduction in \\nhttps://github.com/lkk688/DeepDataMiningLearning\\n•Pytorch Introductions, Tensors, and Autograd :colablink\\n•Pytorch Regression and Logistic Regression: colablink\\n•Pytorch Simple Neural Networks: colabPyTorch\\n•Dynamic Neural Networks: Tape -Based \\nAutograd\\n•PyTorch has a unique way of building neural \\nnetworks: using and replaying a tape \\nrecorder.\\n•Most frameworks such as TensorFlow, \\nTheano, Caffe, and CNTK have a static view \\nof the world. One has to build a neural \\nnetwork and reuse the same structure again \\nand again. Changing the way the network \\nbehaves means that one has to start from \\nscratch.\\n•With PyTorch , we use a technique called \\nreverse -mode auto -differentiation, which \\nallows you to change the way your network \\nbehaves arbitrarily with zero lag or \\noverhead. \\nWhile this technique is not unique to PyTorch , it\\'s \\none of the fastest implementations of it to date. Deep Learning Frameworks\\n•Tensorflow : https://www.tensorflow.org\\n•It was developed by the Google Brain team for Google\\'s internal use in research and production\\n•As TensorFlow\\'s market share among research papers was declining to the advantage of PyTorch , the TensorFlow \\nTeam announced a release of a new major version of the library in September 2019. TensorFlow 2.0 introduced \\nmany changes, the most significant being TensorFlow eager, which changed the automatic differentiation scheme \\nfrom the static computational graph, to the \"Define -by-Run\" scheme originally made popular by Chainer and later \\nPyTorch\\n•Keras abstractions.\\n•Jax: Composable transformations of Python+NumPy programs: differentiate, vectorize, JIT to \\nGPU/TPU, and more\\n•https://github.com/google/jax\\n•use Flax ( https://github.com/google/flax ) on top of JAX, which is a neural network library developed by Google. It \\ncontains many ready -to-use deep learning modules, layers, functions, and operations\\n•Flax is a neural network library for JAX that is designed for flexibility.\\n•Apple MLX: An array framework for Apple silicon\\n•https://github.com/ml -explore/mlx\\n•MLX has a Python API that closely follows NumPy. MLX also has a fully featured C++ API, which closely mirrors \\nthe Python API. MLX has higher -level packages like mlx.nn and mlx.optimizers with APIs that closely follow \\nPyTorch to simplify building more complex models.TORCH.AUTOGRAD\\n•torch.autograd is PyTorch’s automatic differentiation engine that powers \\nneural network training.\\n•Training a NN happens in two steps:\\n•Forward Propagation: In forward prop, the NN makes its best guess about the correct \\noutput. It runs the input data through each of its functions to make this guess.\\n•Backward Propagation: In backprop, the NN adjusts its parameters proportionate to the \\nerror in its guess. It does this by traversing backwards from the output, collecting the \\nderivatives of the error with respect to the parameters of the functions (gradients), and \\noptimizing the parameters using gradient descent. \\n•We use the model’s prediction and the corresponding label to calculate the error (loss). \\nThe next step is to backpropagate this error through the network. Backward propagation \\nis kicked off when we call .backward() on the error tensor. Autograd then calculates and \\nstores the gradients for each model parameter in the parameter’s .grad attribute.\\nTORCH.AUTOGRAD\\n•Next, we load an optimizer, in this case SGD with a learning rate of \\n0.01 and momentum of 0.9. We register all the parameters of the \\nmodel in the optimizer.\\n•Finally, we call .step() to initiate gradient descent. The optimizer \\nadjusts each parameter by its gradient stored in .grad.\\n•If we create two tensors a and b with requires_grad =True . This \\nsignals to autograd that every operation on them should be tracked.\\nComputational Graph\\n•Conceptually, autograd keeps a record of data (tensors) & all executed operations (along \\nwith the resulting new tensors) in a directed acyclic graph (DAG) consisting of Function \\nobjects. In this DAG, leaves are the input tensors , roots are the output tensors . By \\ntracing this graph from roots to leaves , you can automatically compute the gradients \\nusing the chain rule.\\n•In a forward pass, autograd does two things simultaneously:\\n•run the requested operation to compute a resulting tensor, and\\n•maintain the operation’s gradient function in the DAG.\\n•The backward pass kicks off when .backward() is called on the DAG root. autograd then:\\n•computes the gradients from each . grad_fn ,\\n•accumulates them in the respective tensor’s .grad attribute, and\\n•using the chain rule, propagates all the way to the leaf tensors.\\n•DAGs are dynamic in PyTorch : the graph is recreated from scratch; after each \\n.backward() call, autograd starts populating a new graph. This is exactly what allows you to \\nuse control flow statements in your model; you can change the shape, size and operations \\nat every iteration if needed.Computational Graph\\n•torch.autograd tracks operations on all tensors which have their \\nrequires_grad flag set to True. For tensors that don’t require gradients, \\nsetting this attribute to False excludes it from the gradient computation DAG.\\n•The output tensor of an operation will require gradients even if only a single \\ninput tensor has requires_grad =True .\\n•In a NN, parameters that don’t compute gradients are usually called frozen \\nparameters. It is useful to “freeze” part of your model if you know in advance \\nthat you won’t need the gradients of those parameters (this offers some \\nperformance benefits by reducing autograd computations).\\n•In finetuning, we freeze most of the model and typically only modify the \\nclassifier layers to make predictions on new labels. \\n•The same exclusionary functionality is available as a context manager in \\ntorch.no_grad ()TORCH.AUTOGRAD\\n•TORCH.TENSOR.BACKWARD\\n•Tensor.backward (gradient=None, retain_graph =None, create_graph =False, \\ninputs=None)\\n•https://pytorch.org/docs/stable/generated/torch.Tensor.backward.html\\n•Computes the gradient of current tensor wrtgraph leaves.\\n•The graph is differentiated using the chain rule. If the tensor is non -scalar (i.e. \\nits data has more than one element) and requires gradient , the function \\nadditionally requires specifying gradient. It should be a tensor of matching type \\nand location, that contains the gradient of the differentiated function w.r.t. self.\\n•This function accumulates gradients in the leaves -you might need to zero .grad \\nattributes or set them to None before calling it. TORCH.AUTOGRAD\\n•torch.autograd is an engine for computing vector -Jacobian product.\\nif you pass vᵀas the gradient argument, then y.backward (gradient) will give you \\nnotJbutvᵀ・Jas the result of x.gradPyTorch Workflow\\nPyTorch models\\n•Architecture of a classification model\\n•torch.nn.linear : https://pytorch.org/docs/stable/generated/torch.nn.Linear.html\\nPyTorch models\\n•Architecture of a classification model\\n•torch.nn.linear :\\nPyTorch Training Loop\\nPyTorch Testing Loop\\nEvaluation Methods\\n•Classification evaluation methods\\nOptimizer algorithms\\n•Optimizer algorithms are optimization method that helps improve a \\ndeep learning model’s performance. \\n•While training the deep learning optimizers model, modify each \\nepoch’s weights and minimize the loss function. \\n•An optimizer is a function or an algorithm that adjusts the attributes of the neural \\nnetwork, such as weights and learning rates. \\n•Thus, it helps in reducing the overall loss and improving accuracy. \\n•Gradient Descent Deep Learning Optimizer\\n•Stochastic Gradient Descent Deep Learning Optimizer\\n•Mini-batch Stochastic Gradient Descent\\nLimitations and Problems\\n•The gradients are still noisy because we estimate them based only on a \\nsmall sample of our dataset. The noisy updates might not correlate well with \\nthe true direction of the loss function.\\n•If the loss function has a local minimum or a saddle point, it is very possible \\nthat SGD will be stuck there without being able to “jump out” and proceed in \\nfinding a better minimum. This happens because the gradient becomes zero \\nso there is no update in the weight.\\n•Choosing a good loss function is tricky and requires time -consuming \\nexperimentation with different hyperparameters.\\n•The same learning rate is applied to all of our parameters, which can \\nbecome problematic for features with different frequencies or significance.Optimization: Problems with SGD\\n•What if the loss function has a  \\nlocal minima or saddle point ?\\n•Saddle points much more common \\nin high dimension\\n•Our gradients come from  \\nminibatches so they can benoisy!\\n23\\nA gradient close to zero in a saddle point or in a local \\nminimum does not improve the weight parameters and \\nprevents the whole learning process.\\nSGD +Momentum Optimizer\\n•The momentum term results in the individual gradients having less \\nvariance and thus less zig -zagging: more stable and faster convergence\\n-Build up “velocity” as a running mean ofgradients\\n-Rho gives “friction”; typically rho=0.9 or0.99\\nSGD\\n SGD+Momentum\\nMomentum\\n•Momentum\\n•At every time step, we update our velocity by decaying the previous velocity on a \\nfactor of Friction ρ and we add the gradient of the weights on the current time. \\nThen we update our weights in the direction of the velocity vector.\\n•We can now escape local minimums or saddle points because we keep moving \\ndownwards even though the gradient of the mini -batch might be zero.\\n•Momentum can also help us reduce the oscillation of the gradients because the \\nvelocity vectors can smooth out these highly changing landscapes.\\n•Finally, it reduces the noise of the gradients (stochasticity) and follows a more \\ndirect walk down the landscape.Nesterov momentum\\n•An alternative version of momentum, called Nesterov momentum, \\ncalculates the update direction in a slightly different way.\\n•Instead of combining the velocity vector and the gradients, we \\ncalculate where the velocity vector would take us and compute the \\ngradient at this point. \\n•The most famous algorithm that make us of Nesterov momentum is \\ncalled Nesterov accelerated gradient (NAG)\\nLearning Rate\\n•Learning rate as ahyperparameter\\n•Q: Which one of these learning rates is best to use?\\n•A: All of them! Start with large learning rate and decay over time\\nAdaptive Learning Rate\\n•The other big idea of optimization algorithms is adaptive learning rate. \\nThe intuition is that we’d like to perform smaller updates for frequent \\nfeatures and bigger ones for infrequent ones .\\n•Adagrad\\n•Uses different learning rates for each iteration\\n•It performs smaller updates for parameters associated with frequently occurring \\nfeatures, and larger updates for parameters associated with infrequently \\noccurring features.\\n•When the gradient is changing very fast, the learning rate will be smaller. When \\nthe gradient is changing slowly, the learning rate will be bigger.\\n•Achieve a different learning rate for each parameter (or an adaptive learning \\nrate). 31Thank \\nYou\\nAddress:\\nENG257, SJSUEmail Address:\\nKaikai.liu@sjsu.eduCMPE 258 -01 \\nDeep Learning\\nDr. Kaikai Liu, Ph.D. Associate Professor\\nDepartment of Computer Engineering\\nSan Jose State University \\nEmail: kaikai.liu@sjsu.edu\\nWebsite: https://www.sjsu.edu/cmpe/faculty/tenure -\\nline/kaikai -liu.phpSpring 2024Homework1\\n•Design your own neural network and testing various training options for \\nthe provided sample code\\n•https://github.com/lkk688/DeepDataMiningLearning/blob/main/DeepDataMiningL\\nearning/symboldetection.ipynb\\n•Goal: achieve better results (i.e., low BER)\\n•Individual work\\n•Due: 2/25 11:59PMAdaptive Learning Rate\\n•The other big idea of optimization algorithms is adaptive learning rate. \\nThe intuition is that we’d like to perform smaller updates for frequent \\nfeatures and bigger ones for infrequent ones .\\n•Adagrad\\n•Uses different learning rates for each iteration\\n•It performs smaller updates for parameters associated with frequently occurring \\nfeatures, and larger updates for parameters associated with infrequently \\noccurring features.\\n•When the gradient is changing very fast, the learning rate will be smaller. When \\nthe gradient is changing slowly, the learning rate will be bigger.\\n•Achieve a different learning rate for each parameter (or an adaptive learning \\nrate). Other Optimizers\\n•Adagrad (Adaptive Gradient Descent) Deep Learning Optimizer\\n•Adagrad keeps a running sum of the squares of the gradients in each dimension, \\nand in each update, scale the learning rate based on the sum. \\n•Using the root of the squared gradients: take into account the magnitude of the \\ngradients and not the sign.\\n•Paper: https://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf\\n•We speed up the updating process along the axis with weak gradients by \\nincreasing these gradients along this axis. On the other hand, we slow down the \\nupdates of the weights along the axis with large gradients.\\nthe sum of squared gradients up to the time\\ndivide the current gradient by the root of the term\\nOther Optimizers\\n•Adagrad (Adaptive Gradient Descent) Deep Learning Optimizer\\n•A big drawback of Adagrad is that as time goes by, the learning rate becomes \\nsmaller and smaller due to the monotonic increment of the running squared sum.\\n•The sum of squared gradients become big if the training takes too long. When \\nthe current gradient is divided by this large number, the update step for the \\nweights becomes very small. It is as if we were using a very low learning rate, \\nwhich becomes even lower the longer the training takes. In the worst case, we \\nwould get stuck at AdaGrad and the training would go on forever.Other Optimizers\\n•RMSProp : slight modification of AdaGrad\\n•Root Mean Square Propagation ( RMSProp ) that also maintains per -parameter \\nlearning rates that are adapted based on the average of recent magnitudes of the \\ngradients for the weight \\n•This modification is intended to solve the previously described problem that can \\noccur with AdaGrad . In RMSProp , the running sum of squared gradients g_{t+1} \\nis maintained. However, instead of allowing this sum to increase continuously \\nover the training period, we allow the sum to decrease.\\n•RMSProp keep moving average of the squared gradients for each weight. And \\nthen we divide the gradient by square root the mean square.\\nOther Optimizers\\n•Adam is an adaptive learning rate optimization algorithm that utilises\\nboth momentum and scaling, combining the benefits of RMSProp and \\nSGD w/ thMomentum. \\n•Adam paper: https://arxiv.org/pdf/1412.6980.pdf\\n•The algorithm updates exponential moving averages of the gradient (mt) and \\nthe squared gradient ( vt) where the hyper -parameters β1 , β2 ∈[0, 1) control \\nthe exponential decay rates of these moving averages.\\n•Problem: At the very first time step, mt vtare close to 0, This leads to a very \\nlarge first update step. Solution: ADAM includes a correction clause.\\nRMSProp  + momentumm t: first momentum\\nvt: second momentum\\nadd biases in our \\nmoments in order to \\nforce our algorithm to \\ntake smaller steps in the \\nbeginningOther Optimizers\\n•AdaMax\\n•Adam scales the second moment according to the L2 norm values of the \\ngradient. However, we can extend this principle to use the infinity norm L∞\\n•It has been shown that L∞also provides stable behavior and AdaMax can \\nsometimes have better performance than Adam (especially in models with \\nembeddings).\\n•AdaMax calculates the velocity moment as:\\nOther Optimizers\\n•Nadam\\n•The Nadam (Nesterov -accelerated Adaptive Moment Estimation) algorithm is a \\nslight modification of Adam where vanilla momentum is replaced by Nesterov\\nMomentum.\\n•Nadam generally performs well on problems with very noisy gradients or for \\ngradients with high curvature. It usually provides a little faster training time as \\nwell.\\n•the velocity vector and the update rule remain intact. The new momentum (after \\nadding the bias) is then shaped as:\\nOther Optimizers\\n•AdaBelief\\n•Adabelief is a new Optimization algorithm proposed in 2020 that promises :\\n•Faster training convergence\\n•Greater training stability\\n•Better model generalization\\n•The key idea is to change the step size according to the “belief” in the current gradient \\ndirection.\\n•We enhance Adam by computing the variance of the gradient over time instead of \\nthe momentum squared .\\n•The variance of the gradient is the distance from the expected (believed) gradient.\\n•That way the optimizer now considers the curvature of the loss function. If the observed \\ngradient greatly deviates from the belief, we distrust the current observation and take a \\nsmall step.\\nOther Optimizers\\n•Weight Decay\\n•Weight Decay, or L2 Regularization, is a regularization technique applied to the \\nweights of a neural network. We minimize a loss function compromising both the \\nprimary loss function and a penalty on the L2 Norm of the weights:\\n•AdamW :\\n•Paper: Decoupled Weight Decay Regularization\\n•http://arxiv.org/abs/1711.05101v3\\n•AdamW is a stochastic optimization method that modifies the typical \\nimplementation of weight decay in Adam, by decoupling weight decay from the \\ngradient update .\\nhttps://paperswithcode.com/method/weight -decay\\nw is the rate of weight decayOther Optimizers\\n•Visualizing optimizers and observations\\n•Algorithms with momentum have a smoother trajectory than non -momentum \\nbased but this may result in overshooting.\\n•Methods with an adaptive learning rate have a faster convergence rate, better \\nstability, and less jittering.\\nOther Optimizers\\n•Visualizing optimizers and observations\\n•Algorithms that do not scale the step size (adaptive learning rate) have a harder \\ntime to escape local minimums and break the symmetry of the loss function\\n•Saddle points cause momentum -based methods to oscillate before finding the \\ncorrect downhill path\\nTorch Optim\\n•torch.optim is a package implementing various optimization algorithms.\\n•Optimizer s also support specifying per -parameter options.\\n•contain a params key, containing a list of parameters belonging to it.\\n•Taking an optimization stepoptimizer = optim.SGD (model.parameters (), lr=0.01, momentum=0.9)\\noptimizer = optim.Adam (model.parameters (), lr=0.05)\\noptim.SGD ([\\n                {\\'params\\': model.base.parameters ()},\\n                {\\'params\\': model.classifier.parameters (), \\'lr\\': 1e-3}\\n            ], lr=1e-2, momentum=0.9)This means that model.base ’s parameters will use the \\ndefault learning rate of 1e-2,model.classifier ’s parameters \\nwill use a learning rate of 1e-3, and a momentum of 0.9will \\nbe used for all parameters.\\nfor input, target in dataset:\\n    optimizer.zero_grad ()\\n    output = model(input)\\n    loss = loss_fn (output, target)\\n    loss.backward ()\\n    optimizer.step ()Torch Optim\\n•torch.optim.lr_scheduler provides several methods to adjust the \\nlearning rate based on the number of epochs.\\n•https://pytorch.org/docs/stable/optim.html#module -torch.optim.lr_scheduler\\n•Learning rate scheduling should be applied after optimizer’s update\\noptimizer = optim.SGD (model.parameters (), lr=0.01, \\nmomentum=0.9)\\nscheduler = ExponentialLR (optimizer, gamma=0.9)\\nfor epoch in range(20):\\n    for input, target in dataset:\\n        optimizer.zero_grad ()\\n        output = model(input)\\n        loss = loss_fn (output, target)\\n        loss.backward ()\\n        optimizer.step ()\\n    scheduler.step ()Torch Optim\\n•torch.optim.lr_scheduler provides several methods to adjust the \\nlearning rate based on the number of epochs.\\n•https://pytorch.org/docs/stable/optim.html#module -torch.optim.lr_scheduler\\n•Learning rate scheduling should be applied after optimizer’s update\\nDecay gamma after steps19Thank \\nYou\\nAddress:\\nENG257, SJSUEmail Address:\\nKaikai.liu@sjsu.eduCMPE 258 -01 \\nDeep Learning\\nDr. Kaikai Liu, Ph.D. Associate Professor\\nDepartment of Computer Engineering\\nSan Jose State University \\nEmail: kaikai.liu@sjsu.edu\\nWebsite: https://www.sjsu.edu/cmpe/faculty/tenure -\\nline/kaikai -liu.phpSpring 2024Homework1\\n•Design your own neural network and testing various training options for \\nthe provided sample code\\n•https://github.com/lkk688/DeepDataMiningLearning/blob/main/DeepDataMiningL\\nearning/symboldetection.ipynb\\n•Goal: achieve better results (i.e., low BER)\\n•Individual work\\n•Due: 2/25 11:59PMTorch Optim\\n•torch.optim.lr_scheduler provides several methods to adjust the \\nlearning rate based on the number of epochs.\\n•https://pytorch.org/docs/stable/optim.html#module -torch.optim.lr_scheduler\\n•Learning rate scheduling should be applied after optimizer’s update\\nDecay gamma after stepsTorch Optim\\n•torch.optim.lr_scheduler.StepLR (optimizer, step_size , gamma=0.1, \\nlast_epoch =-1).\\n•https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.StepLR.html\\n•optimizer (Optimizer ) – Wrapped optimizer.\\n•step_size (int) – Period of learning rate decay.\\n•gamma (float ) – Multiplicative factor of learning rate decay. \\nDefault: 0.1.\\n•last_epoch (int) – The index of last epoch. Default: -1.\\n{\\'step_size \\': 30, \\'gamma\\': 0.1}Torch Optim\\n•LambdaLR (optimizer, lr_lambda , last_epoch =-1)\\n•https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.LambdaLR.ht\\nml#torch.optim.lr_scheduler.LambdaLR\\n•optimizer (Optimizer ) – Wrapped optimizer.\\n•lr_lambda (function orlist) – A function \\nwhich computes a multiplicative factor given \\nan integer parameter epoch, or a list of such \\nfunctions, one for each group in \\noptimizer.param_groups .\\n•last_epoch (int) – The index of last epoch. \\nDefault: -1.# Lambda function for learning rate decay\\nlambda_lr  = lambda epoch: final_lr  / initial_lr  + (1 - epoch / \\nnum_epochs ) * (1 - final_lr  / initial_lr )\\nfinal_lr  = 0.01\\ninitial_lr  = 0.1\\nnum_epochs  = 120\\nTorch Optim\\n•ConstantLR (self.opt , factor=0.5, total_iters =4)\\n•https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.ConstantLR.ht\\nml#torch.optim.lr_scheduler.ConstantLR\\n•Decays the learning rate of each parameter group by a small constant factor until \\nthe number of epoch reaches a pre -defined milestone: total_iters . \\n•factor (float ) – The number we multiply \\nlearning rate until the milestone. Default: \\n1./3.\\n•total_iters (int) – The number of steps \\nthat the scheduler decays the learning \\nrate. Default: 5.\\nTorch Optim\\n•LinearLR\\n•https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.LinearLR.html\\n#torch.optim.lr_scheduler.LinearLR\\n•start_factor (float ) – The number we \\nmultiply learning rate in the first \\nepoch. The multiplication factor \\nchanges towards end_factor  in the \\nfollowing epochs. Default: 1./3.\\n•end_factor (float ) – The number we \\nmultiply learning rate at the end of \\nlinear changing process. Default: 1.0.\\n•total_iters (int) – The number of \\niterations that multiplicative factor \\nreaches to 1. Default: 5.{\\'start_factor \\': 0.5, \\'end_factor \\': \\n0.8, \\'total_iters\\' :10}\\nTorch Optim\\n•ExponentialLR\\n•https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.ExponentialLR\\n.html#torch.optim.lr_scheduler.ExponentialLR\\n•Decays the learning rate of each parameter group by gamma every epoch. \\nWhen last_epoch =-1, sets initial lras lr.\\n{\\'gamma\\': 0.5}•gamma (float ) – Multiplicative \\nfactor of learning rate decay.\\n•last_epoch (int) – The index of \\nlast epoch. Default: -1.Torch Optim\\n•MultiplicativeLR (optimizer, lr_lambda =lmbda )\\n•https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.MultiplicativeL\\nR.html#torch.optim.lr_scheduler.MultiplicativeLR\\n•lr_lambda (function or list) –A function which computes a multiplicative factor \\ngiven an integer parameter epoch, or a list of such functions, one for each group \\nin optimizer.param_groups .\\nlmbda = lambda epoch: 0.95Torch Loss Function\\n•torch.nn.L1Loss : The L1 loss function computes the mean absolute \\nerror between each value in the predicted and target tensor. It \\ncomputes the sum of all the values returned from each absolute \\ndifference computation and takes the average of this sum value to \\nobtain the Mean Absolute Error (MAE). \\n•torch.nn.SmoothL1Loss : The smooth L1 loss function combines the \\nbenefits of MSE loss and MAE loss through a heuristic value beta. It \\nuses a squared term if the absolute error falls below one and an \\nabsolute term otherwise. It is less sensitive to outliers than the mean \\nsquare error loss and, in some cases, prevents exploding gradients. \\nIf do square for high values may result in \\nexploding gradientsTorch Loss Function\\n•Mean Squared Error Loss(MSE): torch.nn.MSELoss\\n•The Mean Square Error shares similarities with the Mean Absolute Error. It \\ncomputes the square difference between values in the prediction tensor and that \\nof the target tensor. \\n•By doing so, relatively significant differences are penalized more, while relatively \\nminor differences are penalized less. MSE is considered less robust at handling \\noutliers and noise than MAE.\\nloss = nn.MSELoss(size_average =None, \\nreduce=None, reduction =\\'mean\\')loss = nn.SmoothL1Loss ()loss_fn = nn.L1Loss(size_average =None, reduce=None, \\nreduction =\\'mean\\')Torch CrossEntropy\\n•Cross -entropy ( torch.nn.CrossEntropyLoss ) as a loss function is used to learn the \\nprobability distribution of the data. While other loss functions like squared loss \\npenalize wrong predictions, cross -entropy gives a more significant penalty when \\nincorrect predictions are predicted with high confidence. \\n•CLASS torch.nn.CrossEntropyLoss (weight=None, size_average =None, ignore_index =-100, \\nreduce=None, reduction=\\'mean\\', label_smoothing =0.0)\\n•Computes the cross entropy loss between input logits and target , e.g., classification problem with \\nC classes.\\n•The input is expected to contain the unnormalized logits for each class (which do not need to be \\npositive or sum to 1, in general). input has to be a Tensor of size (C) for unbatched input, (N,C) or \\n(N,C, d1,d2, ...dk) for the K -dimensional case (e.g., computing cross entropy loss per -pixel for 2D \\nimages), N: number of batch.\\n•The target that this criterion expects should contain either: Class indices in the range [0,C) . if \\nignore_index is specified, this loss also accepts this class index (this index may not necessarily \\nbe in the class range). If containing class indices, the target shape should be: shape (), (N) or (N, \\nd1,d2,...dk)Torch CrossEntropy\\n•Computes the cross entropy loss between input logits and target\\n•CLASS torch.nn.CrossEntropyLoss (weight=None, size_average =None, \\nignore_index =-100, reduce=None, reduction=\\'mean\\', label_smoothing =0.0)\\n•Input: unnormalized logits for each class (N,C, d1,d2, ...dk), N: number of batch.\\n•The target: Class indices in the range [0,C), e.g., (N, d1,d2,...dk)\\n•If provided, the optional argument weight should be a 1D Tensor assigning \\nweight to each of the classes: useful when you have an unbalanced training set.\\n•The unreduced (i.e. with reduction set to \\'none\\') loss\\n•If reduction is not \\'none\\' (default \\'mean\\'), then\\nTorch CrossEntropy\\n•Computes the cross entropy loss between input logits and target\\n•CLASS torch.nn.CrossEntropyLoss (weight=None, size_average =None, \\nignore_index =-100, reduce=None, reduction=\\'mean\\', label_smoothing =0.0)\\nloss = nn.CrossEntropyLoss ()\\nN = 4 # batch size\\nC = 5 # number of classes\\ninput = torch.randn(N, C, requires_grad =True)\\ntarget = torch.empty(N, dtype=torch.long).random_(C)\\nTarget tensor([3, 0, 2, 0])\\noutput=loss(input, target)\\noutput.backward ()\\nprint(output) #tensor(1.6370, grad_fn=)Torch Loss Function\\n•Negative Log -Likelihood Loss: torch.nn.NLLLoss\\n•CLASS torch.nn.NLLLoss (weight=None, size_average =None, ignore_index =-\\n100, reduce=None, reduction=\\'mean\\')\\n•The negative log likelihood loss. It is useful to train a classification problem with \\nC classes.\\n•The input given through a forward call is expected to contain log-probabilities of \\neach class . input has to be a Tensor of size either (N, C) or (N, C, d1, d2,..dk). \\nThe latter is useful for higher dimension inputs, such as computing NLL loss per -\\npixel for 2D images.\\n•The target that this loss expects should be a class index in the range [0, C -1]\\nreduction set to \\'none\\'Torch Loss Function\\n•Negative Log -Likelihood Loss: torch.nn.NLLLoss\\n•CLASS torch.nn.NLLLoss (weight=None, size_average =None, ignore_index =-\\n100, reduce=None, reduction=\\'mean\\')\\n•Obtaining log -probabilities in a neural network is easily achieved by adding a \\nLogSoftmax layer in the last layer of your network. You may use \\nCrossEntropyLoss instead, if you prefer not to add an extra layer.\\n•The negative expected value of the log probabilities is the information entropy of \\nan event. Likelihoods are often transformed to the log scale, and can be \\ninterpreted as the degree to which an event supports a statistical model.\\n•Cross-entropy also penalizes wrong but confident predictions and correct but less \\nconfident predictions. In contrast, negative log loss does not penalize according to the \\nconfidence of predictions.Torch Loss Function\\n•Negative Log -Likelihood Loss: torch.nn.NLLLoss\\n•CLASS torch.nn.NLLLoss (weight=None, size_average =None, ignore_index =-\\n100, reduce=None, reduction=\\'mean\\')\\nm = nn.LogSoftmax (dim=1)\\nloss = nn.NLLLoss()\\n# input is of size N x C = 3 x 5\\nN, C = 3, 5\\ninput = torch.randn(N, C, \\nrequires_grad =True)\\ny = m(input)\\ntarget = torch.tensor([1, 0, 4]\\noutput = loss(y, target)N, Num_C = 5, 4 #Num_C number of classes\\nloss = nn.NLLLoss()\\n# input is of size N x C x height x width\\nC, H, W = 3, 10, 10 #image size CHW\\nK = 3\\ndata = torch.randn(N, C, H, W)\\nconv = nn.Conv2d(C, Num_C, (K, K))\\nm = nn.LogSoftmax (dim=1)\\ny = m(conv(data)) #N, Num_C, Hout, Wout\\ntorch.Size ([5, 4, 8, 8])\\ntarget = torch.empty(N, Hout, Wout, \\ndtype=torch.long).random_(0, C)\\noutput = loss(y, target)torch.Size ([5, 8, 8])Torch Loss Function\\n•Binary Cross -Entropy loss\\n•torch.nn.BCELoss is a particular class of Cross -Entropy losses used for the \\nunique problem of classifying data points into only two classes. Labels for this \\ntype of problem are usually binary, and our goal is to push the model to predict a \\nnumber close to zero for a zero label and a number close to one for one label.\\n•Creates a criterion that measures the Binary Cross Entropy between the target \\n(targets yshould be numbers between 0 and 1 ) and the input probabilities\\n•Usually, when using BCE loss for binary classification problems, the neural \\nnetwork\\'s output is a Sigmoid layer to ensure that the output is either a value \\nclose to zero or a value close to one.\\nreduction set to \\'none\\'\\nTorch Loss Function\\n•Binary Cross -Entropy loss\\n•CLASS torch.nn.BCELoss (weight=None, size_average =None, reduce=None, \\nreduction=\\'mean’)\\n•Input: ( ∗), where ∗means any number of dimensions.\\n•Target: ( ∗), same shape as the input.\\nm = nn.Sigmoid()\\nloss = nn.BCELoss()\\ninput = torch.randn(3, requires_grad =True)\\ntarget = torch.empty(3).random_(2)\\ny = m(input)\\ny.size()\\noutput = loss(y, target)\\nprint(output)N, C = 5, 4 \\ndata = torch.randn(N, C)\\nm = nn.Sigmoid()\\ny = m(data) #N, C\\ny.size()\\nloss = nn.BCELoss()\\ntarget = torch.empty(N, C).random_(0, 2)\\noutput = loss(y, target)\\nprint(output)Torch Loss Function\\n•Binary Cross -Entropy Loss with Logits\\n•It adds a Sigmoid layer and the BCELoss in one single class. It provides \\nnumerical stability for log -sum-exp. It is more numerically stable than a plain \\nSigmoid followed by a BCELoss .\\nreduction set to \\'none\\'\\nTorch Loss Function\\n•Binary Cross -Entropy Loss with Logits\\n•It adds a Sigmoid layer and the BCELoss in one single class. It provides \\nnumerical stability for log -sum-exp. It is more numerically stable than a plain \\nSigmoid followed by a BCELoss .\\nN, C = 4, 3 # 3 classes, batch size = 4\\ntarget = torch.ones([N, C], dtype=torch.float32) \\noutput = torch.full([N, C], 1.5) # A prediction (logit)\\npos_weight  = torch.ones([C]) # All weights are equal to 1\\ncriterion  = torch.nn.BCEWithLogitsLoss (pos_weight =pos_weight )\\nloss = criterion (output, target) # -log(sigmoid(1.5))\\nprint(loss) #tensor(0.2014)Beyond Training Error\\nBetter optimization algorithms  \\nhelp reduce training loss\\nBut we really care about error on  \\nnew data -how to reduce thegap?\\nFei-Fei Li & Justin Johnson & Serena Yeung Lecture 7- April 24, 2018 Lecture 7-Overfitting\\n•The gap between the training and validation accuracy indicates the \\namount of overfitting. \\nEarly Stopping: Always dothis\\nIterationLoss\\nIterationAccuracyTrain  \\nVal\\nStop training here\\nFei-Fei Li & Justin Johnson & Serena Yeung Lecture 7- April 24, 2018 Lecture 7-April 25,  2019 Fei-Fei Li & Justin Johnson & Serena  Yeung 25Stop training the model when accuracy on the validation set decreases  \\nOr train for a long time, but always keep track of the model snapshot  \\nthat worked best on valHow to improve single -model performance?\\nRegularization\\nFei-Fei Li & Justin Johnson & Serena Yeung Lecture 7- April 24, 2018 Lecture 7-2\\n6Bias-Variance Tradeoff\\n•We have minimized the error (loss) with respect to \\ntraining data\\n•Low training error does not imply good expected performance: \\nover -fitting\\n•We would like to reason about the expected loss \\n(Prediction Risk) over:\\n•Training Data: {(y1, x1), …, ( yn, xn)}\\n•Test point: (y*, x*)\\n•We will decompose the expected loss into:\\nED,(y⇤,x⇤)⇥\\n(y⇤\\x00f(x⇤|D))2⇤\\n= Noise + Bias2+ Variance•Expanding on the model estimation error:\\n•Completing the squares with\\n•Tradeoff between bias and variance:\\n•Simple Models: High Bias, Low Variance\\n•Complex Models: Low Bias, High VarianceED⇥\\n(h(x⇤)\\x00f(x⇤|D))2⇤\\nE[f(x⇤|D)] = ¯f⇤\\n(Bias)2 VarianceED⇥\\n(h(x⇤)\\x00f(x⇤|D))2⇤\\n=(h(x⇤)\\x00E[f(x⇤|D)])2+E⇥\\n(f(x⇤|D)\\x00E[f(x⇤|D)])2⇤Bias-Variance Tradeoff\\n•Choice of models balances bias \\nand variance.\\n•Over -fitting è  Variance is too High\\n•Under -fitting è Bias is too HighBias Variance Plot\\nImage from http:// scott.fortmann -roe.com /docs/ BiasVariance.htmlRegularization\\n•The bias error is an error from wrong assumptions in the learning \\nalgorithm. High bias can cause an algorithm to miss the relevant \\nrelations between features and target outputs. This is called \\nunderfitting .\\n•The variance is an error from sensitivity to small fluctuations in the \\ntraining set. High variance may result in modeling the random noise in \\nthe training data. This is called overfitting .\\n•The bias -variance tradeoff is a term to describe the fact that we can \\nreduce the variance by increasing the bias. Good regularization \\ntechniques strive to simultaneously minimize the two sources of error. \\nHence, achieving better generalization.Regularization\\n•The most common family of approaches used before the Deep \\nLearning era in estimators such as linear and logistic regression, are \\nparameters norm penalties. Here we add a parameter norm penalty \\nΩ(θ) to the loss function\\n•where θ denotes the trainable parameters, X the input, and y and target labels. \\n•a is a hyperparameter that weights the contribution of the norm penalty, hence \\nthe effect of the regularization.\\n•Two popular methods: L1 and L2 regularization\\nRegularization\\n•L2 regularization, also known as weight decay or ridge regression, \\nadds a norm penalty\\n•The equation effectively shows us that each weight of the weight vector will be \\nreduced by a constant factor on each training step.\\n•Note here that we replaced θ with w. This was due to the fact that usually we \\nregularize only the actual weights of the network and not the biases b.\\n•As a result, we reduce the variance of our model, which makes it easier to \\ngeneralize on unseen data.\\nThe loss function:\\ncompute the gradients:\\nsingle training step and a learning rate λ:Regularization\\n•L1 regularization chooses a norm penalty\\n•The L1 regularizer introduces sparsity in the weights by forcing more \\nweights to be zero instead of reducing the average magnitude of all \\nweights (as the L2 regularizer does). In other words, L1 suggests that \\nsome features should be discarded whatsoever from the training \\nprocess.\\n•The derivative of L2is 2 * weight .\\n•forces the weights to be small but does not make them zero and does non sparse solution\\n•The derivative of L1is k(a constant, whose value is independent of weight).\\n•Can push weights to zero, removing some feature altogether. This works well for feature \\nselection in case we have a huge number of features.\\nRegularization: reduce overfit\\nIn common use:  L2\\nregularization  L1\\nregularization\\nElastic net (L1 + L2)\\n(Weight decay)\\nLecture 7-Weight Decay\\n•L2 regularization, also known as weight decay or ridge regression\\n•CLASS torch.optim.SGD\\n(params ,lr=0.001 ,momentum=0 ,dampening=0 ,weight_decay =0,nesterov =F\\nalse,*,maximize=False ,foreach=None ,differentiable=False )\\n•Manually Add L1 and L2 in loss function•weight_decay (float ,optional ) – weight decay (L2 penalty) (default: 0)Label smoothing\\n•Noise injection is one of the most powerful regularization strategies. By \\nadding randomness, we can reduce the variance of the models and \\nlower the generalization error. \\n•Label smoothing is a way of adding noise at the output targets, aka \\nlabels.\\n•Let’s assume that we have a classification problem. In most of them, we use a \\nform of cross -entropy loss and softmax to output the final probabilities.\\n•The target vector has the form of [0, 1 , 0 , 0]. Because of the way softmax is \\nformulated, it can never achieve an output of 1 or 0. As a result, the model will \\ncontinue to be trained, pushing the output values as high and as low as possible. \\nThe model will never converge. That, of course, will cause overfitting.\\n•To address that, label smoothing replaces the hard 0 and 1 targets by a small \\nmargin. \\n0: 1:where k is the number of classes. CLASS torch.nn.CrossEntropyLoss (weight=None, \\nsize_average =None, ignore_index =-100, reduce=None, \\nreduction=\\'mean\\', label_smoothing =0.0)Dropout\\n•Another strategy to regularize deep neural networks is dropout. Dropout falls \\ninto noise injection techniques and can be seen as noise injection into the \\nhidden units of the network.\\n•In training, some number of layer outputs are randomly ignored (dropped \\nout) with probability p.\\n•During test time, all units are present, but they have been scaled down by p. \\n•This is happening because after dropout, the next layers will receive lower values. In the \\ntest phase though, we are keeping all units so the values will be a lot higher than \\nexpected. That’s why we need to scale them down.\\n•By using dropout, the same layer will alter its connectivity and will search for \\nalternative paths to convey the information in the next layer. As a result, each \\nupdate to a layer during training is performed with a different “view” of the \\nconfigured layer. Conceptually, it approximates training a large number of \\nneural networks with different architectures in parallel.Dropout\\n•\"Dropping\" values means temporarily removing them from the network \\nfor the current forward pass, along with all its incoming and outgoing \\nconnections. Dropout has the effect of making the training process \\nnoisy. The choice of the probability p depends on the architecture.\\nSrivastava et al, “Dropout: A simple way to prevent neural networks from overfitting”, JMLR 2014\\nhttps://www.cs.toronto.edu/~rsalakhu/papers/srivastava14a.pdfDropout\\n•Dropout\\n•Dropout is a solution proposed to this \\nproblem by Nitish Srivastava, Geoffrey \\nHinton and few other students at the \\nUniversity of Toronto in 2012. Hinton is \\nnow an employee at Google, leading to \\nthe giant picking up the patent for the \\ntechnology.\\n•https://patents.google.com/patent/WO2014105\\n866A1/en\\n41Thank \\nYou\\nAddress:\\nENG257, SJSUEmail Address:\\nKaikai.liu@sjsu.eduCMPE 258 -01 \\nDeep Learning\\nDr. Kaikai Liu, Ph.D. Associate Professor\\nDepartment of Computer Engineering\\nSan Jose State University \\nEmail: kaikai.liu@sjsu.edu\\nWebsite: https://www.sjsu.edu/cmpe/faculty/tenure -\\nline/kaikai -liu.phpSpring 2024Dropout\\n•Dropout\\n•Dropout is a solution proposed to this \\nproblem by Nitish Srivastava, Geoffrey \\nHinton and few other students at the \\nUniversity of Toronto in 2012. Hinton is \\nnow an employee at Google, leading to \\nthe giant picking up the patent for the \\ntechnology.\\n•https://patents.google.com/patent/WO2014105\\n866A1/en\\nOther Dropout variations\\n•Inverted dropout also randomly drops some units with a probability p. During training, it \\nalso scales the activations by the inverse of the keep probability 1−p: to prevent the \\nactivations from becoming too large thus the need to modify the network during the testing \\nphase. \\n•Gaussian dropout : is injecting noise to the weights of each unit.\\n•DropConnect : Instead of zeroing out random activations (units), it zeros random weights \\nduring each forward pass. The weights are dropped with a probability of 1−p. This \\nessentially transforms a fully connected layer to a sparsely connected layer. DropConnect  \\ncan be seen as a generalization of Dropout to the full -connection structure of a layer.\\n•Variational Dropout : we use the same dropout mask on each timestep. This means that \\nwe will drop the same network units each time. This was initially introduced for Recurrent \\nNeural Networks\\n•Attention Dropout : randomly dropped certain attention units with a probability p\\n•Embedding Dropout : a strategy that performs dropout on the embedding matrix and is \\nused for a full forward and backward pass.\\n•DropBlock : is used in Convolutional Neural networks and it discards all units in a \\ncontinuous region of the feature map.Convolutional Neural \\nNetworksConvolutional Neural Networks\\n•Neural networks for images \\n•A digital image is a 2D grid of pixels.\\n•A neural network expects a vector of numbers as \\ninput.\\n•The ImageNet challenge\\n•Major computer vision benchmark from 2010 to \\n2017\\n•1.4M images, 1000 classes, image classification\\nConvolutional Neural Networks\\n•Fully -connected structure does not scale to larger images\\n•An image of more respectable size, e.g. 200x200x3, would lead to neurons that have \\n200*200*3 = 120,000 weights\\n•Full connectivity is wasteful and the huge number of parameters would quickly lead to \\noverfitting.\\nFully Connected Layer \\n•Fully Connected Layer \\n•32x32x3 image -> stretch to 3072 x 1\\n•the result of taking a dot product between a row of W and the input (a 3072 -\\ndimensional dot product)\\n3072110 x3072\\nweightsactivation\\n input\\n1number:\\nthe result of taking a dot product  \\nbetween a row of W and the input  \\n(a 3072 -dimensional dot product)1\\n10\\n𝑊𝑥Convolutional Neural Networks\\n•From fully connected to locally connected\\n•Locality: nearby pixels are more strongly correlated\\n•Translation invariance: meaningful patterns can occur anywhere in the image\\nConvolutional Neural Networks\\n•1D Convolution\\n•It endows the neural network with the ability to \\nrecognize particular patterns within the pixels\\n•Convolution is a good way to identify patterns in \\ndata that is directly tied to space or time.\\n•In cross -correlation  the kernel is not flipped \\nleft-to-right before calculating the sliding dot \\nproduct\\n•You can cross -correlate a signal with itself, and \\nthe result is called an autocorrelation\\nConvolutional Neural Networks\\n•From locally connected to convolutional \\n•The kernel slides across the image and produces an output value at each \\nposition \\nThe kernel slides across the image and \\nproduces an output value at each position Receptive Field\\n•The human visual system consists of millions of \\nneurons, where each one captures different \\ninformation. We define the neuron’s receptive field \\nas the patch of the total field of view. In other \\nwords, what information a single neuron has \\naccess to. This is in simple terms the biological \\ncell’s receptive field .\\n•In a deep learning context, the Receptive Field \\n(RF) is defined as the size of the region in the \\ninput that produces the feature\\n•it is a measure of association of an output feature (of any \\nlayer) to the input region (patch)\\n•A convolutional unit only depends on a local region \\n(patch) of the input. That’s why we never refer to the RF \\non fully connected layers since each unit has access to all \\nthe input region.\\nReceptive Field\\n•How can we increase the receptive field in a convolutional network?\\n•Add more convolutional layers (make the network deeper)\\n•Add pooling layers or higher stride convolutions (sub -sampling)\\n•Use dilated convolutions\\n•a 3x3 kernel with a dilation rate of 2 will have the same receptive field as a 5x5 kernel, while \\nonly using 9 parameters.\\n•a 3x3 kernel with a dilation rate of 4 will have the same receptive field as a 9x9 kernel without \\ndilation\\nConvolutional Neural Networks\\n•Valid convolution: output size = input size - kernel size + 1\\n•Variants of the convolution operation\\nFull convolution: output size = \\ninput size + kernel size - 1\\nSame convolution: output size = input size\\nStrided  convolution: kernel slides \\nalong the image with a step > 1Convolutional Neural Networks\\n•Dilated convolution: kernel is spread out, step > 1 between kernel \\nelements\\n•Pooling: compute mean or max over small windows to reduce \\nresolution\\nConvolutional Neural Networks\\n•Convolutional Layer\\n•The CONV layer’s parameters consist of a set of learnable filters. Every filter is \\nsmall spatially (along width and height), but extends through the full depth of the \\ninput volume.\\n•For example, a typical filter on a first layer of a ConvNet  might have size 5x5x3\\n3232x32x3 image  \\n5x5x3 filter\\n32\\n1number:\\nthe result of taking a dot product between the  \\nfilter and a small 5x5x3 chunk of the image\\n(i.e. 5*5*3 = 75-dimensional dot product +bias)\\n3Convolutional Neural Networks\\n•Convolutional Neural Networks (CNNs \\n/ ConvNets )\\n•ConvNet  architectures make the explicit \\nassumption that the inputs are images, \\nwhich allows us to encode certain \\nproperties into the architecture. \\n•Convolutional Neural Networks take advantage of \\nthe fact that the input consists of images and they \\nconstrain the architecture in a more sensible way\\n•These then make the forward function more \\nefficient to implement and vastly reduce the \\namount of parameters in the network\\nfiltering an image with two successive filters made \\nof 4x4x3=48 learnable weights each.Convolutional Neural Networks\\n•7x7 input  (spatially)  assume 3x3  filter\\n•=> 5x5  output\\n7\\n77 7 7Convolutional Neural Networks\\n•7x7 input (spatially)  assume 3x3 filter  applied with stride  2\\n•=> 3x3  output!\\n7\\n77\\n77\\n7Convolutional Neural Networks\\n•Output  size:\\n•(N - F) / stride + 1\\nN\\nNF\\nFe.g. N = 7, F =3:\\nstride 1 => (7 -3)/1 + 1 = 5\\nstride 2 => (7 -3)/2 + 1 = 3\\nstride 3 => (7 -3)/3 + 1 = 2.33 :\\\\Convolutional Neural Networks\\n•In practice: Common to zero pad the  \\nborder\\n•e.g. input  7x7\\n•3x3 filter, applied with stride  1\\n•pad with 1 pixel border => what is the  output?\\n•7x7 output!\\n•in general, common to see CONV layers with  stride 1, \\nfilters of size FxF, and zero-padding  with  (F-1)/2. (will \\npreserve size spatially)\\n•e.g. F = 3 => zero pad with 1  \\nF = 5 => zero pad with 2  F = \\n7 => zero pad with 3000000\\n0\\n0\\n0\\n0Convolutional Neural Networks\\n•Input volume:  32x32 x3\\n•10 5x5 filters with stride 1, pad 2\\n•Output volume  size:\\n•(32+2*2-5)/1+1 = 32 spatially,  so\\n•32x32x 10\\n•Number of parameters in this layer?  each filter has 5*5*3 + 1 (+1 for \\nbias)  = 76 params\\n•=> 76*10 = 760Convolutional Neural Networks\\n•Convolve (slide) over all spatial locations\\n•For example, if we had 6 5x5 filters, we’ll get 6 separate activation \\nmaps\\n•We stack these up to get a “new image” of size 28x28x6!\\n3232\\n332x32x3 image  \\n5x5x3 filter\\nconvolve (slide) over all  \\nspatial locationsactivation maps\\n12828Convolutional Neural Networks\\n•convolve (slide) over all spatial locations\\n•For example, if we had 6 5x5 filters, we’ll get 6 separate activation \\nmaps\\n•We stack these up to get a “new image” of size 28x28x6!\\n3232\\n3Convolution Layeractivation maps\\n62828Convolutional Neural Networks\\n•ConvNet  is a sequence of Convolution Layers, interspersed with \\nactivation functions\\n3232\\n3CONV,  \\nReLU\\ne.g.6\\n5x5x3\\nfilters2828\\n6CONV,  \\nReLU\\ne.g.10  \\n5x5x 6  \\nfiltersCONV,  \\nReLU….\\n102424Convolutional Neural Networks\\n•Pooling  layer\\n-makes the representations smaller and more  manageable\\n-operates over each activation map independently:\\n-MAX  POOLING\\n1124\\n5678\\n3210\\n1234Single depth slice\\nx\\ny\\nmax pool with 2x2 filters  \\nand stride 2 68\\n34Convolutional Neural Networks\\n•Convolutional neural nets\\n•Max pooling: a sliding window applying the MAX operation (typically on 2x2 \\npatches, repeated every 2 pixels)\\nsliding the computing window by 3 pixels results in \\nfewer output values. Strided  convolutions or max \\npooling (max on a 2x2 window sliding by a stride of 2).Convolutional Neural Networks\\n•Torch.nn.conv2d: \\nhttps://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html\\n•In the simplest case, the output value of the layer with input \\nsize (N,Cin,H,W)and output (N,Cout,Hout,Wout)can be precisely described \\nas:\\n•N is a batch size, C denotes a number of channels, H is a height of input planes \\nin pixels, and W is width in pixels.CLASS \\ntorch.nn.Conv2d (in_channels ,out_channels ,kernel_size ,stride=1 ,padding=0 ,dilation=1 ,groups=1 ,bias=True ,pad\\nding_mode =\\'zeros\\' ,device=None ,dtype =None )\\nConvolutional Neural Networks\\n•Torch.nn.conv2d: https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html\\n•stride  controls the stride for the cross -correlation\\n•padding  controls the amount of padding applied to the input. It can be either a \\nstring {‘valid’, ‘same’} or an int / a tuple of ints giving the amount of implicit padding \\napplied on both sides.\\n•padding=\\'valid\\' is the same as no padding. padding=\\'same\\' pads the input so the output has the \\nshape as the input. However, this mode doesn’t support any stride values other than 1.\\n•dilation  controls the spacing between the kernel points\\n•groups  controls the connections between inputs and outputs. in_channels  and \\nout_channels  must both be divisible by groups\\n•At groups=1, all inputs are convolved to all outputs.\\n•When groups == in_channels  and out_channels  == K * in_channels , where K is a positive integer, \\nthis operation is also known as a “ depthwise  convolution”.CLASS \\ntorch.nn.Conv2d (in_channels ,out_channels ,kernel_size ,stride=1 ,padding=0 ,dilation=1 ,groups=1 ,bias=True ,pad\\nding_mode =\\'zeros\\' ,device=None ,dtype =None )Convolutional Neural Networks\\n•Torch.nn.conv2d: \\nhttps://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html\\nCLASS \\ntorch.nn.Conv2d (in_channels ,out_channels ,kernel_size ,stride=1 ,padding=0 ,dilation=1 ,groups=1 ,bias=True ,pad\\nding_mode =\\'zeros\\' ,device=None ,dtype =None )\\nConvolutional Neural Networks\\n•MAXPOOL2D\\n•https://pytorch.org/docs/stable/generated/torch.nn.MaxPool2d.html#torch.nn.Max\\nPool2d\\n•In the simplest case, the output value of the layer with input size (N,C,H,W), \\noutput (N,C,Hout ,Wout )and kernel_size (kH,kW)can be precisely described as:CLASS torch.nn.MaxPool2d (kernel_size ,stride=None ,padding=0 ,dilation=1 ,return_indices =False ,ceil_mode =False )\\nConvolutional Neural Networks\\n•MAXPOOL2D\\n•https://pytorch.org/docs/stable/generated/torch.nn.MaxPool2d.html#torch.nn.Max\\nPool2d\\n•Applies a 2D max pooling over an input signal composed of several input planes.\\n•If padding is non -zero, then the input is implicitly padded with negative infinity on \\nboth sides for padding number of points. dilation controls the spacing between \\nthe kernel points. CLASS torch.nn.MaxPool2d (kernel_size ,stride=None ,padding=0 ,dilation=1 ,return_indices =False ,ceil_mode =False )\\n•kernel_size (Union [int,Tuple [int,int]]) – the size of the window to take a max over\\n•stride (Union [int,Tuple [int,int]]) – the stride of the window. Default value is kernel_size\\n•padding (Union [int,Tuple [int,int]]) – Implicit negative infinity padding to be added on both sides\\n•dilation (Union [int,Tuple [int,int]]) – a parameter that controls the stride of elements in the window\\n•return_indices (bool ) – ifTrue , will return the max indices along with the outputs. Useful \\nfortorch.nn.MaxUnpool2d later\\n•ceil_mode (bool ) – when True, will use ceilinstead of floor to compute the output shapeConvolutional Neural Networks\\n•MAXPOOL2D\\n•https://pytorch.org/docs/stable/generated/torch.nn.MaxPool2d.html#torch.nn.Max\\nPool2d\\nCLASS torch.nn.MaxPool2d (kernel_size ,stride=None ,padding=0 ,dilation=1 ,return_indices =False ,ceil_mode =False )\\n# pool of non -square window  \\nm = nn.MaxPool2d(( 3, 2), stride=( 2, 1))Convolutional Neural Networks\\n•Convolve multiple kernels and obtain multiple feature maps or \\nchannels\\n•The size of kernels is a hyper -parameter specified by the designers of the \\nnetwork architecture. \\nhttps://poloclub.github.io/cnn -explainer/\\nThe ReLU  activation function is specifically \\nused as a non -linear activation functionConvolutional Neural Networks\\n•CNN Explainer: https://poloclub.github.io/cnn -explainer/\\n•CNN 3D visualization: https://adamharley.com/nn_vis/cnn/3d.html\\nLeNet -5 (1998) \\n•Lecun , Y.; Bottou , L.; Bengio , Y.; Haffner, \\nP. Gradient -based learning applied to \\ndocument recognition , Proceedings of \\nthe IEEE 86(11) (1998)\\n•Three main types: Convolutional Layer, Pooling \\nLayer, and Fully -Connected Layer\\n•Stack these layers to form a full \\nConvNet architecture\\nConvolutional Neural Networks\\n•Global average pooling\\n•Instead of using an expensive dense layer at the end of a convolutional neural \\nnetwork, you can split the incoming data \"cube\" into as many parts as you have \\nclasses, average their values and feed these through a softmax  activation function. \\nThis way of building the classification head costs 0 weights. \\n•In Keras , the syntax is tf.keras.layers.GlobalAveragePooling2D()\\nTransfer Learning\\n•Transfer learning\\n•Transfer learning allows you to take a trained model and re -train it to perform \\nanother task. \\n•For example, an image classification model could be retrained to recognize new categories of \\nimage. Re -training takes less time and requires less data than training a model from scratch.\\n45Thank \\nYou\\nAddress:\\nENG257, SJSUEmail Address:\\nKaikai.liu@sjsu.eduCMPE 258 -01 \\nDeep Learning\\nDr. Kaikai Liu, Ph.D. Associate Professor\\nDepartment of Computer Engineering\\nSan Jose State University \\nEmail: kaikai.liu@sjsu.edu\\nWebsite: https://www.sjsu.edu/cmpe/faculty/tenure -\\nline/kaikai -liu.phpSpring 2024Transfer Learning\\n•Transfer learning\\n•Transfer learning allows you to take a trained model and re -train it to perform \\nanother task. \\n•For example, an image classification model could be retrained to recognize new categories of \\nimage. Re -training takes less time and requires less data than training a model from scratch.\\nAlexNet  (2012) \\n•Krizhevsky , A.; Sutskever , I.; Hinton, G.E. \\nImageNet classification with deep convolutional \\nneural networks , Neural Information Processing \\nSystems (2012)\\n•Architecture: 8 layers, ReLU , dropout, weight decay\\n•Input image: 224x224x3\\n•Made up of 5 conv layers. It was the first architecture that \\nemployed max -pooling layers, ReLu  activation functions, \\nand dropout for the 3 enormous linear layers.\\n•Layer1 convolution: 11x11, 96 channels, stride=4, \\n•->56x56x96\\n•ReLU , Max -Pooling (2x2 -> 28x28x96)\\n•Layer 8 fully connected\\nAlexNet\\nAlexNet  (2012) \\n•AlexNet  (2012)\\n•Each layer is a linear classifier by itself\\n•More layers – more nonlinearities\\nAlexNet\\nRegularization Solution\\n•Stochastic Depth: It drops entire network blocks \\nwhile keeping the model intact during testing. The \\nmost popular application is in large ResNets  \\nwhere we bypass certain blocks through their skip \\nconnections.\\n•Early stopping: It refers to the process of stopping \\nthe training when the training error is no longer \\ndecreasing but the validation error is starting to \\nrise.\\n•This implies that we store the trainable parameters \\nperiodically and track the validation error. After the \\ntraining stopped, we return the trainable parameters to \\nthe exact point where the validation error started to rise, \\ninstead of the last ones.\\n•It can also be proven that in the case of a simple linear \\nmodel with a quadratic error function and simple gradient \\ndescent, early stopping is equivalent to L2 regularization.\\nData augmentation\\n•Data augmentation\\n•Data augmentation refers to the process of generating new training examples to \\nour dataset. More training data means lower model’s variance, a.k.a  lower \\ngeneralization error. Simple as that. It can also be seen as a form of noise \\ninjection in the training dataset.\\n•Data augmentation can be achieved in many different ways.\\n•Basic Data Manipulations: Image flipping, cropping, rotations, translations, image color \\nmodification, image mixing, cutout, Mixup , cutmix\\n•Feature Space Augmentation: we can apply transformations on the feature space. For \\nexample, an autoencoder might be used to extract the latent representation. Noise can then be \\nadded in the latent representation which results in a transformation of the original data point.\\nData augmentation\\n•Data augmentation\\n•Mixup : (ICLR 2018) Zhang et al. in mixup: Beyond Empirical Risk Minimization\\n•Mixup is a data augmentation technique that generates a weighted combination of random image \\npairs from the training data. Given two images and their ground truth labels: a synthetic training \\nexample is generated as:\\n•Cutout: DeVries et al. in Improved Regularization of Convolutional Neural Networks with Cutout\\n•Cutout is an image augmentation and regularization technique that randomly masks out square regions of input \\nduring training. and can be used to improve the robustness and overall performance of convolutional neural \\nnetworks. \\n•simulate occluded examples\\n•CutMix  \\n•Instead of simply removing pixels as in Cutout, we replace the removed regions with a patch from another \\nimage. The ground truth labels are also mixed proportionally to the number of pixels of combined images. The \\nadded patches further enhance localization ability by requiring the model to identify the object from a partial \\nview.\\nIn-layer Normalization \\n•In-layer normalization techniques for training very deep neural \\nnetworks\\n• The batch features are x with a shape of [N, C, H, W].\\n• N will be the batch size, while  H refers to the height, W to the width, and C to \\nthe feature channels\\n•Visualize the 4D activation maps x by merging the spatial dimensions => 3D\\nBatch -Normalization (BN)\\n•Ioffe, S.; Szegedy , C. Batch normalization: Accelerating deep network \\ntraining by reducing internal covariate shift , International conference on \\nmachine learning (2015)\\n•Makes the training of Deep Neural Networks (DNN) faster andmore stable .\\n•It consists of normalizing activation vectors from hidden layers using the first and the \\nsecond statistical moments (mean and variance) of the current batch. This normalization \\nstep is applied right before (or right after) the nonlinear function.\\n•The BN layer first determines the mean 𝜇andthe standard deviation σ of the activation \\nvalues across the batch. It then normalizes the activation vector\\n11\\nBatch -Normalization (BN)\\n•Batch -Normalization (BN) is an algorithmic method which makes the \\ntraining of Deep Neural Networks (DNN) faster andmore stable .\\n•It finally calculates the layer’s output Ẑ(i)by applying a linear transformation \\nwith 𝛾and𝛽, two trainable parameters\\n•Such step allows the model to choose the optimum distribution for each hidden layers, \\nby adjusting those two parameters :\\n•𝛾allows to adjust the standard deviation\\n•𝛽allows to adjust the bias, shifting the curve on the right or on the left side.\\n•At each iteration, the network computes the mean 𝜇 and the standard deviation σ \\ncorresponding to the current batch. Then it trains 𝛾and𝛽 through gradient descent, \\nusing an Exponential Moving Average (EMA) to give more importance to the latest \\niterations\\n•Pytorch :torch.nn.BatchNorm1d ,torch.nn.BatchNorm2d ,torch.nn.BatchNorm3d\\n•Tensorflow  / Keras :tf.nn.batch_normalization ,tf.keras.layers.BatchNormalization\\n12\\nBatch -Normalization (BN)\\n•Batch -Normalization (BN)\\n•The spatial dimensions, as well as the image \\nbatch, are averaged. This way, we concentrate \\nour features in a compact Gaussian -like space\\n•Some advantages of BN:\\n•BN accelerates the training of deep neural networks.\\n•For every input mini -batch we calculate different \\nstatistics. This introduces some sort of regularization \\n(restricts the complexity)\\n•Disadvantages: \\n•Inaccurate estimation of batch statistics with small batch \\nsize\\n•Problems when batch size is varying.\\nLayer normalization\\n•Layer normalization (2016)\\n•In ΒΝ, the statistics are computed across the batch and the spatial dims. In \\ncontrast, in Layer Normalization (LN), the statistics (mean and variance) are \\ncomputed across all channels and spatial dims. Thus, the statistics are \\nindependent of the batch. This layer was initially introduced to handle vectors \\n(mostly the RNN outputs).\\nLayer normalization\\n•Layer normalization (2016)\\n•Become popular when the transformer paper came out\\n•Vectors with batch size of N:\\n•We don’t want to be dependent on the choice of batch and it’s statistics, we \\nnormalize with the mean and variance of each vector.\\nGeneralizing into 4D \\nfeature map tensors\\nInstance Normalization\\n•Instance Normalization: The Missing \\nIngredient for Fast Stylization (2016)\\n•Instance Normalization (IN) is computed only \\nacross the features’ spatial dimensions. So it is \\nindependent for each channel and sample.\\n•Surprisingly, the affine parameters in IN can \\ncompletely change the style of the output image. IN \\ncan normalize the style of each individual sample to \\na target style (modeled by γ and β): global ones \\n(i.e. style information).\\n•One can design a network to model a plethora of \\nfinite styles, which is exactly the case of the so -\\ncalled conditional IN\\nAdaptive Instance Normalization \\n•Adaptive Instance Normalization \\n(2017)\\n•Normalization and style transfer are \\nclosely related.\\n•Adaptive Instance Normalization \\n(AdaIN ) receives an input image x \\n(content) and a style input y, and \\nsimply aligns the channel -wise mean \\nand variance of x to match those of y. \\nMathematically:\\nhttps:// github.com /xunhuang1995/ AdaIN -styleGroup normalization\\n•Group normalization (2018)\\n•Group normalization (GN) divides the channels into groups and computes the \\nfirst-order statistics within each group.\\n•GN’s computation is independent of batch sizes, and its accuracy is more stable \\nthan BN in a wide range of batch sizes.\\nSynchronized Batch Normalization \\n•Synchronized Batch Normalization (2018)\\n•Paper: https://arxiv.org/abs/1803.08904v1\\n•As the training scale went big, some adjustments to BN were necessary. The \\nnatural evolution of BN is Synchronized BN(Synch BN). Synchronized means \\nthat the mean and variance is not updated in each GPU separately.\\n•In multi -worker setups, Synch BN indicates that the mean and standard -deviation \\nare communicated across workers\\n•They first calculate                                   individually on each device. Then the \\nglobal sums are calculated by applying the reduce parallel programing technique \\n(https://www.youtube -nocookie.com/embed/prLb1MbAm8M )ImageNet Large Scale Visual Recognition Challenge (ILSVRC)\\nwinners\\nLin etal Sanchez &  \\nPerronninKrizhevsky etal  \\n(AlexNet)Zeiler &  \\nFergusSimonyan &Szegedy etal  \\nZisserman (VGG) (GoogLeNet)He et al  \\n(ResNet)Russakovsky etal Shao etal Hu etal  \\n(SENet)shallow 8layers 8layers19layers 22layers152layers 152layers 152layers\\nFirst CNN -based winner\\nTop5 ErrorVGGNet  (2014)\\n•Simonyan , K.; Zisserman, A., Very deep \\nconvolutional networks for large -scale image \\nrecognition , International Conference on \\nLearning Representations (2015)\\n•Stack many convolutional layers before pooling \\n•Use “same” convolutions (3x3) to avoid resolution \\nreduction\\n•Up to 19 layers\\n•The main principle is that a stack of three 3 ×3 conv. \\nlayers are similar to a single 7 ×7 layer. And maybe \\neven better! Because they use three non -linear \\nactivations in between (instead of one), which makes \\nthe function more discriminative.\\n•3∗(32)C2=27×C2weights, compared to a 7 ×7 conv. layer \\nthat would require 1*(72) C2= 49 C2parameters (81% \\nmore)\\nVGG16 VGG19\\nVGG16\\n•VGG16\\n•the input is a 224x224x3 tensor (that means a \\n224x224 pixel RGB image)\\n23\\nVGG16\\n24Thank \\nYou\\nAddress:\\nENG257, SJSUEmail Address:\\nKaikai.liu@sjsu.eduCMPE 258 -01 \\nDeep Learning\\nDr. Kaikai Liu, Ph.D. Associate Professor\\nDepartment of Computer Engineering\\nSan Jose State University \\nEmail: kaikai.liu@sjsu.edu\\nWebsite: https://www.sjsu.edu/cmpe/faculty/tenure -\\nline/kaikai -liu.phpSpring 2024VGGNet  (2014)\\n•Simonyan , K.; Zisserman, A., Very deep \\nconvolutional networks for large -scale image \\nrecognition , International Conference on \\nLearning Representations (2015)\\n•Stack many convolutional layers before pooling \\n•Use “same” convolutions (3x3) to avoid resolution \\nreduction\\n•Up to 19 layers\\n•The main principle is that a stack of three 3 ×3 conv. \\nlayers are similar to  a single 7 ×7 layer. And maybe \\neven better! Because they use three non -linear \\nactivations in between (instead of one), which makes \\nthe function more discriminative.\\n•3∗(32)C2=27×C2weights, compared to a 7 ×7 conv. layer \\nthat would require 1*(72) C2= 49 C2parameters (81% \\nmore)\\nVGG16 VGG19\\nVGG16\\n•VGG16\\n•the input is a 224x224x3 tensor (that means a \\n224x224 pixel RGB image)\\n3\\nVGG16\\nRepVGG\\n•RepVGG : Making VGG -style ConvNets  \\nGreat Again\\n•Paper (2021): https://arxiv.org/abs/2101.03697\\n•It is challenging for a plain model to reach a \\ncomparable level of performance as the multi -\\nbranch architectures\\n•Since the benefits of multi -branch architecture \\nare all for training and the drawbacks are \\nundesired for inference, we propose to \\ndecouple the training -time multi -branch and \\ninference -time plain architecture via structural \\nre-parameterization, which means converting \\nthe architecture from one to another via \\ntransforming its parameters. \\nCase Study:  GoogLeNet\\n•GoogLeNet : 22 layers\\n•ILSVRC’14 classification winner  (6.7% top 5 error)\\n•part of these layers are a total of 9 inception modules\\n•The input layer of the GoogLeNet  architecture takes \\nin an image of the dimension 224 x 224\\n•Efficient “Inception” module: Multi -scale \\nfeature extraction\\n•“Inception module”: design a good local network \\ntopology (network within a network) and then stack \\nthese modules on top of each other\\n•using kernels of various sizes in parallel and thus \\nincreasing the width of the model instead of depth\\n•extract both bigger and smaller features \\nsimultaneously\\n6\\nNaive Inception module\\nInception module \\nwith dimension  \\nreduction (1x1 conv \\n“bottleneck”  layers)Case Study: GoogLeNet\\nNaive Inception module\\nQ: What is the problem with this?  \\n[Hint: Computational complexity]\\nLecture 9-Example:\\nModule input:  \\n28x28x256Q3:What is output sizeafter  \\nfilter concatenation?\\n28x28x128 28x28x192 28x28x96 28x28x25628x28x(128+192+96+256) =28x28x672Conv Ops:\\n[1x1 conv, 128] 28x28x128x1x1x256  \\n[3x3 conv, 192] 28x28x192x3x3x256  \\n[5x5 conv, 96] 28x28x96x5x5x256  \\nTotal: 854M ops\\nVery expensive compute\\nPooling layer also preserves feature  \\ndepth, which means total depth after  \\nconcatenation can only grow at every  \\nlayer!Reminder: 1x1 convolutions\\n641x1CONV\\nwith 32 filters32\\n(each filter has size  \\n1x1x64, and performs a  \\n64-dimensional dot  \\nproduct)\\nProjects depth to lower  dimension (combination  of  \\nfeature  maps)Preserves spatial  dimensions, reduces depth!Case Study: GoogLeNet\\nInception module with dimension reductionUsing same parallel layers as  \\nnaive example, and adding “1x1  \\nconv, 64 filter” bottlenecks:\\nModule input:  \\n28x28x256\\n28x28x64 28x28x64 28x28x25628x28x128 28x28x192 28x28x96 28x28x64Conv Ops:\\n[1x1 conv, 64] 28x28x64x1x1x256  \\n[1x1 conv, 64] 28x28x64x1x1x256  \\n[1x1 conv, 128] 28x28x128x1x1x256  \\n[3x3 conv, 192] 28x28x192x3x3x64  \\n[5x5 conv, 96] 28x28x96x5x5x64  \\n[1x1 conv, 64] 28x28x64x1x1x256  \\nTotal: 358M ops\\nCompared to 854M ops for naive version  \\nBottleneck can also reduce depth after  \\npooling layer28x28x480Case Study:  GoogLeNet\\n•GoogLeNet : classifier\\n•The average pooling layer takes a mean across all the feature maps \\nproduced by the last inception module and reduced the input height and \\nwidth to 1x1\\n•A dropout layer(40%) is utilised  just before the linear layer. The dropout \\nlayer is a regularisation  technique that is used during training to prevent \\noverfitting of the network.\\n•Auxilary  Classifiers\\n•One main problem of an extensive network is that they suffer from \\nvanishing gradient descent. \\n•Vanishing gradient descent occurs when the update to the weights that \\narises from backpropagation is negligible within the bottom layers as a \\nresult of relatively small gradient value.\\n•Auxilary  Classifiers are added to the intermediate layers of the \\narchitecture, namely the third(Inception 4[a]) and sixth (Inception4[d])\\n•Auxilary  Classifiers are only utilised  during training and removed during \\ninference. The purpose of an auxiliary classifier is to perform a \\nclassification based on the inputs within the network\\'s midsection and \\nadd the loss calculated during the training back to the total loss of the \\nnetwork.\\n10\\nconsists of an average pool layer, a \\nconv layer, two fully connected layers, \\na dropout layer(70%), and finally a \\nlinear layer with a softmax  activation \\nfunction.Inception Model\\n•Inception V2: Rethinking the Inception Architecture for Computer \\nVision\\n•Factorize 5x5 and 7x7 (in InceptionV3) convolutions to two and three 3x3 \\nsequential convolutions  respectively. This improves computational speed. This is \\nthe same principle as VGG.\\n•They used spatially separable convolutions. Simply, a 3x3 kernel is decomposed \\ninto two smaller ones: a 1x3 and a 3x1 kernel, which are applied sequentially.\\n•They tried to distribute the computational budget in a balanced way between the \\ndepth and width of the network.\\n•They added batch normalization.\\n•Inceprion  V4: https://arxiv.org/abs/1602.07261ResNet  (2015)\\n•He, K. et al., Deep residual \\nlearning for image recognition , \\nIEEE conference on computer vision \\nand pattern recognition (2016)\\n•Very deep networks using residual \\nconnections\\n•152-layer model for ImageNet\\n•ILSVRC’15 classification winner  (3.57% \\ntop 5 error)\\n•Swept all classification and  detection \\ncompetitions in  ILSVRC’15 and \\nCOCO’15!\\n13\\n..\\n.\\nrelu\\nX\\nidentityF(x) +x\\nF(x)relu\\nX\\nResidual blockResNet  (2015)\\n•ResNet\\n•Problem: during gradient descent, as you backprop from the final layer back to \\nthe first layer, you are multiplying by the weight matrix on each step, and thus the \\ngradient can decrease exponentially quickly to zero.\\n•In ResNets , a \"shortcut\" or a \"skip connection\" allows the gradient to be directly \\nbackpropagated to earlier layers\\n•Nowadays, ResNet  architectures have mostly replaced VGG as a base network \\nfor extracting features\\n14\\nResNet  (2015)\\n•Residual connections facilitate \\ntraining deeper networks\\n•Different flavours\\n•ResNet  V2 (bottom) avoids all \\nnonlinearities in the residual \\npathway \\n•Up to 152 layers\\n16Thank \\nYou\\nAddress:\\nENG257, SJSUEmail Address:\\nKaikai.liu@sjsu.eduCMPE 258 -01 \\nDeep Learning\\nDr. Kaikai Liu, Ph.D. Associate Professor\\nDepartment of Computer Engineering\\nSan Jose State University \\nEmail: kaikai.liu@sjsu.edu\\nWebsite: https://www.sjsu.edu/cmpe/faculty/tenure -\\nline/kaikai -liu.phpSpring 2024ResNet  (2015)\\n•Residual connections facilitate \\ntraining deeper networks\\n•Different flavours\\n•ResNet  V2 (bottom) avoids all \\nnonlinearities in the residual \\npathway \\n•Up to 152 layers\\nResNet50\\n•Torchvision  Resnet50\\n•https://github.com/pytorch/vision/blob\\n/main/torchvision/models/resnet.py\\nmodel_resnet50  = get_model (\\'resnet50\\' , \\nweights=\"DEFAULT\" )\\nsummary( model=model_resnet50 , \\ninput_size =(32, 3, 224, 224), # make \\nsure this is \" input_size \", not \\n\"input_shape \"\\n# col_names =[\"input_size \"], # uncomment \\nfor smaller output\\ncol_names =[\"input_size \", \"output_size \", \\n\"num_params \", \"trainable\" ],\\ncol_width =20,\\nrow_settings =[\"var_names \"]\\n) \\nResNet50\\nResNet50\\nResNet50\\nResNet  TorchVision\\n•Resnet50\\nhttps:// github.com /pytorch /vision/blob/main/ torchvision /models/ resnet.pymodel = ResNet (Bottleneck, [3, 4, 6, 3] , **kwargs )\\nmodel.load_state_dict (weights.get_state_dict (progress=progress, check_hash =True))@register_model ()\\n@handle_legacy_interface (weights=(\"pretrained\", ResNet50_Weights.IMAGENET1K_V1))\\ndef resnet50(*, weights: Optional[ResNet50_Weights] = None, progress: bool = True, ** kwargs : Any) -> ResNet :\\nResNet  TorchVision\\n•Resnet50\\nhttps:// github.com /pytorch /vision/blob/main/ torchvision /models/ resnet.pymodel = ResNet (Bottleneck, [3, 4, 6, 3], ** kwargs )\\nmodel.load_state_dict (weights.get_state_dict (progress=progress, check_hash =True))\\ndef _ make_layerResNet  TorchVision\\n•Resnet50\\nhttps:// github.com /pytorch /vision/blob/main/ torchvision /models/ resnet.pymodel = ResNet (Bottleneck, [3, 4, 6, 3], ** kwargs )\\nmodel.load_state_dict (weights.get_state_dict (progress=progress, check_hash =True))\\nBottleneck\\nResNet  TorchVision\\n•ADAPTIVEAVGPOOL2D: Applies a 2D adaptive average pooling over \\nan input signal composed of several input planes.\\n•torch.nn.AdaptiveAvgPool2d (output_size )\\n•The output is of size H x W, for any input size. The number of output features is equal to the \\nnumber of input planes.\\n•TORCH.FLATTEN\\n•torch.flatten (input, start_dim =0, end_dim =-1)\\nself.avgpool  = nn.AdaptiveAvgPool2d((1, 1))\\nself.fc  = nn.Linear (512 * block.expansion , \\nnum_classes )\\nx = self.avgpool (x)\\nx = torch.flatten (x, 1)\\nx = self.fc (x)ResNeXt\\n•ResNeXt\\n•Paper: https://arxiv.org/abs/1611.05431\\n•The authors of ResNeXt  pursue an architecture that \\nexploits Inception’s split -transform -merge strategy \\nwhilst keeping in mind VGG’s philosophy of \\nrepeating blocks and branches with identical \\ntopologies.\\n•A module with many uniform branches (the number \\nof branches is called cardinality)\\n•Each branch would go like this: Shrink the data to 4 \\nchannels, feed them through 3 X 3 convolutions \\nwithout changing the number of channels, and bring \\nthe width back to 256. These 256 -dimensional \\ntensors are summed together, and we utilize skip \\nconnections by adding the input to the result\\n19\\nit has a simple paradigm and only one \\nhyper -parameter to be adjusted, while \\nInception has many hyper -parameters \\n(like the kernel size of the convolutional \\nlayer of each path) to tuneDenseNet\\n•DenseNet  (2016): Densely Connected \\nCNN\\n•Paper: https://arxiv.org/abs/1608.06993\\n•The input of each layer consists of the \\nfeature maps of all earlier layer, and its \\noutput is passed to each subsequent layer. \\nThe feature maps are aggregated with \\ndepth -concatenation.\\n•Other than tackling the vanishing gradients \\nproblem, the authors argue that this \\narchitecture also encourages feature reuse, \\nmaking the network highly parameter -\\nefficient.\\n21\\nSince each layer receives feature maps from \\nall preceding layers, network can be thinner \\nand compact, i.e. number of channels can be \\nfewer . The growth rate kis the additional \\nnumber of channels for each layer.DenseNet\\n•DenseNet  (2016): Densely Connected CNN\\n•Thus, the core idea behind it is feature reuse, which leads to very compact models. As \\na result it requires fewer parameters than other CNNs, as there are no repeated \\nfeature -maps.\\n•There are two concerns here:\\n•The feature maps have to be of the same size.\\n•The concatenation with all the previous feature maps may result in memory explosion.\\n•We have two solutions:\\n•a) use conv layers with appropriate padding that maintain the spatial dims or\\n•b) use dense skip connectivity only inside blocks called Dense Blocks.\\n22\\nThe transition layer can down -\\nsample the image dimensions with \\naverage pooling.SqueezeNet\\n•SqueezeNet : AlexNet -level accuracy with 50x fewer \\nparameters and <0.5MB model size\\n•Paper: https://arxiv.org/pdf/1602.07360.pdf\\n•Architectural Design Strategies: \\n•Strategy 1. Replace 3 ×3 filters with 1 ×1 filters\\n•Strategy 2. Decrease the number of input channels to 3 ×3 filters\\n•Strategy 3. Downsample  late in the network so that convolution layers \\nhave large activation maps -> improve accuracy\\n•Fire Module\\n•comprised of: a squeeze convolution layer (which has only 1 ×1 filters), \\nfeeding into an expand layer that has a mix of 1 ×1 and 3 ×3 convolution \\nfilters.\\n•There are three tunable dimensions (hyperparameters) in a Fire \\nmodule: s1 ×1, e1 ×1, and e3 ×3.\\n•s1×1: The number of 1 ×1 in squeeze layer.\\n•e1×1 and e3 ×3: The number of 1 ×1 and 3 ×3 in expand layer.SqueezeNet\\n•SqueezeNet : AlexNet -level \\naccuracy with 50x fewer \\nparameters and <0.5MB model \\nsize\\n•Paper: \\nhttps://arxiv.org/pdf/1602.07360.pdf\\n•SqueezeNet  (Left): begins with a \\nstandalone convolution layer (conv1), \\nfollowed by 8 Fire modules (fire2 –9), \\nending with a final conv layer \\n(conv10).\\n•SqueezeNet  with simple bypass \\n(Middle) and SqueezeNet  with \\ncomplex bypass (Right): The use of \\nbypass is inspired by ResNet .\\nSqueeze -and-excitation\\n•Squeeze -and-excitation networks (2017)\\n•Hu, J.; Shen, L.; Sun, G., Squeeze -and-excitation networks , IEEE conference on \\ncomputer vision and pattern recognition (2018)\\n•Adding a content aware mechanism to weight each channel adaptively. In it’s most basic \\nform this could mean adding a single parameter to each channel and giving it a linear \\nscalar how relevant each one is.\\n•First, they get a global understanding of each channel by squeezing the feature maps to \\na single numeric value. This results in a vector of size n, where n is equal to the number \\nof convolutional channels. Afterwards, it is fed through a two -layer neural network, which \\noutputs a vector of the same size. These n values can now be used as weights on the \\noriginal features maps, scaling each channel based on its importance.\\nMobileNet\\n•MobileNetV1\\n•Depthwise  Separable Convolution is used \\nto reduce the model size and complexity. \\nIt is particularly useful for mobile and \\nembedded vision applications.\\n•Smaller model size: Fewer number of \\nparameters\\n•Smaller complexity: Fewer Multiplications \\nand Additions (Multi -Adds)\\n•MobileNet , a smaller and efficient \\nnetwork architecture optimized for speed, \\nhas approximately 3.3M parameters, \\nwhile ResNet -152 (yes, 152 layers), once \\nthe state of the art in the ImageNet \\nclassification competition, has around \\n60M.\\n28\\nMobileNet  V2\\n•MobileNetV2 builds upon the ideas from \\nMobileNetV1, using depthwise  separable \\nconvolution as efficient building blocks. \\n•V2 introduces two new features to the \\narchitecture: 1) linear bottlenecks \\nbetween the layers, and 2) shortcut \\nconnections between the bottlenecks. \\n29\\nMobileNet  V2\\n•Inverted Residuals\\n•Residual blocks connect the beginning and end of a convolutional block with a \\nskip connection. By adding these two states the network has the opportunity of \\naccessing earlier activations that weren’t modified in the convolutional block. This \\napproach turned out to be essential in order to build networks of great depth.\\n•Anoriginal residual block follows a wide ->narrow ->wide approach concerning \\nthe number of channels. The input has a high number of channels, which are \\ncompressed with an inexpensive 1x1 convolution. That way the following 3x3 \\nconvolution has far fewer parameters . In order to add input and output in the \\nend the number of channels is increased again using another 1x1 convolution.\\n30\\nMobileNet  V2\\n•Inverted Residuals\\n•MobileNetV2 follows a narrow ->wide ->narrow approach. The first step widens \\nthe network using a 1x1 convolution because the following 3x3 depthwise  \\nconvolution already greatly reduces the number of parameters. Afterwards \\nanother 1x1 convolution squeezes the network in order to match the initial \\nnumber of channels.\\n31\\nMobileNet  V2\\n•Linear Bottlenecks\\n•ReLU  discards values that are smaller than 0. This loss of information can be \\ntackled by increasing the number of channels  in order to increase the capacity \\nof the network.\\n•With inverted residual blocks, MobileNetV2 do the opposite and squeeze the \\nlayers where the skip connections are linked. This hurts the performance of the \\nnetwork\\n•Linear bottleneck: the last convolution of a residual block has a linear output \\nbefore it’s added to the initial activations\\n32\\nMobileNet  V2\\n•Other changes\\n•adds Batch Normalization behind every convolutional layer\\n•use ReLU6 instead of ReLU : limits the value of activations to a maximum of 6. \\nThe activation is linear as long as it’s between 0 and 6.\\n•This is helpful when you’re dealing with fixed point inference. It limits the information left of the \\ndecimal point to 3 bits, meaning we have a guaranteed precision right of the decimal point.\\n33\\nThe MobileNetV2 \\narchitecture34Thank \\nYou\\nAddress:\\nENG257, SJSUEmail Address:\\nKaikai.liu@sjsu.eduCMPE 258 -01 \\nDeep Learning\\nDr. Kaikai Liu, Ph.D. Associate Professor\\nDepartment of Computer Engineering\\nSan Jose State University \\nEmail: kaikai.liu@sjsu.edu\\nWebsite: https://www.sjsu.edu/cmpe/faculty/tenure -\\nline/kaikai -liu.phpSpring 2024EfficientNet\\n•EfficientNet : Rethinking Model Scaling for Convolutional Neural \\nNetworks\\n•Paper: https://arxiv.org/abs/1905.11946\\n•EfficientNet  is a convolutional neural network architecture and scaling method \\nthat uniformly scales all dimensions of depth/width/resolution using a compound \\ncoefficient. \\n•uniformly scales network width, depth, and resolution with a set of fixed scaling coefficients.\\nThe base EfficientNet -B0 network is based \\non the inverted bottleneck residual blocks \\nof MobileNetV2, in addition to squeeze -\\nand-excitation blocks.RegNet\\n•Designing Network Design Spaces \\n(2020)\\n•Paper: \\nhttps://arxiv.org/pdf/2003.13678.pdf\\n•https://github.com/facebookresearch/p\\nycls/tree/main\\n•Design space is defined by model \\nbuilding parameters which have its \\nown range, and therefore defines the \\nrange of possible model structures.\\n•by chasing design spaces instead of \\nindividual networks, we can discover \\ngeneral design principles that work across \\ngeneral settings.BiT\\n•Big Transfer ( BiT): General Visual Representation Learning (2020)\\n•Paper: https://arxiv.org/pdf/1912.11370.pdf\\n•We revisit the paradigm of pre -training on large supervised datasets and fine -\\ntuning the model on a target task. We scale up pre -training, and propose a \\nsimple recipe that we call Big Transfer ( BiT). \\n•We transfer BiT to many diverse tasks; with training set sizes ranging from 1 \\nexample per class to 1M total examples. These tasks include ImageNet’s \\nILSVRC -2012, CIFAR -10/100, Oxford -IIIT Pet, Oxford Flowers -102 (including \\nfew-shot variants), and the 1000 -sample VTAB -1k benchmark, which consists of \\n19 diverse datasets. BiT-L attains state -of- the-art performance on many of these \\ntasks, and is surprisingly effective when very little downstream data is available. \\nWe also train BiT-M on the public ImageNet -21k dataset, and attain marked \\nimprovements over the popular ILSVRC -2012 pre -training.Pseudo Labels or self -training\\n•Pseudo Labels methods work by having a pair of networks, one as a teacher \\nand one as a student. The teacher generates pseudo labels on unlabeled \\nimages. These pseudo labeled images are then combined with labeled \\nimages to train the student. Thanks to the abundance of pseudo labeled data \\nand the use of regularization methods such as data augmentation, the \\nstudent learns to become better than the teacher\\n•One main drawback: if the pseudo labels are inaccurate, the student will \\nlearn from inaccurate data. As a result, the student may not get significantly \\nbetter than the teacher. This drawback is also known as the problem of \\nconfirmation bias in pseudo -labeling\\nNoisy Student\\n•Self-training with Noisy Student improves \\nImageNet classification (2020)\\n•Paper: [ link]\\n•First train an EfficientNet  model on labeled ImageNet \\nimages and use it as a teacher to generate pseudo \\nlabels on 300M unlabeled images. Then train a larger \\nEfficientNet  as a student model on the combination of \\nlabeled and pseudo labeled images. \\n•Iterate this process by putting back the student as the \\nteacher. During the generation of the pseudo labels, \\nthe teacher is not noised so that the pseudo labels are \\nas accurate as possible. However, during the learning \\nof the student, we inject noise such as dropout, \\nstochastic depth and data augmentation via \\nRandAugment  to the student so that the student \\ngeneralizes better than the teacher.\\nPseudo -Labels\\n•Meta Pseudo -Labels (2021) -- Google AI, Brain Team\\n•Paper: https://arxiv.org/abs/2003.10580\\n•We present Meta Pseudo Labels, a semi -supervised learning method that \\nachieves a new state -of-the-art top -1 accuracy of 90.2% on ImageNet, \\nwhich is 1.6% better than the existing state -of-the-art. \\n•Like Pseudo Labels, Meta Pseudo Labels has a teacher network to \\ngenerate pseudo labels on unlabeled data to teach a student network. \\nHowever, unlike Pseudo Labels where the teacher is fixed, the teacher in \\nMeta Pseudo Labels is constantly adapted by the feedback of the student\\'s \\nperformance on the labeled dataset. As a result, the teacher generates \\nbetter pseudo labels to teach the student. \\nKnowledge Distillation\\n•Knowledge Distillation and \\nLabel Smoothing. \\n•The teacher in Meta Pseudo \\nLabels uses its softmax  predictions \\non unlabeled data to teach the \\nstudent. These softmax  predictions \\nare generally called the soft labels, \\nwhich have been widely utilized in \\nthe literature on knowledge \\ndistillation\\nDeep Learning based \\nObject Detection\\n11Deep learning based object detection\\n•Perception tasks based on Computer Vision\\n12\\nFrom cs231n 2018 Stanford, Fei -Fei LiDeep learning based object detection\\n•Perception tasks based on Computer Vision\\n13From cs231n 2018 Stanford, Fei -Fei Li\\nDeep learning based object detection\\n•(Classic) Deep learning based object detection\\n•The RCNN Object Detector (2014)\\n•The Fast RCNN Object Detector (2015)\\n•The Faster RCNN Object Detector (2016)\\n•The YOLO Object Detector (2016)\\n•The SSD Object Detector (2016)\\n•Mask -RCNN (2017)\\n14\\nObject Detection\\n•Object detection\\n•Object detection as classification\\n•2D object detection with object categories+2D bounding boxes\\n15\\ncatdeer\\nObject Detection\\n•Object detection\\n•Object detection as classification\\n16\\nCNNdeer?\\ncat?\\nbackground?Object Detection\\n•Object detection\\n•Object detection as classification with Sliding Window\\n17\\nCNNdeer?\\ncat?\\nbackground?Object Detection\\n•Object detection\\n•Object detection as classification with Sliding Window\\n•Sliding Window Process\\n•Take a set of image patches\\n•For each patch, classify what object the patch contains, using an object classifier trained \\nwith hand -crafted features\\n•Do not know the location of the object of interest\\n•sliding window at small strides\\n•Do not know the size/ aspect ratio of the object of interest\\n•windows of different sizes\\n•Need to apply CNN to huge number of locations, scales, and aspect ratios, \\nvery computationally expensive!\\n18Object Detection\\n•Object detection\\n•Object detection as classification with Box Proposals\\n19\\nObject Detection\\n•Object detection\\n•Object detection as classification with Box Proposals\\n•As an alternative to sliding window search, evaluate a few hundred region \\nproposals\\n•Can use slower but more powerful features and classifiers\\n•Take advantage of low -level perceptual organization cues\\n•Proposal mechanism can be category -independent\\n•Proposal mechanism can be trained\\n20\\nBox Proposal\\n•Box Proposal Method – SS: Selective Search\\n21\\nSegmentation As \\nSelective Search for \\nObject Recognition. \\nvan de Sande et al. \\nICCV 2011\\nJ. Uijlings , K. van de \\nSande, T. Gevers , and \\nA. Smeulders , \\nSelective Search for \\nObject Recognition, \\nIJCV 2013Box Proposal\\n•Box Proposal Method – SS: Selective Search\\n•Use hierarchical segmentation: start with small superpixels  and merge \\nbased on diverse cues\\n22\\nSelective search detection pipeline\\n•Selective search detection pipeline\\n•Feature extraction: color SIFT, codebook of size 4K, spatial pyramid with four \\nlevels = 360K dimensions\\n23\\nJ. Uijlings , K. van de Sande , T. Gevers , and A. Smeulders , Selective Search for \\nObject Recognition , IJCV 2013Box Proposal - EdgeBoxes\\n•Another proposal method: EdgeBoxes\\n•Box score: number of edges in the box minus \\nnumber of edges that overlap the box boundary\\n•Uses a trained edge detector\\n•Uses efficient data structures (incl. integral \\nimages) for fast evaluation\\n•Gets 75% recall with 800 boxes (vs. 1400 for \\nSelective Search), is 40 times faster\\n24\\nC. Zitnick  and P. Dollar, Edge Boxes: Locating Object Proposals from Edges , \\nECCV 2014. R-CNN\\n•R-CNN ( Girshick  et al. CVPR 2014)\\n•Region proposals reduce brute force search as in sliding window\\n•Each proposed region is warped to match input size expected by CNN\\n•CNN gives more discriminatory features, compared to hand -crafted features\\n•Multi -class SVM trained with CNN features \\n25\\nRegions CNN\\nR. Girshick , J. Donahue, T. Darrell, and J. Malik, Rich Feature Hierarchies for Accurate Object Detection and \\nSemantic Segmentation , CVPR 2014. R-CNN\\n•R-CNN: Region proposals + CNN features\\n26\\nInput imageConvNetConvNet\\nConvNetSVMsSVMsSVMs\\nWarped image regionsForward each region \\nthrough ConvNetClassify regions with SVMs\\nRegion proposals R. Girshick , J. Donahue, T. \\nDarrell, and J. Malik, Rich \\nFeature Hierarchies for \\nAccurate Object Detection \\nand Semantic Segmentation , \\nCVPR 2014. Bbox  regBbox  regBbox  regLocalize objects with regression\\n•Regression is about returning a number instead of a class, in our case \\nwe\\'re going to return 4 numbers (x0,y0,width,height) that are related to \\na bounding box. \\n•You train this system with an image an a ground truth bounding box, and use L2 \\ndistance to calculate the loss between the predicted bounding box and the \\nground truth.\\n27\\nLocalize objects with regression\\n•Normally what you do is attach another fully connected layer on the \\nlast convolution layer\\n28\\nBounding box regressor\\nR-CNN\\n•Bounding box regressor\\n•During training, a bounding box regressor is learnt minimizing \\nregression loss compared to ground truth bounding boxes.\\n•Comparing bounding box prediction accuracy\\n•Compare if the Intersect Over Union ( ioU) between \\nthe prediction and the ground truth is bigger than \\nsome threshold (ex > 0.5)\\n30\\n31Thank \\nYou\\nAddress:\\nENG257, SJSUEmail Address:\\nKaikai.liu@sjsu.eduCMPE 258 -01 \\nDeep Learning\\nDr. Kaikai Liu, Ph.D. Associate Professor\\nDepartment of Computer Engineering\\nSan Jose State University \\nEmail: kaikai.liu@sjsu.edu\\nWebsite: https://www.sjsu.edu/cmpe/faculty/tenure -\\nline/kaikai -liu.phpSpring 2024Deep Learning based \\nObject Detection\\n2R-CNN pros and cons\\n•Pros\\n•Accurate!\\n•Any deep architecture can immediately be \\n“plugged in”\\n•Cons\\n•Ad hoc training objectives\\n•Fine-tune network with softmax  classifier\\n•Train post -hoc linear SVMs \\n•Train post -hoc bounding -box regressions (least \\nsquares)\\n•Training is slow (84h), takes a lot of disk space\\n•2000 CNN passes per image\\n•Inference (detection) is slow (47s / image with \\nVGG16)\\nNumber of CNN runs = number of \\nproposals à high latencyFast R -CNN\\nConvNetForward whole image through ConvNet“conv” feature map of image“RoI Pooling” layerLinear +\\nsoftmax\\nFCs Fully -connected layersSoftmax  classifier\\nRegion \\nproposalsLinear Bounding -box regressors\\nR. Girshick , Fast R -CNN , ICCV 2015 Source: R. Girshick1 CNN run for whole image\\nproposed regions extracted from \\nCNN feature map, instead of image Necessary to convert variable sized ROIs to \\nfixed size inputs \\nexpected by the following FC layers \\n(similar to warping original image patches in \\nR-CNN\\nfor fixed sized inputs to conv  layers)New operation: Region of Interest (ROI) pooling\\nFast R -CNN training\\nConvNetLinear +\\nsoftmax\\nFCsLinearLog loss + smooth L1 loss\\nTrainableMulti -task loss\\nR. Girshick , Fast R -CNN , ICCV 2015Bounding -box \\nregressorsClassifier\\nFully -connected layers\\nROL PollingFast R -CNN results\\nFast R -CNN R-CNN \\nTrain time (h) 9.5 84\\n- Speedup 8.8x 1x\\nTest time / \\nimage0.32s 47.0s\\nTest speedup 146x 1x\\nmAP 66.9% 66.0%\\nTimings exclude object proposal time, which is equal for all methods.\\nAll methods use VGG16 from Simonyan  and Zisserman .\\nSource: R. GirshickR-CNN vs SPP vs Fast R -CNN\\nGirshick  et al, “Rich feature hierarchies for accurate object detection and semantic segmentation”, CVPR 2014.\\nHe et al, “Spatial pyramid pooling in deep convolutional networks for visual recognition”, ECCV 2014\\nGirshick , “Fast R -CNN”, ICCV 2015Faster R -CNN\\nCNNfeature mapRegion \\nproposals\\nCNNfeature mapRegion Proposal \\nNetwork\\nS. Ren, K. He, R. Girshick , and J. Sun, Faster R -CNN: Towards Real -Time Object Detection with \\nRegion Proposal Networks , NIPS 2015share featuresInsert Region Proposal\\nNetwork (RPN) to predict\\nproposals from featuresRegion Proposal Network (RPN)\\nInstead of external region proposals in Fast R -CNN, Faster R -CNN \\ngenerates proposals re-using the same initial convolutional feature map.  Region Proposal Network (RPN)\\n•Make CNN do proposals!\\n•Jointly train with 4 losses:\\n•RPN classify object / not object\\n•RPN regress box coordinates\\n•Final classification score (object classes)\\n•Final box coordinates\\n16\\nRen et al, “Faster R -CNN: Towards Real -Time Object \\nDetection with Region Proposal Networks”, NIPS 2015Region Proposal Network (RPN)\\nforeground vs. background\\nprobabilities x, y, width, height\\ndecides the location of the region proposaldecides the location of the region proposal decides the shape/size of the region proposal\\n(adjust if we are only looking for faces (square),\\nor cars (wide -short) or pedestrians (narrow -tall))\\nRegion proposal network (RPN)\\n•Slide a small window over the conv layer \\n•Predict object/no object\\n•Regress bounding box coordinates\\n•Box regression is with reference to anchors (3 scales x 3 aspect ratios)\\nIn order to choose \\nthe set of anchors we \\nusually define a set \\nof sizes (e.g. 64px, \\n128px, 256px) and a \\nset of ratios between \\nwidth and height of \\nboxes (e.g. 0.5, 1, 1.5) \\nand use all the \\npossible \\ncombinations of sizes \\nand ratios.Faster R -CNN\\n•When modeling deep neural networks, the last block is usually a fixed sized \\ntensor output. For example, in image classification, the output is \\na(N,)shaped tensor, with Nbeing the number of classes, where each scalar \\nin location icontains the probability of that image being label i.\\n•One issue with object detection is generating a variable -length list of \\nbounding boxes\\n•The variable -length problem is solved in the RPN by using anchors: fixed \\nsized reference bounding boxes which are placed uniformly throughout the \\noriginal image. Instead of having to detect where objects are, we model the \\nproblem into two parts. For every anchor, we ask:\\n•Does this anchor contain a relevant object?\\n•How would we adjust this anchor to better fit the relevant object?\\n19Faster R -CNN\\n•Complete Faster R -CNN architecture\\n20\\nrissubsampling ratio\\nVGG16, r=16\\nRPN takes \\nall the \\nanchorsOutputs a \\nset of \\ngood \\nproposals \\nfor \\nobjects\\nRegion of \\nInterest Pooling\\nAnchor points (x c, yc)\\nScales and ratios for (h, w)\\nN candidates per anchorFaster R -CNN\\n•RPN Training\\n•For training, we take all the anchors and put them into two different categories. Those \\nthat overlap a ground -truth object with an Intersection over Union (IoU) bigger than \\n0.5 are considered “foreground” and those that don’t overlap any ground truth object or \\nhave less than 0.1 IoU with ground -truth objects are considered “background”.\\n•Then, we randomly sample those anchors to form a mini batch of size 256 — trying to \\nmaintain a balanced ratio between foreground and background anchors.\\n•The RPN uses all the anchors selected for the mini batch to calculate the classification \\nloss using binary cross entropy. \\n•Then, it uses only those minibatch anchors marked as foreground  to calculate the \\nregression loss. For calculating the targets for the regression, we use the foreground \\nanchor and the closest ground truth object and calculate the correct Δneeded to \\ntransform the anchor into the object.\\n•Instead of using a simple L1 or L2 loss for the regression error, the paper suggests using Smooth \\nL1 loss. Smooth L1 is basically L1, but when the L1 error is small enough, defined by a certain σ, \\nthe error is considered almost correct and the loss diminishes at a faster rate.\\n21Faster R -CNN\\n•Post Processing\\n•Non-maximum suppression (NMS)\\n•Since anchors usually overlap, proposals end up also overlapping over the same object. \\n•NMS takes the list of proposals sorted by score and iterates over the sorted list, discarding those \\nproposals that have an IoU larger than some predefined threshold with a proposal that has a higher \\nscore.\\n•Proposal selection after applying NMS, we keep the top N proposals sorted by score. In the \\npaper N=2000 is used, but it is possible to lower that number to as little as 50 and still get quite \\ngood results\\n•The RPN can be used by itself without needing the second stage model. In problems \\nwhere there is only a single class of objects, the objectness  probability can be used as \\nthe final class probability. This is because for this case, “foreground” = “single class” and \\n“background” = “not single class”.\\n•One of the advantages of using only the RPN is the gain in speed both in training and \\nprediction. Since the RPN is a very simple network which only uses convolutional layers, \\nthe prediction time can be faster than using the classification base network.\\n22Faster R -CNN\\n•Region of Interest Pooling\\n•After the RPN step, we have a bunch of object proposals with no class assigned \\nto them. Our next problem to solve is how to take these bounding boxes and \\nclassify them into our desired categories.\\n•The main problem  is that running the computations for all the 2000 proposals is \\nreally inefficient  and slow.\\n•Reusing the existing convolutional feature map. This is done by extracting fixed -sized feature \\nmaps for each proposal using region of interest pooling. Fixed size feature maps are needed \\nfor the R -CNN in order to  classify them into a fixed number of classes.\\n23\\nresize each crop to a fixed sized 14x14, \\nthen max pooling with a 2x2 kernel Faster R -CNN\\n•Then, it uses two different fully -connected layers for each of the \\ndifferent objects. \\n•A fully -connected layer with N+1units where Nis the total number of classes and \\nthat extra one is for the background class.\\n•A fully -connected layer with 4Nunits. We want to have a regression prediction, \\nthus we need Δcenter x,Δcenter y,Δwidth ,Δheight for each of the N possible \\nclasses.\\n25\\nFaster R -CNN\\n•Training and targets for R -CNN \\n•We take the proposals and the ground -truth boxes, and calculate the IoU \\n•Those proposals that have a IoU greater than 0.5 with any ground truth box get assigned to \\nthat ground truth. \\n•Those that have between 0.1 and 0.5 get labeled as background. \\n•Different from RPN, we ignore proposals without any intersection. This is because at this stage \\nwe are assuming that we have good proposals\\n•The targets for the bounding box regression are calculated as the offset between \\nthe proposal and its corresponding ground -truth box, only for those proposals \\nthat have been assigned a class based on the IoU threshold.\\n•We randomly sample a balanced mini batch of size 64 in which we have up to 25% foreground \\nproposals (with class) and 75% background.\\n•The classification loss is now a multiclass cross entropy loss, using all the selected proposals \\nand the Smooth L1 loss for the 25% proposals that are matched to a ground truth box.\\n26Faster R -CNN\\n•Post processing\\n•After getting the final objects and ignoring those predicted as background, we apply \\nclass -based NMS. This is done by grouping the objects by class, sorting them by \\nprobability and then applying NMS to each independent group before joining them again.\\n•Training\\n•In the original paper, Faster R -CNN was trained using a multi -step approach, training \\nparts independently and merging the trained weights before a final full training approach. \\nSince then, it has been found that doing end -to-end, joint training leads to better results.\\n•After putting the complete model together we end up with 4 different losses, two for the \\nRPN and two for R -CNN. We have the trainable layers in RPN and R -CNN, and we also \\nhave the base network which we can train (fine -tune) or not. The four different losses are \\ncombined using a weighted sum.\\n2728Thank \\nYou\\nAddress:\\nENG257, SJSUEmail Address:\\nKaikai.liu@sjsu.edu'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cassio.init(token = ASTRA_DB_APPLICATION_TOKEN, database_id = ASTRA_DB_ID)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S8vO9IWU6Det",
        "outputId": "0b85df00-b4aa-4d6c-f07f-e2b52494684c"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:cassandra.cluster:Downgrading core protocol version from 66 to 65 for 467263e6-e41c-46a9-b7fc-fde12fc3c6aa-us-east1.db.astra.datastax.com:29042:9a6df4e1-13db-40ef-a4fa-23ead13c781f. To avoid this, it is best practice to explicitly set Cluster(protocol_version) to the version supported by your cluster. http://datastax.github.io/python-driver/api/cassandra/cluster.html#cassandra.cluster.Cluster.protocol_version\n",
            "WARNING:cassandra.cluster:Downgrading core protocol version from 65 to 5 for 467263e6-e41c-46a9-b7fc-fde12fc3c6aa-us-east1.db.astra.datastax.com:29042:9a6df4e1-13db-40ef-a4fa-23ead13c781f. To avoid this, it is best practice to explicitly set Cluster(protocol_version) to the version supported by your cluster. http://datastax.github.io/python-driver/api/cassandra/cluster.html#cassandra.cluster.Cluster.protocol_version\n",
            "ERROR:cassandra.connection:Closing connection <AsyncoreConnection(136747931850672) 467263e6-e41c-46a9-b7fc-fde12fc3c6aa-us-east1.db.astra.datastax.com:29042:9a6df4e1-13db-40ef-a4fa-23ead13c781f> due to protocol error: Error from server: code=000a [Protocol error] message=\"Beta version of the protocol used (5/v5-beta), but USE_BETA flag is unset\"\n",
            "WARNING:cassandra.cluster:Downgrading core protocol version from 5 to 4 for 467263e6-e41c-46a9-b7fc-fde12fc3c6aa-us-east1.db.astra.datastax.com:29042:9a6df4e1-13db-40ef-a4fa-23ead13c781f. To avoid this, it is best practice to explicitly set Cluster(protocol_version) to the version supported by your cluster. http://datastax.github.io/python-driver/api/cassandra/cluster.html#cassandra.cluster.Cluster.protocol_version\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm = OpenAI(openai_api_key = OPENAI_API_KEY)\n",
        "embedding = OpenAIEmbeddings(openai_api_key = OPENAI_API_KEY)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "33u6zU2K6R_E",
        "outputId": "65873957-e073-4c09-f76f-77a1e2f1d066"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.llms.openai.OpenAI` was deprecated in langchain-community 0.0.10 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAI`.\n",
            "  warn_deprecated(\n",
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.embeddings.openai.OpenAIEmbeddings` was deprecated in langchain-community 0.0.9 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAIEmbeddings`.\n",
            "  warn_deprecated(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "astra_vector_store = Cassandra(\n",
        "    embedding = embedding,\n",
        "    table_name = 'qa_mini_demo',\n",
        "    session = None,\n",
        "    keyspace = None\n",
        ")"
      ],
      "metadata": {
        "id": "HyD7MOSr6ntr"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "\n",
        "text_splitter = CharacterTextSplitter(\n",
        "    separator = '\\n',\n",
        "    chunk_size = 900,\n",
        "    chunk_overlap = 200,\n",
        "    length_function = len\n",
        ")\n",
        "\n",
        "texts = text_splitter.split_text(raw_text)"
      ],
      "metadata": {
        "id": "EsQhCfWg7Bv3"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "texts[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hm9RmgEajbnz",
        "outputId": "21885868-19f6-44a3-bd31-0f3171f3a6c5"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['CMPE 258 -01 \\nDeep Learning\\nDr. Kaikai Liu, Ph.D. Associate Professor\\nDepartment of Computer Engineering\\nSan Jose State University \\nEmail: kaikai.liu@sjsu.edu\\nWebsite: https://www.sjsu.edu/cmpe/faculty/tenure -\\nline/kaikai -liu.phpSpring 2024What is Deep Learning\\n•Deep Learning is a part of machine learning that deals with algorithms \\ninspired by the structure and function of the human brain. It uses \\nartificial neural networks to build intelligent models and solve complex \\nproblems. \\nWhat is Artificial Intelligence –old answer\\n•AI textbooks list\\n•http://aima.cs.berkeley.edu/2nd -ed/books.html\\n•Artificial Intelligence: A Modern Approach\\n•http://aima.cs.berkeley.edu/2nd -ed/books.html\\n•http://aima.cs.berkeley.edu/newchap00.pdf\\nWhat is Artificial Intelligence –old answer\\nhttps://aima.cs.berkeley.edu•Peter Norvig is a Director of Research at Google Inc\\n•http://www.norvig.com',\n",
              " '•http://aima.cs.berkeley.edu/newchap00.pdf\\nWhat is Artificial Intelligence –old answer\\nhttps://aima.cs.berkeley.edu•Peter Norvig is a Director of Research at Google Inc\\n•http://www.norvig.com\\n•SJSU EIAC Membership\\nWhat is Artificial Intelligence –new answer\\n•The Present advancements in AI\\nMachine Learning Lifecycle\\n•Machine Learning play with all ‘Data’\\nOffline\\nTraining\\nDataData\\nCollectionCleaning &\\nVisualizatio n\\nFeature Eng. &\\nModel DesignTraining &\\nValidationModel Development\\nTrained\\nModelsTraining Pipelines\\nLive\\nDataTraining\\nValidationEnd User\\nApplicationQuery\\nPredictionPrediction ServiceInference\\nFeedbackLogic\\nData\\nScientistData\\nEngineerData\\nEngineerDeep Learning vs Machine Learning\\n•Deep Learning is a sub -class of Machine Learning algorithms whose \\npeculiarity is a higher level of complexity.\\n•Deep Learning belongs to Machine Learning and they are absolutely not',\n",
              " '•Deep Learning is a sub -class of Machine Learning algorithms whose \\npeculiarity is a higher level of complexity.\\n•Deep Learning belongs to Machine Learning and they are absolutely not \\nopposite concepts. We refer to shallow learning to those techniques of machine \\nlearning that are not deep\\n•Why is this complexity an advantage?\\n•As humans, the information is learnt step by step. First layers focus on learning \\nmore specific concepts while the deeper layers will use the information already \\nlearnt to soak in more abstract concepts. This procedure of constructing \\nrepresentations of the data is known as feature extraction.\\n•Their complex architecture provides deep neural nets with the ability to perform a \\nfeature extraction automatically.Deep Learning vs Machine Learning\\n•In conventional machine learning, \\nor shallow learning, this task is \\ncarried out outside the algorithmic',\n",
              " 'feature extraction automatically.Deep Learning vs Machine Learning\\n•In conventional machine learning, \\nor shallow learning, this task is \\ncarried out outside the algorithmic \\nstage. People, data scientists’ \\nteams and not machines, are in \\ncharge of analyzing raw data and \\nchange it into valuable features.\\n•Deep learning is a black box .\\nMore Data Better Performance\\n•More Data Better Performance\\nDeep Learning History\\n•The theory of AI is fairly old\\n•The backpropagation algorithm, for example, \\nwas invented more than 40 years ago, and the \\nfirst computation model for neural networks was \\nproposed for the first time almost 80 years back.\\n•AI Winters: the general public lost interest and \\nfunding for AI research dried out.\\n•The history of AI has been a chain of boom -and-\\nbust cycles. We’re currently immersed in the \\nthird cycle of optimism\\nDeep Learning History',\n",
              " 'funding for AI research dried out.\\n•The history of AI has been a chain of boom -and-\\nbust cycles. We’re currently immersed in the \\nthird cycle of optimism\\nDeep Learning History\\n•The Birth of AI (1943 –1956)\\n•1943: Warren McCulloch and Walter Pitts, proposed a model of artificial neurons\\n•In 1949, Donald Hebb introduced the Hebbian learning rule\\n•Year 1950: The Turing Test\\n•The term “Artificial Intelligence” was officially coined in 1956 by American computer \\nscientist John McCarthy during the Dartmouth Conference. Establishing AI as a distinct \\narea of research and study. At this time, high -level computer languages like FORTRAN, \\nLISP, and COBOL were also invented, fueling enthusiasm for AI research and \\ndevelopment.\\n•The first AI boom took place in the late 50s and 60s when efforts focused on \\nanswering if machines could actually think',\n",
              " \"development.\\n•The first AI boom took place in the late 50s and 60s when efforts focused on \\nanswering if machines could actually think\\n•The search for the so -called general or strong AI\\n•The invention of the perceptron (an early example of artificial neuron or machine \\nlearning classifier) in 1957 by Frank Rosenblatt was, for some, an unambiguous \\nindication that general or strong AI was very close.Deep Learning History\\n•Year 1969: Limitations of Perceptron was \\nreleased: it could not learn to solve problems that \\nwere not linearly separable. \\n•Year 1972: WABOT -1 —The First Humanoid \\nRobot\\n•The AI Winter of 1973 -1980\\n•UK Parliament analyze the state of AI research after two \\ndecades of disappointing progress in AI (and specifically \\nin Machine Translation): Lighthill Report\\n•DARPA's frustration with the Speech Understanding \\nResearch program at Carnegie Mellon University\",\n",
              " \"decades of disappointing progress in AI (and specifically \\nin Machine Translation): Lighthill Report\\n•DARPA's frustration with the Speech Understanding \\nResearch program at Carnegie Mellon University\\n•DARPA shifting its focus on “mission -oriented”, \\nactionable research which led many AI research groups \\nto lose critical funding\\nDeep Learning History\\n•The AI in the limelight again (80s)\\n•The emergence of expert systems: the developments revolved around the idea of \\ncreating knowledge bases that an inference engine (following logical rules) used to \\nanswer questions about a specific domain of knowledge, e.g. medical diagnosis.\\n•Recurrent neural networks and the backpropagation algorithm (1986) were also \\ndeveloped. \\n•The Second AI Winter (1987 –1993)\\n•The computational power at the time hampered remarkable improvements which \\nbrought the second AI winter.\",\n",
              " \"developed. \\n•The Second AI Winter (1987 –1993)\\n•The computational power at the time hampered remarkable improvements which \\nbrought the second AI winter.\\n•1987: collapse of the LISP machine market (general -purpose computers designed to \\nefficiently run Lisp as their main software and programming language)\\n•1988: cancellation of new spending on AI by the Strategic Computing Initiative\\n•1993: resistance to new expert systems deployment and maintenanceDeep Learning History\\n•In the 90s, a new vision brought fresh air to AI\\n•Moravec's paradox is the observation in artificial intelligence and robotics that, \\ncontrary to traditional assumptions, reasoning requires very little computation, but \\nsensorimotor and perception skills require enormous computational resources.\\n•The more difficult tasks , however, were indeed those that we do innately,\",\n",
              " 'sensorimotor and perception skills require enormous computational resources.\\n•The more difficult tasks , however, were indeed those that we do innately, \\nalmost effortlessly, like recognising faces and moving around. \\n•They advocated building intelligence “from the bottom up” and taking into account the role of \\n“the body” in human intelligence. \\n•Consequently, the quest for general AI lost momentum and efforts were redirected to solve \\nspecific isolated problems . This gave rise to the so -called narrow or weak AI.\\n•It was also the time when advanced ML algorithms like Support Vector Machines, \\nRandom Forests, and the area of Reinforcement Learning were developed.Deep Learning History\\n•2006: birth of deep learning\\n•While everybody moved to the algorithms like SVM and all, Geoffrey Hinton still \\nbelieved that true intelligence would be achieved only through Neural Networks.',\n",
              " '•2006: birth of deep learning\\n•While everybody moved to the algorithms like SVM and all, Geoffrey Hinton still \\nbelieved that true intelligence would be achieved only through Neural Networks. \\nSo for almost 20 years i.e. from 1986 to 2006, he worked on neural networks. \\n•And in 2006 he came up with a phenomenal paper on training a deep neural \\nnetwork. This is the beginning of the era known as Deep Learning. This paper by \\nGeoffrey Hinton did not receive much popularity until 2012.Deep Learning Milestones\\n•MILESTONES IN THE DEVELOPMENT OF NEURAL NETWORKS\\nCurrent “AI Spring”\\n•The successes of the current \"AI spring\" or \"AI boom\" are advances in language \\ntranslation (in particular, Google Translate), image recognition (spurred by the ImageNet \\ntraining database) as commercialized by Google Image Search, and in game -playing']"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "astra_vector_store.add_texts(texts)\n",
        "print('Inserted %i headlines.' % len(texts))\n",
        "\n",
        "astra_vector_index = VectorStoreIndexWrapper(vectorstore = astra_vector_store)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "92mpTI4njfDR",
        "outputId": "62024e81-8899-4d92-af99-cec12e14fd38"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inserted 241 headlines.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "first_question = True\n",
        "while True:\n",
        "    if first_question:\n",
        "        query_text = input(\"\\nEnter your question (or type 'quit' to exit): \").strip()\n",
        "    else:\n",
        "        query_text = input(\"\\nWhat's your next question (or type 'quit' to exit): \").strip()\n",
        "\n",
        "    if query_text.lower() == \"quit\":\n",
        "        break\n",
        "\n",
        "    if query_text == \"\":\n",
        "        continue\n",
        "\n",
        "    first_question = False\n",
        "\n",
        "    print(\"\\nQUESTION: \\\"%s\\\"\" % query_text)\n",
        "    answer = astra_vector_index.query(query_text, llm=llm).strip()\n",
        "    print(\"ANSWER: \\\"%s\\\"\\n\" % answer)\n",
        "\n",
        "    print(\"FIRST DOCUMENTS BY RELEVANCE:\")\n",
        "    for doc, score in astra_vector_store.similarity_search_with_score(query_text, k=4):\n",
        "        print(\"    [%0.4f] \\\"%s ...\\\"\" % (score, doc.page_content[:84]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9perxjuzkIc3",
        "outputId": "fad4ceac-b437-4fe1-b59d-633bec699b72"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Enter your question (or type 'quit' to exit): what is resnet\n",
            "\n",
            "QUESTION: \"what is resnet\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:cassandra.protocol:Server warning: Top-K queries can only be run with consistency level ONE / LOCAL_ONE / NODE_LOCAL. Consistency level LOCAL_QUORUM was requested. Downgrading the consistency level to LOCAL_ONE.\n",
            "WARNING:cassandra.protocol:Server warning: Top-K queries can only be run with consistency level ONE / LOCAL_ONE / NODE_LOCAL. Consistency level LOCAL_QUORUM was requested. Downgrading the consistency level to LOCAL_ONE.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ANSWER: \"ResNet stands for Residual Network, which is a type of deep neural network architecture that uses residual connections to facilitate training of very deep networks. It was first introduced in 2015 and has since become a popular choice for image recognition and classification tasks, often outperforming other architectures. ResNet is known for its ability to train very deep networks (up to 152 layers) without suffering from the vanishing gradient problem.\"\n",
            "\n",
            "FIRST DOCUMENTS BY RELEVANCE:\n",
            "    [0.8874] \"top 5 error)\n",
            "•Swept all classification and  detection \n",
            "competitions in  ILSVRC’15 an ...\"\n",
            "    [0.8837] \"•ResNet  V2 (bottom) avoids all \n",
            "nonlinearities in the residual \n",
            "pathway \n",
            "•Up to 152 ...\"\n",
            "    [0.8723] \"sequential convolutions  respectively. This improves computational speed. This is \n",
            "t ...\"\n",
            "    [0.8718] \"def resnet50(*, weights: Optional[ResNet50_Weights] = None, progress: bool = True, * ...\"\n",
            "\n",
            "What's your next question (or type 'quit' to exit): what is the core idea behind densenet\n",
            "\n",
            "QUESTION: \"what is the core idea behind densenet\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:cassandra.protocol:Server warning: Top-K queries can only be run with consistency level ONE / LOCAL_ONE / NODE_LOCAL. Consistency level LOCAL_QUORUM was requested. Downgrading the consistency level to LOCAL_ONE.\n",
            "WARNING:cassandra.protocol:Server warning: Top-K queries can only be run with consistency level ONE / LOCAL_ONE / NODE_LOCAL. Consistency level LOCAL_QUORUM was requested. Downgrading the consistency level to LOCAL_ONE.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ANSWER: \"The core idea behind DenseNet is feature reuse, where the input of each layer consists of the feature maps of all earlier layers, and its output is passed to each subsequent layer. This encourages feature reuse and makes the network highly parameter-efficient.\"\n",
            "\n",
            "FIRST DOCUMENTS BY RELEVANCE:\n",
            "    [0.9067] \"CNN\n",
            "•Paper: https://arxiv.org/abs/1608.06993\n",
            "•The input of each layer consists of th ...\"\n",
            "    [0.8963] \"whilst keeping in mind VGG’s philosophy of \n",
            "repeating blocks and branches with ident ...\"\n",
            "    [0.8817] \"a result it requires fewer parameters than other CNNs, as there are no repeated \n",
            "fea ...\"\n",
            "    [0.8808] \"ImageNet classification with deep convolutional \n",
            "neural networks , Neural Informatio ...\"\n",
            "\n",
            "What's your next question (or type 'quit' to exit): what are auxiliary classifiers in googlenet\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:cassandra.protocol:Server warning: Top-K queries can only be run with consistency level ONE / LOCAL_ONE / NODE_LOCAL. Consistency level LOCAL_QUORUM was requested. Downgrading the consistency level to LOCAL_ONE.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "QUESTION: \"what are auxiliary classifiers in googlenet\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:cassandra.protocol:Server warning: Top-K queries can only be run with consistency level ONE / LOCAL_ONE / NODE_LOCAL. Consistency level LOCAL_QUORUM was requested. Downgrading the consistency level to LOCAL_ONE.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ANSWER: \"Auxiliary classifiers in GoogLeNet are additional classifiers added to the intermediate layers of the architecture, specifically the third (Inception 4[a]) and sixth (Inception4[d]) layers. They are only used during training and removed during inference. Their purpose is to perform classification based on the inputs within the network's midsection and add the calculated loss back to the total loss of the network. This helps prevent vanishing gradient descent and overfitting in extensive networks. In GoogLeNet, there are a total of 9 inception modules, with 22 layers in total, and the input layer takes in an image of dimension 224x224. The \"Inception\" module design involves using kernels of various sizes in parallel to extract both bigger and smaller features simultaneously, increasing the width of the model instead of depth.\"\n",
            "\n",
            "FIRST DOCUMENTS BY RELEVANCE:\n",
            "    [0.8918] \"architecture, namely the third(Inception 4[a]) and sixth (Inception4[d])\n",
            "•Auxilary   ...\"\n",
            "    [0.8915] \"•GoogLeNet : classifier\n",
            "•The average pooling layer takes a mean across all the featu ...\"\n",
            "    [0.8676] \"•GoogLeNet : 22 layers\n",
            "•ILSVRC’14 classification winner  (6.7% top 5 error)\n",
            "•part of ...\"\n",
            "    [0.8663] \"confirmation bias in pseudo -labeling\n",
            "Noisy Student\n",
            "•Self-training with Noisy Studen ...\"\n",
            "\n",
            "What's your next question (or type 'quit' to exit): quuit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:cassandra.protocol:Server warning: Top-K queries can only be run with consistency level ONE / LOCAL_ONE / NODE_LOCAL. Consistency level LOCAL_QUORUM was requested. Downgrading the consistency level to LOCAL_ONE.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "QUESTION: \"quuit\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:cassandra.protocol:Server warning: Top-K queries can only be run with consistency level ONE / LOCAL_ONE / NODE_LOCAL. Consistency level LOCAL_QUORUM was requested. Downgrading the consistency level to LOCAL_ONE.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ANSWER: \"I don't see any mention of quitting or dropping the class in the given context. It seems like the policies and protocols mentioned are regarding assignments, grades, and conduct in the classroom. If you are considering quitting the class, I suggest speaking with the instructor directly to discuss your options.\"\n",
            "\n",
            "FIRST DOCUMENTS BY RELEVANCE:\n",
            "    [0.8685] \"acceptable to turnitin (such as WORD and PDF); otherwise the reports will be \n",
            "consid ...\"\n",
            "    [0.8685] \"acceptable to turnitin (such as WORD and PDF); otherwise the reports will be \n",
            "consid ...\"\n",
            "    [0.8637] \"Website: https://www.sjsu.edu/cmpe/faculty/tenure -\n",
            "line/kaikai -liu.phpSpring 2024D ...\"\n",
            "    [0.8632] \"required policies, which means what you perceive as a low score by summing up \n",
            "all y ...\"\n",
            "\n",
            "What's your next question (or type 'quit' to exit): quit\n"
          ]
        }
      ]
    }
  ]
}